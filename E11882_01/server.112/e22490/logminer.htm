<!DOCTYPE html>
<html lang="en" >
<head>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta charset="utf-8">
<title>Using LogMiner to Analyze Redo Log Files</title>
<meta name="generator" content="Oracle DARB XHTML Converter (Mode = document) - Merged Version 1093" />
<meta name="dcterms.created" content="2018-03-26T15:55:17Z" />
<meta name="robots" content="all" />
<meta name="dcterms.title" content="Database Utilities" />
<meta name="dcterms.identifier" content="E22490-08" />
<meta name="dcterms.isVersionOf" content="SUTIL" />
<meta name="dcterms.rights" content="Copyright&nbsp;&copy;&nbsp;1996, 2018,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved." />
<link rel="Start" href="../../index.htm" title="Home" type="text/html" />
<link rel="Copyright" href="../../dcommon/html/cpyr.htm" title="Copyright" type="text/html" />

<script type="application/javascript"  src="../../dcommon/js/headfoot.js"></script>
<script type="application/javascript"  src="../../nav/js/doccd.js"></script>
<link rel="Contents" href="toc.htm" title="Contents" type="text/html" />
<link rel="Index" href="index.htm" title="Index" type="text/html" />
<link rel="Prev" href="dbnewid.htm" title="Previous" type="text/html" />
<link rel="Next" href="metadata_api.htm" title="Next" type="text/html" />
<link rel="alternate" href="../e22490.pdf" title="PDF version" type="application/pdf" />
<link rel="schema.dcterms" href="http://purl.org/dc/terms/" />
<link rel="stylesheet" href="../../dcommon/css/fusiondoc.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/header.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/footer.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/fonts.css">
<link rel="stylesheet" href="../../dcommon/css/foundation.css">
<link rel="stylesheet" href="../../dcommon/css/codemirror.css">
<link rel="stylesheet" type="text/css" title="Default" href="../../nav/css/html5.css">
<link rel="stylesheet" href="../../dcommon/css/respond-480-tablet.css">
<link rel="stylesheet" href="../../dcommon/css/respond-768-laptop.css">
<link rel="stylesheet" href="../../dcommon/css/respond-1140-deskop.css">
<script type="application/javascript" src="../../dcommon/js/modernizr.js"></script>
<script type="application/javascript" src="../../dcommon/js/codemirror.js"></script>
<script type="application/javascript" src="../../dcommon/js/jquery.js"></script>
<script type="application/javascript" src="../../dcommon/js/foundation.min.js"></script>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-552992c80ef99c8d" async="async"></script>
<script type="application/javascript" src="../../dcommon/js/jqfns.js"></script>
<script type="application/javascript" src="../../dcommon/js/ohc-inline-videos.js"></script>
<!-- Add fancyBox -->
<link rel="stylesheet" href="../../dcommon/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<!-- Optionally add helpers - button, thumbnail and/or media -->
<link rel="stylesheet"  href="../../dcommon/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5"  type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<link rel="stylesheet"  href="../../dcommon/fancybox/helpers/jquery.fancybox-thumbs.css?v=1.0.7"  type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
</head>
<body>
<a href="#BEGIN" class="accessibility-top skipto" tabindex="0">Go to main content</a><header><!--
<div class="zz-skip-header"><a id="top" href="#BEGIN">Go to main content</a>--></header>
<div class="row" id="CONTENT">
<div class="IND large-9 medium-8 columns" dir="ltr">
<a id="BEGIN" name="BEGIN"></a>
<span id="PAGE" style="display:none;">30/36</span> <!-- End Header -->
<div id="SUTIL019" class="chapter"><a id="i1005553"></a>
<h1 class="chapter"><span class="secnum">19</span> Using LogMiner to Analyze Redo Log Files</h1>
<p><a id="sthref1529"></a><a id="sthref1530"></a><a id="sthref1531"></a>Oracle LogMiner, which is part of Oracle Database, enables you to query online and archived redo log files through a SQL interface. Redo log files contain information about the history of activity on a database.</p>
<p>This chapter contains the following sections:</p>
<ul>
<li>
<p><a href="#i1005606">LogMiner Benefits</a></p>
</li>
<li>
<p><a href="#i1010243">Introduction to LogMiner</a></p>
</li>
<li>
<p><a href="#i1015913">LogMiner Dictionary Files and Redo Log Files</a></p>
</li>
<li>
<p><a href="#i1014404">Starting LogMiner</a></p>
</li>
<li>
<p><a href="#i1016535">Querying V$LOGMNR_CONTENTS for Redo Data of Interest</a></p>
</li>
<li>
<p><a href="#i1016539">Filtering and Formatting Data Returned to V$LOGMNR_CONTENTS</a></p>
</li>
<li>
<p><a href="#i1017514">Reapplying DDL Statements Returned to V$LOGMNR_CONTENTS</a></p>
</li>
<li>
<p><a href="#i1016542">Calling DBMS_LOGMNR.START_LOGMNR Multiple Times</a></p>
</li>
<li>
<p><a href="#i1021068">Supplemental Logging</a></p>
</li>
<li>
<p><a href="#i1014119">Accessing LogMiner Operational Information in Views</a></p>
</li>
<li>
<p><a href="#i1009063">Steps in a Typical LogMiner Session</a></p>
</li>
<li>
<p><a href="#i1028311">Examples Using LogMiner</a></p>
</li>
<li>
<p><a href="#i1031090">Supported Datatypes, Storage Attributes, and Database and Redo Log File Versions</a></p>
</li>
</ul>
<p>This chapter describes LogMiner as it is used from the command line. You can also access LogMiner through the Oracle LogMiner Viewer graphical user interface.<a id="sthref1532"></a><a id="sthref1533"></a> Oracle LogMiner Viewer is a part of Oracle Enterprise Manager. See the Oracle Enterprise Manager online Help for more information about Oracle LogMiner Viewer.</p>
<a id="i1005606"></a>
<div id="SUTIL1552" class="sect1">
<h2 class="sect1">LogMiner Benefits</h2>
<p>All changes made to user data or to the database dictionary are recorded in the Oracle redo log files so that database recovery operations can be performed.</p>
<p>Because LogMiner provides a well-defined, easy-to-use, and comprehensive relational interface to redo log files, it can be used as a powerful data auditing tool, and also as a sophisticated data analysis tool. The following list describes some key capabilities of LogMiner:</p>
<ul>
<li>
<p>Pinpointing when a logical corruption to a database, such as errors made at the application level, may have begun. These might include errors such as those where the wrong rows were deleted because of incorrect values in a <code dir="ltr">WHERE</code> clause, rows were updated with incorrect values, the wrong index was dropped, and so forth. For example, a user application could mistakenly update a database to give all employees 100 percent salary increases rather than 10 percent increases, or a database administrator (DBA) could accidently delete a critical system table. It is important to know exactly when an error was made so that you know when to initiate time-based or change-based recovery. This enables you to restore the database to the state it was in just before corruption. See <a href="#i1016607">"Querying V$LOGMNR_CONTENTS Based on Column Values"</a> for details about how you can use LogMiner to accomplish this.</p>
</li>
<li>
<p>Determining what actions you would have to take to perform fine-grained recovery at the transaction level. If you fully understand and take into account existing dependencies, then it may be possible to perform a table-specific undo operation to return the table to its original state. This is achieved by applying table-specific reconstructed SQL statements that LogMiner provides in the reverse order from which they were originally issued. See <a href="#i1040290">"Scenario 1: Using LogMiner to Track Changes Made by a Specific User"</a> for an example.</p>
<p>Normally you would have to restore the table to its previous state, and then apply an archived redo log file to roll it forward.</p>
</li>
<li>
<p>Performance tuning and capacity planning through trend analysis. You can determine which tables get the most updates and inserts. That information provides a historical perspective on disk access statistics, which can be used for tuning purposes. See <a href="#i1028674">"Scenario 2: Using LogMiner to Calculate Table Access Statistics"</a> for an example.</p>
</li>
<li>
<p>Performing postauditing. LogMiner can be used to track any data manipulation language (DML) and data definition language (DDL) statements executed on the database, the order in which they were executed, and who executed them. (However, to use LogMiner for such a purpose, you need to have an idea when the event occurred so that you can specify the appropriate logs for analysis; otherwise you might have to mine a large number of redo log files, which can take a long time. Consider using LogMiner as a complementary activity to auditing database use. See the <a class="olink ADMIN" href="../e25494/toc.htm"><span class="italic">Oracle Database Administrator's Guide</span></a> for information about database auditing.)</p>
</li>
</ul>
</div>
<!-- class="sect1" -->
<a id="i1010243"></a>
<div id="SUTIL1553" class="sect1">
<h2 class="sect1">Introduction to LogMiner</h2>
<p>The following sections provide a brief introduction to LogMiner, including the following topics:</p>
<ul>
<li>
<p><a href="#i1014375">LogMiner Configuration</a></p>
</li>
<li>
<p><a href="#i1014380">Directing LogMiner Operations and Retrieving Data of Interest</a></p>
</li>
</ul>
<p>The remaining sections in this chapter describe these concepts and related topics in more detail.</p>
<a id="i1014375"></a>
<div id="SUTIL1554" class="sect2">
<h3 class="sect2">LogMiner Configuration</h3>
<p><a id="sthref1534"></a><a id="sthref1535"></a>There are four basic objects in a LogMiner configuration that you should be familiar with: the source database, the mining database, the LogMiner dictionary, and the redo log files containing the data of interest:</p>
<ul>
<li>
<p>The <a id="sthref1536"></a><span class="bold">source database</span> is the database that produces all the redo log files that you want LogMiner to analyze.</p>
</li>
<li>
<p>The <a id="sthref1537"></a><span class="bold">mining database</span> is the database that LogMiner uses when it performs the analysis.</p>
</li>
<li>
<p>The <a id="sthref1538"></a><span class="bold">LogMiner dictionary</span> allows LogMiner to provide table and column names, instead of internal object IDs, when it presents the redo log data that you request.</p>
<p><a id="sthref1539"></a>LogMiner uses the dictionary to translate internal object identifiers and datatypes to object names and external data formats. Without a dictionary, LogMiner returns internal object IDs and presents data as binary data.</p>
<p>For example, consider the following SQL statement:</p>
<pre dir="ltr">
 INSERT INTO HR.JOBS(JOB_ID, JOB_TITLE, MIN_SALARY, MAX_SALARY)  VALUES('IT_WT','Technical Writer', 4000, 11000);
</pre>
<p>Without the dictionary, LogMiner will display:</p>
<pre dir="ltr">
insert into "UNKNOWN"."OBJ# 45522"("COL 1","COL 2","COL 3","COL 4") values
(HEXTORAW('45465f4748'),HEXTORAW('546563686e6963616c20577269746572'),
HEXTORAW('c229'),HEXTORAW('c3020b'));
</pre></li>
<li>
<p>The <span class="bold">redo log files</span> contain the changes made to the database or database dictionary.</p>
</li>
</ul>
<div id="SUTIL1555" class="sect3"><a id="sthref1540"></a>
<h4 class="sect3"><a id="sthref1541"></a>Sample Configuration</h4>
<p><a href="#i1018483">Figure 19-1</a> shows a sample LogMiner configuration. In this figure, the source database in Boston generates redo log files that are archived and shipped to a database in San Francisco. A LogMiner dictionary has been extracted to these redo log files. The mining database, where LogMiner will actually analyze the redo log files, is in San Francisco. The Boston database is running Oracle9<span class="italic">i,</span> and the San Francisco database is running Oracle Database 10<span class="italic">g.</span></p>
<div id="SUTIL3613" class="figure">
<p class="titleinfigure"><a id="i1018483"></a>Figure 19-1 Sample LogMiner Database Configuration</p>
<img width="455" height="115" src="img/remote_config.gif" alt="Description of Figure 19-1 follows" /><br />
<a id="sthref1542" href="img_text/remote_config.htm">Description of ''Figure 19-1 Sample LogMiner Database Configuration''</a><br />
<br /></div>
<!-- class="figure" -->
<p><a href="#i1018483">Figure 19-1</a> shows just one valid LogMiner configuration. Other valid configurations are those that use the same database for both the source and mining database, or use another method for providing the data dictionary. These other data dictionary options are described in <a href="#i1014687">"LogMiner Dictionary Options"</a>.</p>
</div>
<!-- class="sect3" -->
<div id="SUTIL1556" class="sect3"><a id="sthref1543"></a>
<h4 class="sect3">Requirements</h4>
<p>The following are requirements for the source and mining database, the data dictionary, and the redo log files that LogMiner will mine:</p>
<ul>
<li>
<p><a id="sthref1544"></a>Source and mining database</p>
<ul>
<li>
<p>Both the source database and the mining database must be running on the same hardware platform.</p>
</li>
<li>
<p>The mining database can be the same as, or completely separate from, the source database.</p>
</li>
<li>
<p>The mining database must run the same release or a later release of the Oracle Database software as the source database.</p>
</li>
<li>
<p>The mining database must use the same character set (or a superset of the character set) used by the source database.</p>
</li>
</ul>
</li>
<li>
<p><a id="sthref1545"></a><a id="sthref1546"></a>LogMiner dictionary</p>
<ul>
<li>
<p>The dictionary must be produced by the same source database that generates the redo log files that LogMiner will analyze.</p>
</li>
</ul>
</li>
<li>
<p><a id="sthref1547"></a><a id="sthref1548"></a>All redo log files:</p>
<ul>
<li>
<p>Must be produced by the same source database.</p>
</li>
<li>
<p>Must be associated with the same database <code dir="ltr">RESETLOGS SCN</code>.</p>
</li>
<li>
<p>Must be from a release 8.0 or later Oracle Database. However, several of the LogMiner features introduced as of release 9.0.1 work only with redo log files produced on an Oracle9<span class="italic">i</span> or later database. See <a href="#i1031050">"Supported Databases and Redo Log File Versions"</a>.</p>
</li>
</ul>
</li>
</ul>
<p>LogMiner does not allow you to mix redo log files from different databases or to use a dictionary from a different database than the one that generated the redo log files to be analyzed.</p>
<div class="infobox-note">
<p class="notep1"><span class="bold">Note</span>:</p>
You must enable supplemental logging before generating log files that will be analyzed by LogMiner.
<p>When you enable supplemental logging, additional information is recorded in the redo stream that is needed to make the information in the redo log files useful to you. Therefore, at the very least, you must enable minimal supplemental logging, as the following SQL statement shows:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
</pre>
<p>To determine whether supplemental logging is enabled, query the <code dir="ltr">V$DATABASE</code> view, as the following SQL statement shows:</p>
<pre dir="ltr">
SELECT SUPPLEMENTAL_LOG_DATA_MIN FROM V$DATABASE;
</pre>
<p>If the query returns a value of <code dir="ltr">YES</code> or <code dir="ltr">IMPLICIT</code>, then minimal supplemental logging is enabled. See <a href="#i1021068">"Supplemental Logging"</a> for complete information about supplemental logging.</p>
</div>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="i1014380"></a>
<div id="SUTIL1557" class="sect2">
<h3 class="sect2">Directing LogMiner Operations and Retrieving Data of Interest</h3>
<p><a id="sthref1549"></a>You direct LogMiner operations using the <a id="sthref1550"></a><a id="sthref1551"></a><code dir="ltr">DBMS_LOGMNR</code> and <a id="sthref1552"></a><a id="sthref1553"></a><code dir="ltr">DBMS_LOGMNR_D</code> PL/SQL packages, and retrieve data of interest using the <a id="sthref1554"></a><a id="sthref1555"></a><code dir="ltr">V$LOGMNR_CONTENTS</code> view, as follows:</p>
<ol>
<li>
<p>Specify a LogMiner dictionary.</p>
<p>Use the <a id="sthref1556"></a><a id="sthref1557"></a><code dir="ltr">DBMS_LOGMNR_D.BUILD</code> procedure or specify the dictionary when you start LogMiner (in Step 3), or both, depending on the type of dictionary you plan to use.</p>
</li>
<li>
<p>Specify a list of redo log files for analysis.</p>
<p>Use the <a id="sthref1558"></a><a id="sthref1559"></a><code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> procedure, or direct LogMiner to create a list of log files for analysis automatically when you start LogMiner (in Step 3).</p>
</li>
<li>
<p>Start LogMiner.</p>
<p>Use the <a id="sthref1560"></a><a id="sthref1561"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure.</p>
</li>
<li>
<p>Request the redo data of interest.</p>
<p>Query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view. (You must have the <code dir="ltr">SELECT ANY TRANSACTION</code> privilege to query this view.)</p>
</li>
<li>
<p>End the LogMiner session.</p>
<p>Use the <a id="sthref1562"></a><a id="sthref1563"></a><code dir="ltr">DBMS_LOGMNR.END_LOGMNR</code> procedure.</p>
</li>
</ol>
<p>You must have been granted the <code dir="ltr">EXECUTE_CATALOG_ROLE</code> role to use the LogMiner PL/SQL packages and to query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
When mining a specified time or SCN range of interest within archived logs generated by an Oracle RAC database, you must ensure that you have specified all archived logs from all redo threads that were active during that time or SCN range. If you fail to do this, then any queries of <code dir="ltr">V$LOGMNR_CONTENTS</code> return only partial results (based on the archived logs specified to LogMiner through the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> procedure). This restriction is also in effect when you are mining the archived logs at the source database using the <code dir="ltr">CONTINUOUS_MINE</code> option. You should only use <code dir="ltr">CONTINUOUS_MINE</code> on an Oracle RAC database if no thread is being enabled or disabled.</div>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#i1009063">"Steps in a Typical LogMiner Session"</a> for an example of using LogMiner</div>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1015913"></a>
<div id="SUTIL1558" class="sect1">
<h2 class="sect1">LogMiner Dictionary Files and Redo Log Files</h2>
<p>Before you begin using LogMiner, it is important to understand how LogMiner works with the LogMiner dictionary file (or files) and redo log files. This will help you to get accurate results and to plan the use of your system resources.</p>
<p>The following concepts are discussed in this section:</p>
<ul>
<li>
<p><a href="#i1014687">LogMiner Dictionary Options</a></p>
</li>
<li>
<p><a href="#i1005688">Redo Log File Options</a></p>
</li>
</ul>
<a id="i1014687"></a>
<div id="SUTIL1559" class="sect2">
<h3 class="sect2">LogMiner Dictionary Options<a id="sthref1564"></a></h3>
<p>LogMiner requires a dictionary to translate object IDs into object names when it returns redo data to you. LogMiner gives you three options for supplying the dictionary:</p>
<ul>
<li>
<p><a href="#i1014720">Using the Online Catalog</a></p>
<p><a id="sthref1565"></a>Oracle recommends that you use this option when you will have access to the source database from which the redo log files were created and when no changes to the column definitions in the tables of interest are anticipated. This is the most efficient and easy-to-use option.</p>
</li>
<li>
<p><a href="#i1014735">Extracting a LogMiner Dictionary to the Redo Log Files</a></p>
<p><a id="sthref1566"></a>Oracle recommends that you use this option when you do not expect to have access to the source database from which the redo log files were created, or if you anticipate that changes will be made to the column definitions in the tables of interest.</p>
</li>
<li>
<p><a href="#i1014763">Extracting the LogMiner Dictionary to a Flat File</a></p>
<p><a id="sthref1567"></a>This option is maintained for backward compatibility with previous releases. This option does not guarantee transactional consistency. Oracle recommends that you use either the online catalog or extract the dictionary from redo log files instead.</p>
</li>
</ul>
<p><a href="#i1014713">Figure 19-2</a> shows a decision tree to help you select a LogMiner dictionary, depending on your situation.</p>
<div id="SUTIL3614" class="figure">
<p class="titleinfigure"><a id="i1014713"></a>Figure 19-2 Decision Tree for Choosing a LogMiner Dictionary</p>
<img width="483" height="406" src="img/decision_tree.gif" alt="Description of Figure 19-2 follows" /><br />
<a id="sthref1568" href="img_text/decision_tree.htm">Description of ''Figure 19-2 Decision Tree for Choosing a LogMiner Dictionary''</a><br />
<br /></div>
<!-- class="figure" -->
<p>The following sections provide instructions on how to specify each of the available dictionary options.</p>
<a id="i1014720"></a>
<div id="SUTIL1560" class="sect3">
<h4 class="sect3">Using the Online Catalog<a id="sthref1569"></a></h4>
<p><a id="sthref1570"></a>To direct LogMiner to use the dictionary currently in use for the database, specify the online catalog as your dictionary source when you start LogMiner, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG);
</pre>
<p>In addition to using the online catalog to analyze online redo log files, you can use it to analyze archived redo log files, if you are on the same system that generated the archived redo log files.</p>
<p>The online catalog contains the latest information about the database and may be the fastest way to start your analysis. Because DDL operations that change important tables are somewhat rare, the online catalog generally contains the information you need for your analysis.</p>
<p>Remember, however, that the online catalog can only reconstruct SQL statements that are executed on the latest version of a table. As soon as a table is altered, the online catalog no longer reflects the previous version of the table. This means that LogMiner will not be able to reconstruct any SQL statements that were executed on the previous version of the table. Instead, LogMiner generates nonexecutable SQL (including hexadecimal-to-raw formatting of binary values) in the <code dir="ltr">SQL_REDO</code> column of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view similar to the following example:</p>
<pre dir="ltr">
insert into HR.EMPLOYEES(col#1, col#2) values (hextoraw('4a6f686e20446f65'),
hextoraw('c306'));"
</pre>
<p>The online catalog option requires that the database be open.</p>
<p>The online catalog option is not valid with the <code dir="ltr">DDL_DICT_TRACKING</code> option of <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>.</p>
</div>
<!-- class="sect3" -->
<a id="i1014735"></a>
<div id="SUTIL1561" class="sect3">
<h4 class="sect3">Extracting a LogMiner Dictionary to the Redo Log Files</h4>
<p><a id="sthref1571"></a>To extract a LogMiner dictionary to the redo log files, the database must be open and in <code dir="ltr">ARCHIVELOG</code> mode and archiving must be enabled. While the dictionary is being extracted to the redo log stream, no DDL statements can be executed. Therefore, the dictionary extracted to the redo log files is guaranteed to be consistent (whereas the dictionary extracted to a flat file is not).</p>
<p>To extract dictionary information to the redo log files, execute the PL/SQL <code dir="ltr">DBMS_LOGMNR_D.BUILD</code> procedure with the <code dir="ltr">STORE_IN_REDO_LOGS</code> option. Do not specify a file name or location.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR_D.BUILD( -
   OPTIONS=&gt; DBMS_LOGMNR_D.STORE_IN_REDO_LOGS);
</pre>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<ul>
<li>
<p><a class="olink BRADV" href="../../backup.112/e10642/toc.htm"><span class="italic">Oracle Database Backup and Recovery User's Guide</span></a> for more information about <code dir="ltr">ARCHIVELOG</code> mode</p>
</li>
<li>
<p><a class="olink ARPLS" href="../../appdev.112/e40758/toc.htm"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for a complete description of <code dir="ltr">DBMS_LOGMNR_D.BUILD</code></p>
</li>
</ul>
</div>
<p>The process of extracting the dictionary to the redo log files does consume database resources, but if you limit the extraction to off-peak hours, then this should not be a problem, and it is faster than extracting to a flat file. Depending on the size of the dictionary, it may be contained in multiple redo log files. If the relevant redo log files have been archived, then you can find out which redo log files contain the start and end of an extracted dictionary. To do so, query the <code dir="ltr">V$ARCHIVED_LOG</code> view, as follows:</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG WHERE DICTIONARY_BEGIN='YES';
SELECT NAME FROM V$ARCHIVED_LOG WHERE DICTIONARY_END='YES';
</pre>
<p>Specify the names of the start and end redo log files, and possibly other logs in between them, with the <code dir="ltr">ADD_LOGFILE</code> procedure when you are preparing to begin a LogMiner session.</p>
<p>Oracle recommends that you periodically back up the redo log files so that the information is saved and available at a later date. Ideally, this will not involve any extra steps because if your database is being properly managed, then there should already be a process in place for backing up and restoring archived redo log files. Again, because of the time required, it is good practice to do this during off-peak hours.</p>
</div>
<!-- class="sect3" -->
<a id="i1014763"></a>
<div id="SUTIL1562" class="sect3">
<h4 class="sect3">Extracting the LogMiner Dictionary to a Flat File</h4>
<p><a id="sthref1572"></a>When the LogMiner dictionary is in a flat file, fewer system resources are used than when it is contained in the redo log files. Oracle recommends that you regularly back up the dictionary extract to ensure correct analysis of older redo log files.</p>
<p>To extract database dictionary information to a flat file, use the <code dir="ltr">DBMS_LOGMNR_D.BUILD</code> procedure with the <code dir="ltr">STORE_IN_FLAT_FILE</code> option.</p>
<p>Be sure that no DDL operations occur while the dictionary is being built.</p>
<p>The following steps describe how to extract a dictionary to a flat file. Steps 1 and 2 are preparation steps. You only need to do them once, and then you can extract a dictionary to a flat file as many times as you want to.</p>
<ol>
<li>
<p>The <code dir="ltr">DBMS_LOGMNR_D.BUILD</code> procedure requires access to a directory where it can place the dictionary file. Because PL/SQL procedures do not normally access user directories, you must specify a directory for use by the <code dir="ltr">DBMS_LOGMNR_D.BUILD</code> procedure or the procedure will fail. To specify a directory, set the initialization parameter, <code dir="ltr">UTL_FILE_DIR</code>, in the initialization parameter file.</p>
<p>For example, to set <code dir="ltr">UTL_FILE_DIR</code> to use <code dir="ltr">/oracle/database</code> as the directory where the dictionary file is placed, place the following in the initialization parameter file:</p>
<pre dir="ltr">
UTL_FILE_DIR = /oracle/database
</pre>
<p>Remember that for the changes to the initialization parameter file to take effect, you must stop and restart the database.</p>
</li>
<li>
<p>If the database is closed, then use SQL*Plus to mount and open the database whose redo log files you want to analyze. For example, entering the SQL <code dir="ltr">STARTUP</code> command mounts and opens the database:</p>
<pre dir="ltr">
STARTUP
</pre></li>
<li>
<p>Execute the PL/SQL procedure <code dir="ltr">DBMS_LOGMNR_D.BUILD</code>. Specify a file name for the dictionary and a directory path name for the file. This procedure creates the dictionary file. For example, enter the following to create the file <code dir="ltr">dictionary.ora</code> in <code dir="ltr">/oracle/database</code>:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR_D.BUILD('dictionary.ora', - 
   '/oracle/database/', -
    DBMS_LOGMNR_D.STORE_IN_FLAT_FILE);
</pre>
<p>You could also specify a file name and location without specifying the <code dir="ltr">STORE_IN_FLAT_FILE</code> option. The result would be the same.</p>
</li>
</ol>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="i1005688"></a>
<div id="SUTIL1563" class="sect2">
<h3 class="sect2">Redo Log File Options</h3>
<p><a id="sthref1573"></a><a id="sthref1574"></a>To mine data in the redo log files, LogMiner needs information about which redo log files to mine. Changes made to the database that are found in these redo log files are delivered to you through the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<p>You can direct LogMiner to automatically and dynamically create a list of redo log files to analyze, or you can explicitly specify a list of redo log files for LogMiner to analyze, as follows:</p>
<ul>
<li>
<p>Automatically</p>
<p><a id="sthref1575"></a>If LogMiner is being used on the source database, then you can direct LogMiner to find and create a list of redo log files for analysis automatically. Use the <code dir="ltr">CONTINUOUS_MINE</code> option when you start LogMiner with the <code dir="ltr">DBMS_<a id="sthref1576"></a>LOGMNR.START_LOGMNR</code> procedure, and specify a time or SCN range. Although this example specifies the dictionary from the online catalog, any LogMiner dictionary can be used.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
The <code dir="ltr">CONTINUOUS_MINE</code> option requires that the database be mounted and that archiving be enabled.</div>
<p>LogMiner will use the database control file to find and add redo log files that satisfy your specified time or SCN range to the LogMiner redo log file list. For example:</p>
<pre dir="ltr">
ALTER SESSION SET NLS_DATE_FORMAT = 'DD-MON-YYYY HH24:MI:SS';
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   STARTTIME =&gt; '01-Jan-2003 08:30:00', -
   ENDTIME =&gt; '01-Jan-2003 08:45:00', -
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
   DBMS_LOGMNR.CONTINUOUS_MINE);
</pre>
<p>(To avoid the need to specify the date format in the PL/SQL call to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure, this example uses the SQL <code dir="ltr">ALTER</code> <code dir="ltr">SESSION SET</code> <code dir="ltr">NLS_DATE_FORMAT</code> statement first.)</p>
<p>You can also direct LogMiner to automatically build a list of redo log files to analyze by specifying just one redo log file using <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code>, and then specifying the <code dir="ltr">CONTINUOUS_MINE</code> option when you start LogMiner. The previously described method is more typical, however.</p>
</li>
<li>
<p>Manually</p>
<p><a id="sthref1577"></a>Use the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> procedure to manually create a list of redo log files before you start LogMiner. After the first redo log file has been added to the list, each subsequently added redo log file must be from the same database and associated with the same database RESETLOGS SCN. When using this method, LogMiner need not be connected to the source database.</p>
<p>For example, to start a new list of redo log files, specify the <code dir="ltr">NEW</code> option of the <code dir="ltr"><a id="sthref1578"></a>DBMS_LOGMNR.ADD_LOGFILE</code> PL/SQL procedure to signal that this is the beginning of a new list. For example, enter the following to specify <code dir="ltr">/oracle/logs/log1.f</code>:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/oracle/logs/log1.f', -
   OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre>
<p>If desired, add more redo log files by specifying the <code dir="ltr"><a id="sthref1579"></a>ADDFILE</code> option of the <code dir="ltr">PL/SQL DBMS_LOGMNR.ADD_LOGFILE</code> procedure. For example, enter the following to add <code dir="ltr">/oracle/logs/log2.f</code>:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/oracle/logs/log2.f', -
   OPTIONS =&gt; DBMS_LOGMNR.ADDFILE);
</pre>
<p><a id="sthref1580"></a><a id="sthref1581"></a>To determine which redo log files are being analyzed in the current LogMiner session, you can query the <code dir="ltr">V$LOGMNR_LOGS</code> view, which contains one row for each redo log file.</p>
</li>
</ul>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1014404"></a>
<div id="SUTIL1564" class="sect1">
<h2 class="sect1">Starting LogMiner</h2>
<p><a id="sthref1582"></a><a id="sthref1583"></a>You call the <a id="sthref1584"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure to start LogMiner. Because the options available with the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure allow you to control output to the <a id="sthref1585"></a><code dir="ltr">V$LOGMNR_CONTENTS</code> view, you must call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> before querying the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<p>When you start LogMiner, you can:</p>
<ul>
<li>
<p>Specify how LogMiner should filter data it returns (for example, by starting and ending time or SCN value)</p>
</li>
<li>
<p>Specify options for formatting the data returned by LogMiner</p>
</li>
<li>
<p>Specify the LogMiner dictionary to use</p>
</li>
</ul>
<p>The following list is a summary of LogMiner settings that you can specify with the <code dir="ltr">OPTIONS</code> parameter to <a id="sthref1586"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> and where to find more information about them.</p>
<ul>
<li>
<p><code dir="ltr">DICT_FROM_ONLINE_CATALOG</code> &mdash; See <a href="#i1014720">"Using the Online Catalog"</a></p>
</li>
<li>
<p><code dir="ltr">DICT_FROM_REDO_LOGS</code> &mdash; See <a href="#i1006391">"Start LogMiner"</a></p>
</li>
<li>
<p><code dir="ltr">CONTINUOUS_MINE</code> &mdash; See <a href="#i1005688">"Redo Log File Options"</a></p>
</li>
<li>
<p><code dir="ltr">COMMITTED_DATA_ONLY</code> &mdash; See <a href="#i1016732">"Showing Only Committed Transactions"</a></p>
</li>
<li>
<p><code dir="ltr">SKIP_CORRUPTION</code> &mdash; See <a href="#i1016321">"Skipping Redo Corruptions"</a></p>
</li>
<li>
<p><code dir="ltr">NO_SQL_DELIMITER</code> &mdash; See <a href="#i1021032">"Formatting Reconstructed SQL Statements for Re-execution"</a></p>
</li>
<li>
<p><code dir="ltr">PRINT_PRETTY_SQL</code> &mdash; See <a href="#i1021043">"Formatting the Appearance of Returned Data for Readability"</a></p>
</li>
<li>
<p><code dir="ltr">NO_ROWID_IN_STMT</code> &mdash; See <a href="#i1021032">"Formatting Reconstructed SQL Statements for Re-execution"</a></p>
</li>
<li>
<p><code dir="ltr">DDL_DICT_TRACKING</code> &mdash; See <a href="#i1039093">"Tracking DDL Statements in the LogMiner Dictionary"</a></p>
</li>
</ul>
<p>When you execute the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure, LogMiner checks to ensure that the combination of options and parameters that you have specified is valid and that the dictionary and redo log files that you have specified are available. However, the <code dir="ltr">V$LOGMNR_CONTENTS</code> view is not populated until you query the view, as described in <a href="#i1016853">"How the V$LOGMNR_CONTENTS View Is Populated"</a>.</p>
<p>Note that parameters and options are not persistent across calls to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>. You must specify all desired parameters and options (including SCN and time ranges) each time you call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>.</p>
</div>
<!-- class="sect1" -->
<a id="i1016535"></a>
<div id="SUTIL1565" class="sect1">
<h2 class="sect1">Querying V$LOGMNR_CONTENTS for Redo Data of Interest</h2>
<p>You access the <a id="sthref1587"></a>redo data of interest by querying the <a id="sthref1588"></a><code dir="ltr">V$LOGMNR_CONTENTS</code> view. (Note that you must have the <code dir="ltr">SELECT ANY TRANSACTION</code> privilege to query <code dir="ltr">V$LOGMNR_CONTENTS</code>.) This view provides historical information about changes made to the database, including (but not limited to) the following:</p>
<ul>
<li>
<p>The type of change made to the database: <code dir="ltr">INSERT</code>, <code dir="ltr">UPDATE</code>, <code dir="ltr">DELETE</code>, or <code dir="ltr">DDL</code> (<code dir="ltr">OPERATION</code> column).</p>
</li>
<li>
<p>The SCN at which a change was made (<code dir="ltr">SCN</code> column).</p>
</li>
<li>
<p>The SCN at which a change was committed (<code dir="ltr">COMMIT_SCN</code> column).</p>
</li>
<li>
<p>The transaction to which a change belongs (<code dir="ltr">XIDUSN</code>, <code dir="ltr">XIDSLT</code>, and <code dir="ltr">XIDSQN</code> columns).</p>
</li>
<li>
<p>The table and schema name of the modified object (<code dir="ltr">SEG_NAME</code> and <code dir="ltr">SEG_OWNER</code> columns).</p>
</li>
<li>
<p>The name of the user who issued the DDL or DML statement to make the change (<code dir="ltr">USERNAME</code> column).</p>
</li>
<li>
<p>If the change was due to a SQL DML statement, the reconstructed SQL statements showing SQL DML that is equivalent (but not necessarily identical) to the SQL DML used to generate the redo records (<code dir="ltr">SQL_REDO</code> column).</p>
</li>
<li>
<p>If a password is part of the statement in a <code dir="ltr">SQL_REDO</code> column, then the password is encrypted. <code dir="ltr">SQL_REDO</code> column values that correspond to DDL statements are always identical to the SQL DDL used to generate the redo records.</p>
</li>
<li>
<p>If the change was due to a SQL DML change, the reconstructed SQL statements showing the SQL DML statements needed to undo the change (<code dir="ltr">SQL_UNDO</code> column).</p>
<p><code dir="ltr">SQL_UNDO</code> columns that correspond to DDL statements are always <code dir="ltr">NULL</code>. The <code dir="ltr">SQL_UNDO</code> column may be <code dir="ltr">NULL</code> also for some datatypes and for rolled back operations.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
LogMiner supports<a id="sthref1589"></a> Oracle Advanced Security <a id="sthref1590"></a>transparent data encryption (TDE) in that <code dir="ltr">V$LOGMNR_CONTENTS</code> shows DML operations performed on tables with encrypted columns (including the encrypted columns being updated), provided the LogMiner data dictionary contains the metadata for the object in question and provided the appropriate master key is in the Oracle wallet. The wallet must be open or <code dir="ltr">V$LOGMNR_CONTENTS</code> cannot interpret the associated redo records. TDE support is not available if the database is not open (either read-only or read-write). See <a class="olink ASOAG600" href="../../network.112/e40393/asotrans.htm#ASOAG600"><span class="italic">Oracle Database Advanced Security Administrator's Guide</span></a> for more information about transparent data encryption.</div>
<p class="subhead1"><a id="SUTIL3615"></a>Example of Querying V$LOGMNR_CONTENTS</p>
<p>Suppose you wanted to find out about any delete operations that a user named Ron had performed on the <code dir="ltr">oe.orders</code> table. You could issue a SQL query similar to the following:</p>
<pre dir="ltr">
SELECT OPERATION, SQL_REDO, SQL_UNDO
   FROM V$LOGMNR_CONTENTS
   WHERE SEG_OWNER = 'OE' AND SEG_NAME = 'ORDERS' AND
   OPERATION = 'DELETE' AND USERNAME = 'RON';
</pre>
<p>The following output would be produced. The formatting may be different on your display than that shown here.</p>
<pre dir="ltr">
OPERATION   SQL_REDO                        SQL_UNDO

DELETE      delete from "OE"."ORDERS"       insert into "OE"."ORDERS"        
            where "ORDER_ID" = '2413'       ("ORDER_ID","ORDER_MODE",
            and "ORDER_MODE" = 'direct'      "CUSTOMER_ID","ORDER_STATUS",
            and "CUSTOMER_ID" = '101'        "ORDER_TOTAL","SALES_REP_ID",
            and "ORDER_STATUS" = '5'         "PROMOTION_ID")
            and "ORDER_TOTAL" = '48552'      values ('2413','direct','101',
            and "SALES_REP_ID" = '161'       '5','48552','161',NULL);     
            and "PROMOTION_ID" IS NULL  
            and ROWID = 'AAAHTCAABAAAZAPAAN';

DELETE      delete from "OE"."ORDERS"        insert into "OE"."ORDERS"
            where "ORDER_ID" = '2430'        ("ORDER_ID","ORDER_MODE",
            and "ORDER_MODE" = 'direct'       "CUSTOMER_ID","ORDER_STATUS",
            and "CUSTOMER_ID" = '101'         "ORDER_TOTAL","SALES_REP_ID",
            and "ORDER_STATUS" = '8'          "PROMOTION_ID")
            and "ORDER_TOTAL" = '29669.9'     values('2430','direct','101',
            and "SALES_REP_ID" = '159'        '8','29669.9','159',NULL);
            and "PROMOTION_ID" IS NULL 
            and ROWID = 'AAAHTCAABAAAZAPAAe';
</pre>
<p>This output shows that user Ron deleted two rows from the <code dir="ltr">oe.orders</code> table. The reconstructed SQL statements are equivalent, but not necessarily identical, to the actual statement that Ron issued. The reason for this is that the original <code dir="ltr">WHERE</code> clause is not logged in the redo log files, so LogMiner can only show deleted (or updated or inserted) rows individually.</p>
<p>Therefore, even though a single <code dir="ltr">DELETE</code> statement may have been responsible for the deletion of both rows, the output in <code dir="ltr">V$LOGMNR_CONTENTS</code> does not reflect that. Thus, the actual <code dir="ltr">DELETE</code> statement may have been <code dir="ltr">DELETE FROM OE.ORDERS WHERE CUSTOMER_ID ='101</code>' or it might have been <code dir="ltr">DELETE FROM OE.ORDERS WHERE PROMOTION_ID = NULL.</code></p>
<a id="i1016853"></a>
<div id="SUTIL1566" class="sect2">
<h3 class="sect2">How the V$LOGMNR_CONTENTS View Is Populated</h3>
<p><a id="sthref1591"></a>The <code dir="ltr">V$LOGMNR_CONTENTS</code> fixed view is unlike other views in that it is not a selective presentation of data stored in a table. Instead, it is a relational presentation of the data that you request from the redo log files. LogMiner populates the view only in response to a query against it. <a id="sthref1592"></a><a id="sthref1593"></a><a id="sthref1594"></a>You must successfully start LogMiner before you can query <code dir="ltr">V$LOGMNR_CONTENTS.</code></p>
<p>When a SQL select operation is executed against the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, the redo log files are read sequentially. Translated information from the redo log files is returned as rows in the <code dir="ltr">V$LOGMNR_CONTENTS</code> view. This continues until either the filter criteria specified at startup are met or the end of the redo log file is reached.</p>
<p>In some cases, certain columns in <code dir="ltr">V$LOGMNR_CONTENTS</code> may not be populated. For example:</p>
<ul>
<li>
<p>The <code dir="ltr">TABLE_SPACE</code> column is not populated for rows where the value of the <code dir="ltr">OPERATION</code> column is <code dir="ltr">DDL</code>. This is because a DDL may operate on more than one tablespace. For example, a table can be created with multiple partitions spanning multiple table spaces; hence it would not be accurate to populate the column.</p>
</li>
<li>
<p>LogMiner does not generate SQL redo or SQL undo for temporary tables. The <code dir="ltr">SQL_REDO</code> column will contain the string <code dir="ltr">"/* No SQL_REDO for temporary tables */"</code> and the <code dir="ltr">SQL_UNDO</code> column will contain the string <code dir="ltr">"/* No SQL_UNDO for temporary tables */"</code>.</p>
</li>
</ul>
<p>LogMiner returns all the rows in SCN order unless you have used the <code dir="ltr">COMMITTED_DATA_ONLY</code> option to specify that only committed transactions should be retrieved. SCN order is the order normally applied in media recovery.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#i1016732">"Showing Only Committed Transactions"</a> for more information about the <code dir="ltr">COMMITTED_DATA_ONLY</code> option to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code></div>
<div class="infobox-note">
<p class="notep1">Note:</p>
Bec<a id="sthref1595"></a>ause LogMiner populates the <code dir="ltr">V$LOGMNR_CONTENTS</code> view only in response to a query and does not store the requested data in the database, the following is true:
<ul>
<li>
<p>Every time you query <code dir="ltr">V$LOGMNR_CONTENTS</code>, LogMiner analyzes the redo log files for the data you request.</p>
</li>
<li>
<p>The amount of memory consumed by the query is not dependent on the number of rows that must be returned to satisfy a query.</p>
</li>
<li>
<p>The time it takes to return the requested data is dependent on the amount and type of redo log data that must be mined to find that data.</p>
</li>
</ul>
</div>
<p>For the reasons stated in the previous note, Oracle recommends that you create a table to temporarily hold the results from a query of <code dir="ltr">V$LOGMNR_CONTENTS</code> if you need to maintain the data for further analysis, particularly if the amount of data returned by a query is small in comparison to the amount of redo data that LogMiner must analyze to provide that data.</p>
</div>
<!-- class="sect2" -->
<a id="i1016607"></a>
<div id="SUTIL1567" class="sect2">
<h3 class="sect2">Querying V$LOGMNR_CONTENTS Based on Column Values</h3>
<p><a id="sthref1596"></a>LogMiner lets you make queries based on column values. For instance, you can perform a query to show all updates to the <code dir="ltr">hr.employees</code> table that increase <code dir="ltr">salary</code> more than a certain amount. Data such as this can be used to analyze system behavior and to perform auditing tasks.</p>
<p>LogMiner data extraction from redo log files is performed using two mine functions: <a id="sthref1597"></a><code dir="ltr">DBMS_LOGMNR.MINE_VALUE</code> and <a id="sthref1598"></a><code dir="ltr">DBMS_LOGMNR.COLUMN_PRESENT</code>. Support for these mine functions is provided by the <code dir="ltr">REDO_VALUE</code> and <code dir="ltr">UNDO_VALUE</code> columns in the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<p>The following is an example of how you could use the <code dir="ltr">MINE_VALUE</code> function to select all updates to <code dir="ltr">hr.employees</code> that increased the <code dir="ltr">salary</code> column to more than twice its original value:</p>
<pre dir="ltr">
SELECT SQL_REDO FROM V$LOGMNR_CONTENTS
   WHERE
   SEG_NAME = 'EMPLOYEES' AND
   SEG_OWNER = 'HR' AND
   OPERATION = 'UPDATE' AND
   DBMS_LOGMNR.MINE_VALUE(REDO_VALUE, 'HR.EMPLOYEES.SALARY') &gt;
   2*DBMS_LOGMNR.MINE_VALUE(UNDO_VALUE, 'HR.EMPLOYEES.SALARY');
</pre>
<p>As shown in this example, the <code dir="ltr">MINE_VALUE</code> function takes two arguments:</p>
<ul>
<li>
<p>The first one specifies whether to mine the redo (<code dir="ltr">REDO_VALUE</code>) or undo (<code dir="ltr">UNDO_VALUE</code>) portion of the data. The redo portion of the data is the data that is in the column after an insert, update, or delete operation; the undo portion of the data is the data that was in the column before an insert, update, or delete operation. It may help to think of the <code dir="ltr">REDO_VALUE</code> as the new value and the <code dir="ltr">UNDO_VALUE</code> as the old value.</p>
</li>
<li>
<p>The second argument is a string that specifies the fully qualified name of the column to be mined (in this case, <code dir="ltr">hr.employees.salary</code>). The <code dir="ltr">MINE_VALUE</code> function always returns a string that can be converted back to the original datatype.</p>
</li>
</ul>
<a id="i1016622"></a>
<div id="SUTIL1568" class="sect3">
<h4 class="sect3">The Meaning of NULL Values Returned by the MINE_VALUE Function</h4>
<p><a id="sthref1599"></a>If the <code dir="ltr">MINE_VALUE</code> function returns a <code dir="ltr">NULL</code> value, then it can mean either:</p>
<ul>
<li>
<p>The specified column is not present in the redo or undo portion of the data.</p>
</li>
<li>
<p>The specified column is present and has a null value.</p>
</li>
</ul>
<p>To distinguish between these two cases, use the <code dir="ltr">DBMS_LOGMNR</code>.<code dir="ltr">COLUMN_PRESENT</code> function which returns a <code dir="ltr">1</code> if the column is present in the redo or undo portion of the data. Otherwise, it returns a <code dir="ltr">0</code>. For example, suppose you wanted to find out the increment by which the values in the <code dir="ltr">salary</code> column were modified and the corresponding transaction identifier. You could issue the following SQL query:</p>
<pre dir="ltr">
SELECT 
  (XIDUSN || '.' || XIDSLT || '.' || XIDSQN) AS XID,
  (DBMS_LOGMNR.MINE_VALUE(REDO_VALUE, 'HR.EMPLOYEES.SALARY') -
   DBMS_LOGMNR.MINE_VALUE(UNDO_VALUE, 'HR.EMPLOYEES.SALARY')) AS INCR_SAL
   FROM V$LOGMNR_CONTENTS
   WHERE
   OPERATION = 'UPDATE' AND
   DBMS_LOGMNR.COLUMN_PRESENT(REDO_VALUE, 'HR.EMPLOYEES.SALARY') = 1 AND
   DBMS_LOGMNR.COLUMN_PRESENT(UNDO_VALUE, 'HR.EMPLOYEES.SALARY') = 1;
</pre></div>
<!-- class="sect3" -->
<a id="i1016637"></a>
<div id="SUTIL1569" class="sect3">
<h4 class="sect3">Usage Rules for the MINE_VALUE and COLUMN_PRESENT Functions</h4>
<p>The following usage rules apply to the <code dir="ltr">MINE_VALUE</code> and <code dir="ltr">COLUMN_PRESENT</code> functions:</p>
<ul>
<li>
<p>They can only be used within a LogMiner session.</p>
</li>
<li>
<p>They must be invoked in the context of a select operation from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
</li>
<li>
<p>They do not support <code dir="ltr">LONG</code>, <code dir="ltr">LONG RAW,</code> <code dir="ltr">CLOB,</code> <code dir="ltr">BLOB,</code> <code dir="ltr">NCLOB</code>, <code dir="ltr">ADT</code>, or <code dir="ltr">COLLECTION</code> datatypes.</p>
</li>
</ul>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<div id="SUTIL1570" class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1600"></a>
<h3 class="sect2">Querying V$LOGMNR_CONTENTS Based on XMLType Columns and Tables</h3>
<p>LogMiner supports redo generated for <code dir="ltr">XMLType</code> columns. <code dir="ltr">XMLType</code> data stored as <code dir="ltr">CLOB</code> is supported when redo is generated at a compatibility setting of 11.0.0.0 or higher. <code dir="ltr">XMLType</code> data stored as object-relational and binary XML is supported for redo generated at a compatibility setting of 11.2.0.3 and higher.</p>
<p>LogMiner presents the <code dir="ltr">SQL_REDO</code> in <code dir="ltr">V$LOGMNR_CONTENTS</code> in different ways depending on the <code dir="ltr">XMLType</code> storage. In all cases, the contents of the <code dir="ltr">SQL_REDO</code> column, in combination with the <code dir="ltr">STATUS</code> column, require careful scrutiny, and usually require reassembly before a SQL or PL/SQL statement can be generated to redo the change. There may be cases when it is not possible to use the <code dir="ltr">SQL_REDO</code> data to construct such a change. The examples in the following subsections are based on <code dir="ltr">XMLType</code> stored as <code dir="ltr">CLOB</code> which is generally the simplest to use for reconstruction of the complete row change.</p>
<p class="subhead1"><a id="SUTIL3616"></a>Querying V$LOGMNR_CONTENTS For Changes to Tables With XMLType Columns</p>
<p>The example in this section is for a table named <code dir="ltr">XML_CLOB_COL_TAB</code> that has the following columns:</p>
<ul>
<li>
<p>f1 <code dir="ltr">NUMBER</code></p>
</li>
<li>
<p>f2 <code dir="ltr">VARCHAR2(100)</code></p>
</li>
<li>
<p>f3 <code dir="ltr">XMLTYPE</code></p>
</li>
<li>
<p>f4 <code dir="ltr">XMLTYPE</code></p>
</li>
<li>
<p>f5 <code dir="ltr">VARCHAR2(10)</code></p>
</li>
</ul>
<p>Assume that a LogMiner session has been started with the logs and with the <code dir="ltr">COMMITED_DATA_ONLY</code> option. The following query is executed against <code dir="ltr">V$LOGMNR_CONTENTS</code> for changes to the <code dir="ltr">XML_CLOB_COL_TAB</code> table.</p>
<pre dir="ltr">
SELECT OPERATION, STATUS, SQL_REDO FROM V$LOGMNR_CONTENTS 
  WHERE SEG_OWNER = 'SCOTT' AND TABLE_NAME = 'XML_CLOB_COL_TAB';
</pre>
<p>The query output looks similar to the following:</p>
<pre dir="ltr">
OPERATION         STATUS  SQL_REDO

INSERT            0       insert into "SCOTT"."XML_CLOB_COL_TAB"("F1","F2","F5") values
                             ('5010','Aho40431','PETER')
         
XML DOC BEGIN     5       update "SCOTT"."XML_CLOB_COL_TAB" a set a."F3" = XMLType(:1)
                             where a."F1" = '5010' and a."F2" = 'Aho40431' and a."F5" = 'PETER'

XML DOC WRITE     5       XML Data

XML DOC WRITE     5       XML Data

XML DOC WRITE     5       XML Data

XML DOC END       5
                                                                  
</pre>
<p>In the <code dir="ltr">SQL_REDO</code> columns for the <code dir="ltr">XML DOC WRITE</code> operations there will be actual data for the XML document. It will not be the string 'XML Data'.</p>
<p>This output shows that the general model for an insert into a table with an <code dir="ltr">XMLType</code> column is the following:</p>
<ol>
<li>
<p>An initial insert with all of the scalar columns.</p>
</li>
<li>
<p>An <code dir="ltr">XML DOC BEGIN</code> operation with an update statement that sets the value for one <code dir="ltr">XMLType</code> column using a bind variable.</p>
</li>
<li>
<p>One or more <code dir="ltr">XML DOC WRITE</code> operations with the data for the XML document.</p>
</li>
<li>
<p>An <code dir="ltr">XML DOC END</code> operation to indicate that all of the data for that XML document has been seen.</p>
</li>
<li>
<p>If there is more than one <code dir="ltr">XMLType</code> column in the table, then steps 2 through 4 will be repeated for each <code dir="ltr">XMLType</code> column that is modified by the original DML.</p>
</li>
</ol>
<p>If the XML document is not stored as an out-of-line column, then there will be no <code dir="ltr">XML DOC BEGIN</code>, <code dir="ltr">XML DOC WRITE</code>, or <code dir="ltr">XML DOC END</code> operations for that column. The document will be included in an update statement similar to the following:</p>
<pre dir="ltr">
OPERATION   STATUS         SQL_REDO

UPDATE      0              update "SCOTT"."XML_CLOB_COL_TAB" a
                           set a."F3" = XMLType('&lt;?xml version="1.0"?&gt;
                           &lt;PO pono="1"&gt;
                           &lt;PNAME&gt;Po_99&lt;/PNAME&gt; 
                           &lt;CUSTNAME&gt;Dave Davids&lt;/CUSTNAME&gt; 
                           &lt;/PO&gt;') 
                           where a."F1" = '5006' and a."F2" = 'Janosik' and a."F5" = 'MMM' 
</pre>
<p class="subhead1"><a id="SUTIL3617"></a>Queries V$LOGMNR_CONTENTS For Changes to XMLType Tables</p>
<p>DMLs to <code dir="ltr">XMLType</code> tables are slightly different from DMLs to <code dir="ltr">XMLType</code> columns. The XML document represents the value for the row in the <code dir="ltr">XMLType</code> table. Unlike the <code dir="ltr">XMLType</code> column case, an initial insert cannot be done which is then followed by an update containing the XML document. Rather, the whole document must be assembled before anything can be inserted into the table.</p>
<p>Another difference for <code dir="ltr">XMLType</code> tables is the presence of the <code dir="ltr">OBJECT_ID</code> column. An object identifier is used to uniquely identify every object in an object table. For <code dir="ltr">XMLType</code> tables stored as CLOBs, this value is generated by Oracle Database when the row is inserted into the table. The <code dir="ltr">OBJECT_ID</code> value cannot be directly inserted into the table using SQL. Therefore, LogMiner cannot generate <code dir="ltr">SQL_REDO</code> which is executable that includes this value.</p>
<p>The <code dir="ltr">V$LOGMNR_CONTENTS</code> view has a new <code dir="ltr">OBJECT_ID</code> column which is populated for changes to <code dir="ltr">XMLType</code> tables. This value is the object identifier from the original table. However, even if this same XML document is inserted into the same <code dir="ltr">XMLType</code> table, a new object identifier will be generated. The <code dir="ltr">SQL_REDO</code> for subsequent DMLs, such as updates and deletes, on the <code dir="ltr">XMLType</code> table will include the object identifier in the <code dir="ltr">WHERE</code> clause to uniquely identify the row from the original table.</p>
<p>The following shows an example of mining changes to an <code dir="ltr">XMLType</code> table stored as CLOB:</p>
<pre dir="ltr">
select operation, status, object_id, sql_redo from v$logmnr_contents 
where  seg_owner = 'SCOTT' and table_name = 'XML_TYPE_CLOB_TAB';
</pre>
<pre dir="ltr">
OPERATION     STATUS   OBJECT_ID                         SQL_REDO

INSERT          2      300A9394B0F7B2D0E040578CF5025CC3  insert into "SCOTT"."XML_TYPE_CLOB_TAB"
                                                           values(EMPTY_CLOB()) 

XML DOC BEGIN   5      300A9394B0F7B2D0E040578CF5025CC3  insert into "SCOTT"."XML_TYPE_CLOB_TAB"
                                                           values (XMLType(:1)

XML DOC WRITE   5      300A9394B0F7B2D0E040578CF5025CC3  XML Data

XML DOC WRITE   5      300A9394B0F7B2D0E040578CF5025CC3  XML Data

XML DOC WRITE   5      300A9394B0F7B2D0E040578CF5025CC3  XML Data

XML DOC END     5
</pre>
<p>The general pattern is very similar to <code dir="ltr">XMLType</code> columns. However, there are a few key differences. The first is that now the <code dir="ltr">OBJECT_ID</code> column is populated. The second difference is that there is an initial insert, but its status is 2 for <code dir="ltr">INVALID_SQL</code>. This indicates that this record occurs in the redo as a placeholder for the change to come, but that the SQL generated for this change should not be applied. The <code dir="ltr">SQL_REDO</code> from the <code dir="ltr">XML DOC BEGIN</code> operation reflects the changes that were made to the row when used with the assembled XML document.</p>
<p>If the XML document is not stored as an out-of-line column, then there will be no <code dir="ltr">XML DOC BEGIN</code>, <code dir="ltr">XML DOC WRITE</code>, or <code dir="ltr">XML DOC END</code> operations for that document. The document will be included in an <code dir="ltr">INSERT</code> statement similar to the following:</p>
<pre dir="ltr">
OPERATION   STATUS  OBJECT_ID                           SQL_REDO

INSERT      2       300AD8CECBA75ACAE040578CF502640C    insert into "SCOTT"."XML_TYPE_CLOB_TAB"
                                                           values (EMPTY_CLOB())

INSERT      0       300AD8CECBA75ACAE040578CF502640C    insert into "SCOTT"."XML_TYPE_CLOB_TAB"
                                                           values (XMLType(
                                                           '&lt;?xml version="1.0"?&gt;
                                                           &lt;PO pono="1"&gt;
                                                           &lt;PNAME&gt;Po_99&lt;/PNAME&gt;
                                                           &lt;CUSTNAME&gt;
                                                           Dave Davids
                                                           &lt;/CUSTNAME&gt;
                                                           &lt;/PO&gt;'))
</pre>
<div id="SUTIL1571" class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1601"></a>
<h4 class="sect3">Restrictions When Using LogMiner With XMLType Data<a id="sthref1602"></a></h4>
<p>Mining <code dir="ltr">XMLType</code> data should only be done when using the <code dir="ltr">DBMS_LOGMNR.COMMITTED_DATA_ONLY</code> option. Otherwise, incomplete changes could be displayed or changes which should be displayed as XML might be displayed as CLOB changes due to missing parts of the row change. This can lead to incomplete and invalid <code dir="ltr">SQL_REDO</code> for these SQL DML statements.</p>
<p>The <code dir="ltr">SQL_UNDO</code> column is not populated for changes to <code dir="ltr">XMLType</code> data.</p>
</div>
<!-- class="sect3" -->
<div id="SUTIL1572" class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1603"></a>
<h4 class="sect3">Example of a PL/SQL Procedure for Assembling XMLType Data</h4>
<p>The example presented in this section shows a procedure that can be used to mine and assemble XML redo for tables that contain out of line XML data. This shows how to assemble the XML data using a temporary LOB. Once the XML document is assembled, it can be used in a meaningful way. This example queries the assembled document for the <code dir="ltr">EmployeeName</code> element and then stores the returned name, the XML document and the <code dir="ltr">SQL_REDO</code> for the original DML in the <code dir="ltr">EMPLOYEE_XML_DOCS</code> table.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
This procedure is an example only and is simplified. It is only intended to illustrate that DMLs to tables with XMLType data can be mined and assembled using LogMiner.</div>
<p>Before calling this procedure, all of the relevant logs must be added to a LogMiner session and <code dir="ltr">DBMS_LOGMNR.START_LOGMNR()</code> must be called with the <code dir="ltr">COMMITTED_DATA_ONLY</code> option. The <code dir="ltr">MINE_AND_ASSEMBLE()</code> procedure can then be called with the schema and table name of the table that has XML data to be mined.</p>
<pre dir="ltr">
-- table to store assembled XML documents
create table employee_xml_docs  (
  employee_name         varchar2(100),
  sql_stmt                     varchar2(4000),
  xml_doc                     SYS.XMLType);
        
-- procedure to assemble the XML documents
create or replace procedure mine_and_assemble(
  schemaname        in varchar2,
  tablename         in varchar2)
AS
  loc_c      CLOB; 
  row_op     VARCHAR2(100); 
  row_status NUMBER; 
  stmt       VARCHAR2(4000);
  row_redo   VARCHAR2(4000);
  xml_data   VARCHAR2(32767 CHAR); 
  data_len   NUMBER; 
  xml_lob    clob;
  xml_doc    XMLType;
BEGIN 
 
-- Look for the rows in V$LOGMNR_CONTENTS that are for the appropriate schema 
-- and table name but limit it to those that are valid sql or that need assembly
-- because they are XML documents.
 
 For item in ( SELECT operation, status, sql_redo  FROM v$logmnr_contents
 where seg_owner = schemaname and table_name = tablename
 and status IN (DBMS_LOGMNR.VALID_SQL, DBMS_LOGMNR.ASSEMBLY_REQUIRED_SQL))
 LOOP
    row_op := item.operation;
    row_status := item.status;
    row_redo := item.sql_redo;
 
     CASE row_op 
 
          WHEN 'XML DOC BEGIN' THEN 
             BEGIN 
               -- save statement and begin assembling XML data 
               stmt := row_redo; 
               xml_data := ''; 
               data_len := 0; 
               DBMS_LOB.CreateTemporary(xml_lob, TRUE);
             END; 
 
          WHEN 'XML DOC WRITE' THEN 
             BEGIN 
               -- Continue to assemble XML data
               xml_data := xml_data || row_redo; 
               data_len := data_len + length(row_redo); 
               DBMS_LOB.WriteAppend(xml_lob, length(row_redo), row_redo);
             END; 
 
          WHEN 'XML DOC END' THEN 
             BEGIN 
               -- Now that assembly is complete, we can use the XML document 
              xml_doc := XMLType.createXML(xml_lob);
              insert into employee_xml_docs values
                        (extractvalue(xml_doc, '/EMPLOYEE/NAME'), stmt, xml_doc);
              commit;
 
              -- reset
              xml_data := ''; 
              data_len := 0; 
              xml_lob := NULL;
             END; 
 
          WHEN 'INSERT' THEN 
             BEGIN 
                stmt := row_redo;
             END; 
 
          WHEN 'UPDATE' THEN 
             BEGIN 
                stmt := row_redo;
             END; 
 
          WHEN 'INTERNAL' THEN 
             DBMS_OUTPUT.PUT_LINE('Skip rows marked INTERNAL'); 
 
          ELSE 
             BEGIN 
                stmt := row_redo;
                DBMS_OUTPUT.PUT_LINE('Other - ' || stmt); 
                IF row_status != DBMS_LOGMNR.VALID_SQL then 
                   DBMS_OUTPUT.PUT_LINE('Skip rows marked non-executable'); 
                ELSE 
                   dbms_output.put_line('Status : ' || row_status);
                END IF; 
             END; 
 
     END CASE;
 
 End LOOP; 
 
End;
/
 
show errors;
</pre>
<p>This procedure can then be called to mine the changes to the <code dir="ltr">SCOTT.XML_DATA_TAB</code> and apply the DMLs.</p>
<pre dir="ltr">
EXECUTE MINE_AND_ASSEMBLE ('SCOTT', 'XML_DATA_TAB');
</pre>
<p>As a result of this procedure, the <code dir="ltr">EMPLOYEE_XML_DOCS</code> table will have a row for each out-of-line XML column that was changed. The <code dir="ltr">EMPLOYEE_NAME</code> column will have the value extracted from the XML document and the <code dir="ltr">SQL_STMT</code> column and the <code dir="ltr">XML_DOC</code> column reflect the original row change.</p>
<p>The following is an example query to the resulting table that displays only the employee name and SQL statement:</p>
<pre dir="ltr">
SELECT EMPLOYEE_NAME, SQL_STMT FROM EMPLOYEE_XML_DOCS;
                
EMPLOYEE_NAME          SQL_STMT                                                                                           
 
Scott Davis          update "SCOTT"."XML_DATA_TAB" a set a."F3" = XMLType(:1) 
                         where a."F1" = '5000' and a."F2" = 'Chen' and a."F5" = 'JJJ'
        
Richard Harry        update "SCOTT"."XML_DATA_TAB" a set a."F4" = XMLType(:1)  
                         where a."F1" = '5000' and a."F2" = 'Chen' and a."F5" = 'JJJ'
        
Margaret Sally       update "SCOTT"."XML_DATA_TAB" a set a."F4" = XMLType(:1)  
                         where a."F1" = '5006' and a."F2" = 'Janosik' and a."F5" = 'MMM'
</pre></div>
<!-- class="sect3" --></div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1016539"></a>
<div id="SUTIL1573" class="sect1">
<h2 class="sect1">Filtering and Formatting Data Returned to V$LOGMNR_CONTENTS</h2>
<p>LogMiner can potentially deal with large amounts of information. You can limit the information that is returned to the <a id="sthref1604"></a><a id="sthref1605"></a><code dir="ltr">V$LOGMNR_CONTENTS</code> view, and the speed at which it is returned. The following sections demonstrate how to specify these limits and their impact on the data returned when you query <code dir="ltr">V$LOGMNR_CONTENTS</code>.</p>
<ul>
<li>
<p><a href="#i1016732">Showing Only Committed Transactions</a></p>
</li>
<li>
<p><a href="#i1016321">Skipping Redo Corruptions</a></p>
</li>
<li>
<p><a href="#i1016328">Filtering Data by Time</a></p>
</li>
<li>
<p><a href="#i1016339">Filtering Data by SCN</a></p>
</li>
</ul>
<p><a id="sthref1606"></a>In addition, LogMiner offers features for formatting the data that is returned to <code dir="ltr">V$LOGMNR_CONTENTS</code>, as described in the following sections:</p>
<ul>
<li>
<p><a href="#i1021032">Formatting Reconstructed SQL Statements for Re-execution</a></p>
</li>
<li>
<p><a href="#i1021043">Formatting the Appearance of Returned Data for Readability</a></p>
</li>
</ul>
<p>You request each of these filtering and formatting features using parameters or options to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure.</p>
<a id="i1016732"></a>
<div id="SUTIL1574" class="sect2">
<h3 class="sect2">Showing Only Committed Transactions</h3>
<p>When you use the <code dir="ltr">COMMITTED_DATA_ONLY</code> option to <a id="sthref1607"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>, only rows belonging to committed transactions are shown in the <a id="sthref1608"></a><code dir="ltr">V$LOGMNR_CONTENTS</code> view. This enables you to filter out rolled back transactions, transactions that are in progress, and internal operations.</p>
<p>To enable this option, specify it when you start LogMiner, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(OPTIONS =&gt; -
  DBMS_LOGMNR.COMMITTED_DATA_ONLY);
</pre>
<p>When you specify the <code dir="ltr">COMMITTED_DATA_ONLY</code> option, LogMiner groups together all DML operations that belong to the same transaction. Transactions are returned in the order in which they were committed.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If the <code dir="ltr">COMMITTED_DATA_ONLY</code> option is specified and you issue a query, then LogMiner stages all redo records within a single transaction in memory until LogMiner finds the commit record for that transaction. Therefore, it is possible to exhaust memory, in which case an "Out of Memory" error will be returned. If this occurs, then you must restart LogMiner without the <code dir="ltr">COMMITTED_DATA_ONLY</code> option specified and reissue the query.</div>
<p>The default is for LogMiner to show rows corresponding to all transactions and to return them in the order in which they are encountered in the redo log files.</p>
<p>For example, suppose you start LogMiner without specifying the <code dir="ltr">COMMITTED_DATA_ONLY</code> option and you execute the following query:</p>
<pre dir="ltr">
SELECT (XIDUSN || '.' || XIDSLT || '.' || XIDSQN) AS XID, 
   USERNAME, SQL_REDO FROM V$LOGMNR_CONTENTS WHERE USERNAME != 'SYS' 
   AND SEG_OWNER IS NULL OR SEG_OWNER NOT IN ('SYS', 'SYSTEM');
</pre>
<p>The output is as follows. Both committed and uncommitted transactions are returned and rows from different transactions are interwoven.</p>
<pre dir="ltr">
XID         USERNAME  SQL_REDO

1.15.3045   RON       set transaction read write;
1.15.3045   RON       insert into "HR"."JOBS"("JOB_ID","JOB_TITLE",
                      "MIN_SALARY","MAX_SALARY") values ('9782',
                      'HR_ENTRY',NULL,NULL);
1.18.3046   JANE      set transaction read write;
1.18.3046   JANE      insert into "OE"."CUSTOMERS"("CUSTOMER_ID",
                      "CUST_FIRST_NAME","CUST_LAST_NAME",
                      "CUST_ADDRESS","PHONE_NUMBERS","NLS_LANGUAGE",
                      "NLS_TERRITORY","CREDIT_LIMIT","CUST_EMAIL",
                      "ACCOUNT_MGR_ID") values ('9839','Edgar',
                      'Cummings',NULL,NULL,NULL,NULL,
                       NULL,NULL,NULL);
1.9.3041    RAJIV      set transaction read write;
1.9.3041    RAJIV      insert into "OE"."CUSTOMERS"("CUSTOMER_ID",
                       "CUST_FIRST_NAME","CUST_LAST_NAME","CUST_ADDRESS",
                       "PHONE_NUMBERS","NLS_LANGUAGE","NLS_TERRITORY",
                       "CREDIT_LIMIT","CUST_EMAIL","ACCOUNT_MGR_ID") 
                       values ('9499','Rodney','Emerson',NULL,NULL,NULL,NULL,
                       NULL,NULL,NULL);
1.15.3045    RON       commit;
1.8.3054     RON       set transaction read write;
1.8.3054     RON       insert into "HR"."JOBS"("JOB_ID","JOB_TITLE",
                       "MIN_SALARY","MAX_SALARY") values ('9566',
                       'FI_ENTRY',NULL,NULL);
1.18.3046    JANE      commit;
1.11.3047    JANE      set transaction read write;
1.11.3047    JANE      insert into "OE"."CUSTOMERS"("CUSTOMER_ID",
                       "CUST_FIRST_NAME","CUST_LAST_NAME",
                       "CUST_ADDRESS","PHONE_NUMBERS","NLS_LANGUAGE",
                       "NLS_TERRITORY","CREDIT_LIMIT","CUST_EMAIL",
                       "ACCOUNT_MGR_ID") values ('8933','Ronald',
                       'Frost',NULL,NULL,NULL,NULL,NULL,NULL,NULL);
1.11.3047    JANE      commit;
1.8.3054     RON       commit;
</pre>
<p>Now suppose you start LogMiner, but this time you specify the <code dir="ltr">COMMITTED_DATA_ONLY</code> option. If you execute the previous query again, then the output is as follows:</p>
<pre dir="ltr">
1.15.3045   RON       set transaction read write;
1.15.3045   RON       insert into "HR"."JOBS"("JOB_ID","JOB_TITLE",
                      "MIN_SALARY","MAX_SALARY") values ('9782',
                      'HR_ENTRY',NULL,NULL);
1.15.3045    RON       commit;
1.18.3046   JANE      set transaction read write;
1.18.3046   JANE      insert into "OE"."CUSTOMERS"("CUSTOMER_ID",
                      "CUST_FIRST_NAME","CUST_LAST_NAME",
                      "CUST_ADDRESS","PHONE_NUMBERS","NLS_LANGUAGE",
                      "NLS_TERRITORY","CREDIT_LIMIT","CUST_EMAIL",
                      "ACCOUNT_MGR_ID") values ('9839','Edgar',
                      'Cummings',NULL,NULL,NULL,NULL,
                       NULL,NULL,NULL);
1.18.3046    JANE      commit;
1.11.3047    JANE      set transaction read write;
1.11.3047    JANE      insert into "OE"."CUSTOMERS"("CUSTOMER_ID",
                       "CUST_FIRST_NAME","CUST_LAST_NAME",
                       "CUST_ADDRESS","PHONE_NUMBERS","NLS_LANGUAGE",
                       "NLS_TERRITORY","CREDIT_LIMIT","CUST_EMAIL",
                       "ACCOUNT_MGR_ID") values ('8933','Ronald',
                       'Frost',NULL,NULL,NULL,NULL,NULL,NULL,NULL);
1.11.3047    JANE      commit;
1.8.3054     RON       set transaction read write;
1.8.3054     RON       insert into "HR"."JOBS"("JOB_ID","JOB_TITLE",
                       "MIN_SALARY","MAX_SALARY") values ('9566',
                       'FI_ENTRY',NULL,NULL);
1.8.3054     RON       commit;
</pre>
<p>Because the <code dir="ltr">COMMIT</code> statement for the 1.15.3045 transaction was issued before the <code dir="ltr">COMMIT</code> statement for the 1.18.3046 transaction, the entire 1.15.3045 transaction is returned first. This is true even though the 1.18.3046 transaction started before the 1.15.3045 transaction. None of the 1.9.3041 transaction is returned because a <code dir="ltr">COMMIT</code> statement was never issued for it.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
See <a href="#i1028311">"Examples Using LogMiner"</a> for a complete example that uses the <code dir="ltr">COMMITTED_DATA_ONLY</code> option</div>
</div>
<!-- class="sect2" -->
<a id="i1016321"></a>
<div id="SUTIL1575" class="sect2">
<h3 class="sect2">Skipping Redo Corruptions</h3>
<p><a id="sthref1609"></a>When you use the <code dir="ltr">SKIP_CORRUPTION</code> option to <a id="sthref1610"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>, any corruptions in the redo log files are skipped during select operations from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view. For every corrupt redo record encountered, a row is returned that contains the value <code dir="ltr">CORRUPTED_BLOCKS</code> in the <code dir="ltr">OPERATION</code> column, <code dir="ltr">1343</code> in the <code dir="ltr">STATUS</code> column, and the number of blocks skipped in the <code dir="ltr">INFO</code> column.</p>
<p>Be aware that the skipped records may include changes to ongoing transactions in the corrupted blocks; such changes will not be reflected in the data returned from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<p>The default is for the select operation to terminate at the first corruption it encounters in the redo log file.</p>
<p>The following SQL example shows how this option works:</p>
<pre dir="ltr">
-- Add redo log files of interest.
--
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   logfilename =&gt; '/usr/oracle/data/db1arch_1_16_482701534.log' -
   options =&gt; DBMS_LOGMNR.NEW);

-- Start LogMiner
--
EXECUTE DBMS_LOGMNR.START_LOGMNR();

-- Select from the V$LOGMNR_CONTENTS view. This example shows corruptions are -- in the redo log files.
-- 
SELECT rbasqn, rbablk, rbabyte, operation, status, info 
   FROM V$LOGMNR_CONTENTS;

ERROR at line 3:
ORA-00368: checksum error in redo log block 
ORA-00353: log corruption near block 6 change 73528 time 11/06/2002 11:30:23 
ORA-00334: archived log: /usr/oracle/data/dbarch1_16_482701534.log

-- Restart LogMiner. This time, specify the SKIP_CORRUPTION option.
-- 
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   options =&gt; DBMS_LOGMNR.SKIP_CORRUPTION);

-- Select from the V$LOGMNR_CONTENTS view again. The output indicates that 
-- corrupted blocks were skipped: CORRUPTED_BLOCKS is in the OPERATION 
-- column, 1343 is in the STATUS column, and the number of corrupt blocks 
-- skipped is in the INFO column.
--
SELECT rbasqn, rbablk, rbabyte, operation, status, info 
   FROM V$LOGMNR_CONTENTS;

RBASQN  RBABLK RBABYTE  OPERATION        STATUS  INFO
13      2        76     START              0
13      2        76     DELETE             0
13      3       100     INTERNAL           0
13      3       380     DELETE             0
13      0         0     CORRUPTED_BLOCKS   1343  corrupt blocks 4 to 19 skipped
13      20      116     UPDATE             0
</pre></div>
<!-- class="sect2" -->
<a id="i1016328"></a>
<div id="SUTIL1576" class="sect2">
<h3 class="sect2">Filtering Data by Time</h3>
<p><a id="sthref1611"></a>To filter data by time, set the <code dir="ltr">STARTTIME</code> and <code dir="ltr">ENDTIME</code> parameters in the <a id="sthref1612"></a><a id="sthref1613"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure.</p>
<p>To avoid the need to specify the date format in the call to the PL/SQL <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure, you can use the SQL <code dir="ltr">ALTER</code> <code dir="ltr">SESSION</code> <code dir="ltr">SET NLS_DATE_FORMAT</code> statement first, as shown in the following example.</p>
<pre dir="ltr">
ALTER SESSION SET NLS_DATE_FORMAT = 'DD-MON-YYYY HH24:MI:SS';
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   DICTFILENAME =&gt; '/oracle/database/dictionary.ora', -
   STARTTIME =&gt; '01-Jan-2008 08:30:00', -
   ENDTIME =&gt; '01-Jan-2008 08:45:00'-
   OPTIONS =&gt; DBMS_LOGMNR.CONTINUOUS_MINE); 
</pre>
<p>The timestamps should not be used to infer ordering of redo records. You can infer the order of redo records by using the SCN.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<ul>
<li>
<p><a href="#i1028311">"Examples Using LogMiner"</a> for a complete example of filtering data by time</p>
</li>
<li>
<p><a class="olink ARPLS" href="../../appdev.112/e40758/toc.htm"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for information about what happens if you specify starting and ending times and they are not found in the LogMiner redo log file list, and for information about how these parameters interact with the <code dir="ltr">CONTINUOUS_MINE</code> option</p>
</li>
</ul>
</div>
</div>
<!-- class="sect2" -->
<a id="i1016339"></a>
<div id="SUTIL1577" class="sect2">
<h3 class="sect2">Filtering Data by SCN</h3>
<p><a id="sthref1614"></a>To filter data by SCN (system change number), use the <a id="sthref1615"></a><a id="sthref1616"></a><code dir="ltr">STARTSCN</code> and <code dir="ltr">ENDSCN</code> parameters to the PL/SQL <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure, as shown in this example:</p>
<pre dir="ltr">
 EXECUTE DBMS_LOGMNR.START_LOGMNR(-
    STARTSCN =&gt; 621047, -
    ENDSCN   =&gt; 625695, -
    OPTIONS  =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
                DBMS_LOGMNR.CONTINUOUS_MINE);
</pre>
<p>The <code dir="ltr">STARTSCN</code> and <code dir="ltr">ENDSCN</code> parameters override the <code dir="ltr">STARTTIME</code> and <code dir="ltr">ENDTIME</code> parameters in situations where all are specified.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<ul>
<li>
<p><a href="#i1028311">"Examples Using LogMiner"</a> for a complete example of filtering data by SCN</p>
</li>
<li>
<p><a class="olink ARPLS" href="../../appdev.112/e40758/toc.htm"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for information about what happens if you specify starting and ending SCN values and they are not found in the LogMiner redo log file list and for information about how these parameters interact with the <code dir="ltr">CONTINUOUS_MINE</code> option</p>
</li>
</ul>
</div>
</div>
<!-- class="sect2" -->
<a id="i1021032"></a>
<div id="SUTIL1578" class="sect2">
<h3 class="sect2">Formatting Reconstructed SQL Statements for Re-execution</h3>
<p><a id="sthref1617"></a>By default, a <code dir="ltr">ROWID</code> clause is included in the reconstructed <code dir="ltr">SQL_REDO</code> and <code dir="ltr">SQL_UNDO</code> statements and the statements are ended with a semicolon.</p>
<p>However, you can override the default settings, as follows:</p>
<ul>
<li>
<p>Specify the <code dir="ltr">NO_ROWID_IN_STMT</code> option when you start LogMiner.</p>
<p>This excludes the <code dir="ltr">ROWID</code> clause from the reconstructed statements. Because row IDs are not consistent between databases, if you intend to re-execute the <code dir="ltr">SQL_REDO</code> or <code dir="ltr">SQL_UNDO</code> statements against a different database than the one against which they were originally executed, then specify the <code dir="ltr">NO_ROWID_IN_STMT</code> option when you start LogMiner.</p>
</li>
<li>
<p>Specify the <code dir="ltr">NO_SQL_DELIMITER</code> option when you start LogMiner.</p>
<p><a id="sthref1618"></a>This suppresses the semicolon from the reconstructed statements. This is helpful for applications that open a cursor and then execute the reconstructed statements.</p>
</li>
</ul>
<p>Note that if the <code dir="ltr">STATUS</code> field of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view contains the value <code dir="ltr">2</code> (<code dir="ltr">invalid sql)</code>, then the associated SQL statement cannot be executed.</p>
</div>
<!-- class="sect2" -->
<a id="i1021043"></a>
<div id="SUTIL1579" class="sect2">
<h3 class="sect2">Formatting the Appearance of Returned Data for Readability</h3>
<p><a id="sthref1619"></a><a id="sthref1620"></a>Sometimes a query can result in a large number of columns containing reconstructed SQL statements, which can be visually busy and hard to read. LogMiner provides the <code dir="ltr">PRINT_PRETTY_SQL</code> option to address this problem. The <code dir="ltr">PRINT_PRETTY_SQL</code> option to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure formats the reconstructed SQL statements as follows, which makes them easier to read:</p>
<pre dir="ltr">
insert into "HR"."JOBS"
 values
    "JOB_ID" = '9782',
    "JOB_TITLE" = 'HR_ENTRY',
    "MIN_SALARY" IS NULL,
    "MAX_SALARY" IS NULL;
  update "HR"."JOBS"
  set
    "JOB_TITLE" = 'FI_ENTRY'
  where
    "JOB_TITLE" = 'HR_ENTRY' and
    ROWID = 'AAAHSeAABAAAY+CAAX';

update "HR"."JOBS"
  set
    "JOB_TITLE" = 'FI_ENTRY'
  where
    "JOB_TITLE" = 'HR_ENTRY' and
    ROWID = 'AAAHSeAABAAAY+CAAX';

delete from "HR"."JOBS"
 where
    "JOB_ID" = '9782' and
    "JOB_TITLE" = 'FI_ENTRY' and
    "MIN_SALARY" IS NULL and
    "MAX_SALARY" IS NULL and
    ROWID = 'AAAHSeAABAAAY+CAAX';
</pre>
<p>SQL statements that are reconstructed when the <code dir="ltr">PRINT_PRETTY_SQL</code> option is enabled are not executable, because they do not use standard SQL syntax.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#i1028311">"Examples Using LogMiner"</a> for a complete example of using the <code dir="ltr">PRINT_PRETTY_SQL</code> option</div>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1017514"></a>
<div id="SUTIL1580" class="sect1">
<h2 class="sect1">Reapplying DDL Statements Returned to V$LOGMNR_CONTENTS</h2>
<p><a id="sthref1621"></a>Be aware that some DDL statements issued by a user cause Oracle to internally execute one or more other DDL statements. If you want to reapply SQL DDL from the <code dir="ltr">SQL_REDO</code> or <code dir="ltr">SQL_UNDO</code> columns of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view as it was originally applied to the database, then you should not execute statements that were executed internally by Oracle.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you execute DML statements that were executed internally by Oracle, then you may corrupt your database. See Step 5 of <a href="#i1028830">"Example 4: Using the LogMiner Dictionary in the Redo Log Files"</a> for an example.</div>
<p>To differentiate between DDL statements that were issued by a user from those that were issued internally by Oracle, query the <code dir="ltr">INFO</code> column of <code dir="ltr">V$LOGMNR_CONTENTS</code>. The value of the <code dir="ltr">INFO</code> column indicates whether the DDL was executed by a user or by Oracle.</p>
<p>If you want to reapply SQL DDL as it was originally applied, then you should only re-execute the DDL SQL contained in the <code dir="ltr">SQL_REDO</code> or <code dir="ltr">SQL_UNDO</code> column of <code dir="ltr">V$LOGMNR_CONTENTS</code> if the <code dir="ltr">INFO</code> column contains the value <code dir="ltr">USER_DDL.</code></p>
</div>
<!-- class="sect1" -->
<a id="i1016542"></a>
<div id="SUTIL1581" class="sect1">
<h2 class="sect1">Calling DBMS_LOGMNR.START_LOGMNR Multiple Times</h2>
<p>Even after you have successfully called <a id="sthref1622"></a><a id="sthref1623"></a><code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> and selected from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, you can call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> again without ending the current LogMiner session and specify different options and time or SCN ranges. The following list presents reasons why you might want to do this:</p>
<ul>
<li>
<p>You want to limit the amount of redo data that LogMiner has to analyze.</p>
</li>
<li>
<p>You want to specify different options. For example, you might decide to specify the <code dir="ltr">PRINT_PRETTY_SQL</code> option or that you only want to see committed transactions (so you specify the <code dir="ltr">COMMITTED_DATA_ONLY</code> option).</p>
</li>
<li>
<p>You want to change the time or SCN range to be analyzed.</p>
</li>
</ul>
<p>The following examples illustrate situations where it might be useful to call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> multiple times.</p>
<dl>
<dd><a id="SUTIL3896"></a><a id="sthref1624"></a></dd>
<dt class="seghead">Example 1&nbsp;&nbsp;&nbsp;Mining Only a Subset of the Data in the Redo Log Files</dt>
<dd>
<p><a id="sthref1625"></a>Suppose the list of redo log files that LogMiner has to mine include those generated for an entire week. However, you want to analyze only what happened from 12:00 to 1:00 each day. You could do this most efficiently by:</p>
</dd>
</dl>
<ol>
<li>
<p>Calling <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> with this time range for Monday.</p>
</li>
<li>
<p>Selecting changes from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
</li>
<li>
<p>Repeating Steps 1 and 2 for each day of the week.</p>
</li>
</ol>
<p>If the total amount of redo data is large for the week, then this method would make the whole analysis much faster, because only a small subset of each redo log file in the list would be read by LogMiner.</p>
<dl>
<dd><a id="SUTIL3897"></a><a id="sthref1626"></a></dd>
<dt class="seghead">Example 1&nbsp;&nbsp;&nbsp;Adjusting the Time Range or SCN Range</dt>
<dd>
<p><a id="sthref1627"></a>Suppose you specify a redo log file list and specify a time (or SCN) range when you start LogMiner. When you query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, you find that only part of the data of interest is included in the time range you specified. You can call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> again to expand the time range by an hour (or adjust the SCN range).</p>
</dd>
<dd><a id="SUTIL3898"></a><a id="sthref1628"></a></dd>
<dt class="seghead">Example 2&nbsp;&nbsp;&nbsp;Analyzing Redo Log Files As They Arrive at a Remote Database</dt>
<dd>
<p><a id="sthref1629"></a>Suppose you have written an application to analyze changes or to replicate changes from one database to another database. The source database sends its redo log files to the mining database and drops them into an operating system directory. Your application:</p>
</dd>
</dl>
<ol>
<li>
<p>Adds all redo log files currently in the directory to the redo log file list</p>
</li>
<li>
<p>Calls <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> with appropriate settings and selects from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view</p>
</li>
<li>
<p>Adds additional redo log files that have newly arrived in the directory</p>
</li>
<li>
<p>Repeats Steps 2 and 3, indefinitely</p>
</li>
</ol>
</div>
<!-- class="sect1" -->
<a id="i1021068"></a>
<div id="SUTIL1582" class="sect1">
<h2 class="sect1">Supplemental Logging</h2>
<p><a id="sthref1630"></a><a id="sthref1631"></a>Redo log files are generally used for instance recovery and media recovery. The data needed for such operations is automatically recorded in the redo log files. However, a redo-based application may require that additional columns be logged in the redo log files. The process of logging these additional columns is called <span class="bold">supplemental logging.</span></p>
<p>By default, Oracle Database does not provide any supplemental logging, which means that by default LogMiner is not usable. Therefore, you must enable at least minimal supplemental logging before generating log files which will be analyzed by LogMiner.</p>
<p>The following are examples of situations in which additional columns may be needed:</p>
<ul>
<li>
<p>An application that applies reconstructed SQL statements to a different database must identify the update statement by a set of columns that uniquely identify the row (for example, a primary key), not by the <code dir="ltr">ROWID</code> shown in the reconstructed SQL returned by the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, because the <code dir="ltr">ROWID</code> of one database will be different and therefore meaningless in another database.</p>
</li>
<li>
<p>An application may require that the before-image of the whole row be logged, not just the modified columns, so that tracking of row changes is more efficient.</p>
</li>
</ul>
<p>A <span class="bold"><a id="sthref1632"></a><a id="sthref1633"></a>supplemental log group</span> is the set of additional columns to be logged when supplemental logging is enabled. There are two types<a id="sthref1634"></a><a id="sthref1635"></a> of supplemental log groups that determine when columns in the log group are logged:</p>
<ul>
<li>
<p><span class="bold"><a id="sthref1636"></a>Unconditional supplemental log groups:</span> The before-images of specified columns are logged any time a row is updated, regardless of whether the update affected any of the specified columns. This is sometimes referred to as an ALWAYS log group.</p>
</li>
<li>
<p><span class="bold"><a id="sthref1637"></a>Conditional supplemental log groups:</span> The before-images of all specified columns are logged only if at least one of the columns in the log group is updated.</p>
</li>
</ul>
<p>Supplemental log groups can be system-generated or user-defined.</p>
<p>In addition to the two types of supplemental logging, there are two levels of supplemental logging, as described in the following sections:</p>
<ul>
<li>
<p><a href="#i1032078">Database-Level Supplemental Logging</a></p>
</li>
<li>
<p><a href="#i1036064">Table-Level Supplemental Logging</a></p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#i1014108">"Querying Views for Supplemental Logging Settings"</a></div>
</li>
</ul>
<a id="i1032078"></a>
<div id="SUTIL1583" class="sect2">
<h3 class="sect2">Database-Level Supplemental Logging</h3>
<p><a id="sthref1638"></a>There are two types of database-level supplemental logging: minimal supplemental logging and identification key logging, as described in the following sections. Minimal supplemental logging does not impose significant overhead on the database generating the redo log files. However, enabling database-wide identification key logging can impose overhead on the database generating the redo log files. Oracle recommends that you at least enable minimal supplemental logging for LogMiner.</p>
<div id="SUTIL1584" class="sect3"><a id="sthref1639"></a>
<h4 class="sect3">Minimal Supplemental Logging</h4>
<p><a id="sthref1640"></a>Minimal supplemental logging logs the minimal amount of information needed for LogMiner to identify, group, and merge the redo operations associated with DML changes. It ensures that LogMiner (and any product building on LogMiner technology) has sufficient information to support chained rows and various storage arrangements, such as cluster tables and index-organized tables. To enable minimal supplemental logging, execute the following SQL statement:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
In Oracle Database release 9.0.1, minimal supplemental logging was the default behavior in LogMiner. In release 9.2 and later, the default is no supplemental logging. Supplemental logging must be specifically enabled.</div>
</div>
<!-- class="sect3" -->
<a id="i1018531"></a>
<div id="SUTIL1585" class="sect3">
<h4 class="sect3">Database-Level Identification Key Logging</h4>
<p><a id="sthref1641"></a><a id="sthref1642"></a>Identification key logging is necessary when redo log files will not be mined at the source database instance, for example, when the redo log files will be mined at a logical standby database.</p>
<p>Using database identification key logging, you can enable database-wide before-image logging for all updates by specifying one or more of the following options to the SQL <code dir="ltr">ALTER</code> <code dir="ltr">DATABASE</code> <code dir="ltr">ADD</code> <code dir="ltr">SUPPLEMENTAL</code> <code dir="ltr">LOG</code> statement:</p>
<ul>
<li>
<p><code dir="ltr">ALL</code> system-generated unconditional supplemental log group</p>
<p>This option specifies that when a row is updated, all columns of that row (except for LOBs, <code dir="ltr">LONGS</code>, and <code dir="ltr">ADT</code>s) are placed in the redo log file.</p>
<p>To enable all column logging at the database level, execute the following statement:</p>
<pre dir="ltr">
SQL&gt; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
</pre></li>
<li>
<p><code dir="ltr">PRIMARY KEY</code> system-generated unconditional supplemental log group</p>
<p>This option causes the database to place all columns of a row's primary key in the redo log file whenever a row containing a primary key is updated (even if no value in the primary key has changed).</p>
<p>If a table does not have a primary key, but has one or more non-null unique index key constraints or index keys, then one of the unique index keys is chosen for logging as a means of uniquely identifying the row being updated.</p>
<p>If the table has neither a primary key nor a non-null unique index key, then all columns except <code dir="ltr">LONG</code> and LOB are supplementally logged; this is equivalent to specifying <code dir="ltr">ALL</code> supplemental logging for that row. Therefore, Oracle recommends that when you use database-level primary key supplemental logging, all or most tables be defined to have primary or unique index keys.</p>
<p>To enable primary key logging at the database level, execute the following statement:</p>
<pre dir="ltr">
SQL&gt; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
</pre></li>
<li>
<p>UNIQUE system-generated conditional supplemental log group</p>
<p>This option causes the database to place all columns of a row's composite unique key or bitmap index in the redo log file if any column belonging to the composite unique key or bitmap index is modified. The unique key can be due to either a unique constraint or a unique index.</p>
<p>To enable unique index key and bitmap index logging at the database level, execute the following statement:</p>
<pre dir="ltr">
SQL&gt; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
</pre></li>
<li>
<p><code dir="ltr">FOREIGN KEY</code> system-generated conditional supplemental log group</p>
<p>This option causes the database to place all columns of a row's foreign key in the redo log file if any column belonging to the foreign key is modified.</p>
<p>To enable foreign key logging at the database level, execute the following SQL statement:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (FOREIGN KEY) COLUMNS;
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
Regardless of whether identification key logging is enabled, the SQL statements returned by LogMiner always contain the <code dir="ltr">ROWID</code> clause. You can filter out the <code dir="ltr">ROWID</code> clause by using the <code dir="ltr">NO_ROWID_IN_STMT</code> option to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure call. See <a href="#i1021032">"Formatting Reconstructed SQL Statements for Re-execution"</a> for details.</div>
</li>
</ul>
<p>Keep the following in mind when you use identification key logging:</p>
<ul>
<li>
<p>If the database is open when you enable identification key logging, then all DML cursors in the cursor cache are invalidated. This can affect performance until the cursor cache is repopulated.</p>
</li>
<li>
<p>When you enable identification key logging at the database level, minimal supplemental logging is enabled implicitly.</p>
</li>
<li>
<p>Supplemental logging statements are cumulative. If you issue the following SQL statements, then both primary key and unique key supplemental logging is enabled:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
</pre></li>
</ul>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<div id="SUTIL1586" class="sect2"><a id="sthref1643"></a>
<h3 class="sect2">Disabling Database-Level Supplemental Logging</h3>
<p><a id="sthref1644"></a>You disable database-level supplemental logging using the SQL <code dir="ltr">ALTER DATABASE</code> statement with the <code dir="ltr">DROP SUPPLEMENTAL LOGGING</code> clause. You can drop supplemental logging attributes incrementally. For example, suppose you issued the following SQL statements, in the following order:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
ALTER DATABASE DROP SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER DATABASE DROP SUPPLEMENTAL LOG DATA;
</pre>
<p>The statements would have the following effects:</p>
<ul>
<li>
<p>After the first statement, primary key supplemental logging is enabled.</p>
</li>
<li>
<p>After the second statement, primary key and unique key supplemental logging are enabled.</p>
</li>
<li>
<p>After the third statement, only unique key supplemental logging is enabled.</p>
</li>
<li>
<p>After the fourth statement, all supplemental logging is not disabled. The following error is returned: <code dir="ltr">ORA-32589: unable to drop minimal supplemental logging</code>.</p>
</li>
</ul>
<p>To disable all database supplemental logging, you must first disable any identification key logging that has been enabled, then disable minimal supplemental logging. The following example shows the correct order:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
ALTER DATABASE DROP SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER DATABASE DROP SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
ALTER DATABASE DROP SUPPLEMENTAL LOG DATA;
</pre>
<p>Dropping minimal supplemental log data is allowed only if no other variant of database-level supplemental logging is enabled.</p>
</div>
<!-- class="sect2" -->
<a id="i1036064"></a>
<div id="SUTIL1587" class="sect2">
<h3 class="sect2">Table-Level Supplemental Logging</h3>
<p><a id="sthref1645"></a><a id="sthref1646"></a>Table-level supplemental logging specifies, at the table level, which columns are to be supplementally logged. You can use identification key logging or user-defined conditional and unconditional supplemental log groups to log supplemental information, as described in the following sections.</p>
<div id="SUTIL1588" class="sect3"><a id="sthref1647"></a>
<h4 class="sect3">Table-Level Identification Key Logging</h4>
<p><a id="sthref1648"></a><a id="sthref1649"></a>Identification key logging at the table level offers the same options as those provided at the database level: all, primary key, foreign key, and unique key. However, when you specify identification key logging at the table level, only the specified table is affected. For example, if you enter the following SQL statement (specifying database-level supplemental logging), then whenever a column in any database table is changed, the entire row containing that column (except columns for LOBs, <code dir="ltr">LONG</code>s, and <code dir="ltr">ADT</code>s) will be placed in the redo log file:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
</pre>
<p>However, if you enter the following SQL statement (specifying table-level supplemental logging) instead, then only when a column in the <code dir="ltr">employees</code> table is changed will the entire row (except for LOB, <code dir="ltr">LONG</code>s, and <code dir="ltr">ADT</code>s) of the table be placed in the redo log file. If a column changes in the <code dir="ltr">departments</code> table, then only the changed column will be placed in the redo log file.</p>
<pre dir="ltr">
ALTER TABLE HR.EMPLOYEES ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
</pre>
<p>Keep the following in mind when you use table-level identification key logging:</p>
<ul>
<li>
<p>If the database is open when you enable identification key logging on a table, then all DML cursors for that table in the cursor cache are invalidated. This can affect performance until the cursor cache is repopulated.</p>
</li>
<li>
<p>Supplemental logging statements are cumulative. If you issue the following SQL statements, then both primary key and unique index key table-level supplemental logging is enabled:</p>
<pre dir="ltr">
ALTER TABLE HR.EMPLOYEES 
  ADD SUPPLEMENTAL LOG DATA (PRIMARY KEY) COLUMNS;
ALTER TABLE HR.EMPLOYEES 
  ADD SUPPLEMENTAL LOG DATA (UNIQUE) COLUMNS;
</pre></li>
</ul>
<p>See <a href="#i1018531">"Database-Level Identification Key Logging"</a> for a description of each of the identification key logging options.</p>
</div>
<!-- class="sect3" -->
<div id="SUTIL1589" class="sect3"><a id="sthref1650"></a>
<h4 class="sect3">Table-Level User-Defined Supplemental Log Groups</h4>
<p><a id="sthref1651"></a><a id="sthref1652"></a><a id="sthref1653"></a>In addition to table-level identification key logging, Oracle supports user-defined supplemental log groups. With user-defined supplemental log groups, you can specify which columns are supplementally logged. You can specify conditional or unconditional log groups, as follows:</p>
<ul>
<li>
<p>User-defined unconditional log groups</p>
<p>To enable supplemental logging that uses user-defined unconditional log groups, use the <code dir="ltr">ALWAYS</code> clause as shown in the following example:</p>
<pre dir="ltr">
ALTER TABLE HR.EMPLOYEES
   ADD SUPPLEMENTAL LOG GROUP emp_parttime (EMPLOYEE_ID, LAST_NAME, 
   DEPARTMENT_ID) ALWAYS;
</pre>
<p>This creates a log group named <code dir="ltr">emp_parttime</code> on the <code dir="ltr">hr.employees</code> table that consists of the columns <code dir="ltr">employee_id</code>, <code dir="ltr">last_name</code>, and <code dir="ltr">department_id</code>. These columns will be logged every time an <code dir="ltr">UPDATE</code> statement is executed on the <code dir="ltr">hr.employees</code> table, regardless of whether the update affected these columns. (If you want to have the entire row image logged any time an update was made, then use table-level <code dir="ltr">ALL</code> identification key logging, as described previously).</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
LOB, <code dir="ltr">LONG</code>, and <code dir="ltr">ADT</code> columns cannot be supplementally logged.</div>
</li>
<li>
<p>User-defined conditional supplemental log groups</p>
<p>To enable supplemental logging that uses user-defined conditional log groups, omit the <code dir="ltr">ALWAYS</code> clause from the SQL <code dir="ltr">ALTER</code> <code dir="ltr">TABLE</code> statement, as shown in the following example:</p>
<pre dir="ltr">
ALTER TABLE HR.EMPLOYEES
   ADD SUPPLEMENTAL LOG GROUP emp_fulltime (EMPLOYEE_ID, LAST_NAME, 
   DEPARTMENT_ID);
</pre>
<p>This creates a log group named <code dir="ltr">emp_fulltime</code> on table <code dir="ltr">hr.employees</code>. Just like the previous example, it consists of the columns <code dir="ltr">employee_id</code>, <code dir="ltr">last_name</code>, and <code dir="ltr">department_id</code>. But because the <code dir="ltr">ALWAYS</code> clause was omitted, before-images of the columns will be logged only if at least one of the columns is updated.</p>
</li>
</ul>
<p>For both unconditional and conditional user-defined supplemental log groups, you can explicitly specify that a column in the log group be excluded from supplemental logging by specifying the <code dir="ltr">NO</code> <code dir="ltr">LOG</code> option. When you specify a log group and use the <code dir="ltr">NO</code> <code dir="ltr">LOG</code> option, you must specify at least one column in the log group without the <code dir="ltr">NO</code> <code dir="ltr">LOG</code> option, as shown in the following example:</p>
<pre dir="ltr">
ALTER TABLE HR.EMPLOYEES
   ADD SUPPLEMENTAL LOG GROUP emp_parttime(
   DEPARTMENT_ID NO LOG, EMPLOYEE_ID);
</pre>
<p>This enables you to associate this column with other columns in the named supplemental log group such that any modification to the <code dir="ltr">NO</code> <code dir="ltr">LOG</code> column causes the other columns in the supplemental log group to be placed in the redo log file. This might be useful, for example, if you want to log certain columns in a group if a <code dir="ltr">LONG</code> column changes. You cannot supplementally log the <code dir="ltr">LONG</code> column itself; however, you can use changes to that column to trigger supplemental logging of other columns in the same row.</p>
</div>
<!-- class="sect3" -->
<div id="SUTIL1590" class="sect3"><a id="sthref1654"></a>
<h4 class="sect3"><a id="sthref1655"></a>Usage Notes for User-Defined Supplemental Log Groups</h4>
<p>Keep the following in mind when you specify user-defined supplemental log groups:</p>
<ul>
<li>
<p>A column can belong to more than one supplemental log group. However, the before-image of the columns gets logged only once.</p>
</li>
<li>
<p>If you specify the same columns to be logged both conditionally and unconditionally, then the columns are logged unconditionally.</p>
</li>
</ul>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="i1039093"></a>
<div id="SUTIL1591" class="sect2">
<h3 class="sect2">Tracking DDL Statements in the LogMiner Dictionary</h3>
<p><a id="sthref1656"></a>LogMiner automatically builds its own internal dictionary from the LogMiner dictionary that you specify when you start LogMiner (either an online catalog, a dictionary in the redo log files, or a flat file). This dictionary provides a snapshot of the database objects and their definitions.</p>
<p>If your LogMiner dictionary is in the redo log files or is a flat file, then you can use the <code dir="ltr">DDL_DICT_TRACKING</code> option to the PL/SQL <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure to direct LogMiner to track data definition language (DDL) statements. DDL tracking enables LogMiner to successfully track structural changes made to a database object, such as adding or dropping columns from a table. For example:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(OPTIONS =&gt; -
   DBMS_LOGMNR.DDL_DICT_TRACKING + DBMS_LOGMNR.DICT_FROM_REDO_LOGS);
</pre>
<p>See <a href="#i1028835">"Example 5: Tracking DDL Statements in the Internal Dictionary"</a> for a complete example.</p>
<p>With this option set, LogMiner applies any DDL statements seen in the redo log files to its internal dictionary.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
In general, it is a good idea to keep supplemental logging and the DDL tracking feature enabled, because if they are not enabled and a DDL event occurs, then LogMiner returns some of the redo data as binary data. Also, a metadata version mismatch could occur.</div>
<p>When you enable <code dir="ltr">DDL_DICT_TRACKING,</code> data manipulation language (DML) operations performed on tables created after the LogMiner dictionary was extracted can be shown correctly.</p>
<p>For example, if a table <code dir="ltr">employees</code> is updated through two successive DDL operations such that column <code dir="ltr">gender</code> is added in one operation, and column <code dir="ltr">commission_pct</code> is dropped in the next, then LogMiner will keep versioned information for <code dir="ltr">employees</code> for each of these changes. This means that LogMiner can successfully mine redo log files that are from before and after these DDL changes, and no binary data will be presented for the <code dir="ltr">SQL_REDO</code> or <code dir="ltr">SQL_UNDO</code> columns.</p>
<p>Because LogMiner automatically assigns versions to the database metadata, it will detect and notify you of any mismatch between its internal dictionary and the dictionary in the redo log files. If LogMiner detects a mismatch, then it generates binary data in the <code dir="ltr">SQL_REDO</code> column of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, the <code dir="ltr">INFO</code> column contains the string "D<a id="sthref1657"></a>ictionary Version Mismatch", and the <code dir="ltr">STATUS</code> column will contain the value <code dir="ltr">2</code>.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
It is important to understand that the LogMiner internal dictionary is not the same as the LogMiner dictionary contained in a flat file, in redo log files, or in the online catalog. LogMiner does update its internal dictionary, but it does not update the dictionary that is contained in a flat file, in redo log files, or in the online catalog.</div>
<p><a id="sthref1658"></a>The following list describes the requirements for specifying the <code dir="ltr">DDL_DICT_TRACKING</code> option with the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure.</p>
<ul>
<li>
<p>The <code dir="ltr">DDL_DICT_TRACKING</code> option is not valid with the <code dir="ltr">DICT_FROM_ONLINE_CATALOG</code> option.</p>
</li>
<li>
<p>The <code dir="ltr">DDL_DICT_TRACKING</code> option requires that the database be open.</p>
</li>
<li>
<p>Supplemental logging must be enabled database-wide, or log groups must have been created for the tables of interest.</p>
</li>
</ul>
</div>
<!-- class="sect2" -->
<a id="CCHHCFJI"></a>
<div id="SUTIL1592" class="sect2">
<h3 class="sect2">DDL_DICT_TRACKING and Supplemental Logging Settings</h3>
<p><a id="sthref1659"></a>Note the following interactions that occur when various settings of dictionary tracking and supplemental logging are combined:</p>
<ul>
<li>
<p>If <code dir="ltr">DDL_DICT_TRACKING</code> is enabled, but supplemental logging is not enabled and:</p>
<ul>
<li>
<p>A DDL transaction is encountered in the redo log file, then a query of <code dir="ltr">V$LOGMNR_CONTENTS</code> will terminate with the ORA-01347 error.</p>
</li>
<li>
<p>A DML transaction is encountered in the redo log file, then LogMiner will not assume that the current version of the table (underlying the DML) in its dictionary is correct, and columns in <code dir="ltr">V$LOGMNR_CONTENTS</code> will be set as follows:</p>
<ul>
<li>
<p>The <code dir="ltr">SQL_REDO</code> column will contain binary data.</p>
</li>
<li>
<p>The <code dir="ltr">STATUS</code> column will contain a value of <code dir="ltr">2</code> (which indicates that the SQL is not valid).</p>
</li>
<li>
<p>The <code dir="ltr">INFO</code> column will contain the string 'Dictionary Mismatch'.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If <code dir="ltr">DDL_DICT_TRACKING</code> is not enabled and supplemental logging is not enabled, and the columns referenced in a DML operation match the columns in the LogMiner dictionary, then LogMiner assumes that the latest version in its dictionary is correct, and columns in <code dir="ltr">V$LOGMNR_CONTENTS</code> will be set as follows:</p>
<ul>
<li>
<p>LogMiner will use the definition of the object in its dictionary to generate values for the <code dir="ltr">SQL_REDO</code> and <code dir="ltr">SQL_UNDO</code> columns.</p>
</li>
<li>
<p>The status column will contain a value of <code dir="ltr">3</code> (which indicates that the SQL is not guaranteed to be accurate).</p>
</li>
<li>
<p>The <code dir="ltr">INFO</code> column will contain the string 'no supplemental log data found'.</p>
</li>
</ul>
</li>
<li>
<p>If <code dir="ltr">DDL_DICT_TRACKING</code> is not enabled and supplemental logging is not enabled and there are more modified columns in the redo log file for a table than the LogMiner dictionary definition for the table defines, then:</p>
<ul>
<li>
<p>The <code dir="ltr">SQL_REDO</code> and <code dir="ltr">SQL_UNDO</code> columns will contain the string 'Dictionary Version Mismatch'.</p>
</li>
<li>
<p>The <code dir="ltr">STATUS</code> column will contain a value of <code dir="ltr">2</code> (which indicates that the SQL is not valid).</p>
</li>
<li>
<p>The <code dir="ltr">INFO</code> column will contain the string 'Dictionary Mismatch'.</p>
</li>
</ul>
<p>Also be aware that it is possible to get unpredictable behavior if the dictionary definition of a column indicates one type but the column is really another type.</p>
</li>
</ul>
</div>
<!-- class="sect2" -->
<div id="SUTIL1593" class="sect2"><a id="sthref1660"></a>
<h3 class="sect2">DDL_DICT_TRACKING and Specified Time or SCN Ranges</h3>
<p><a id="sthref1661"></a>Because LogMiner must not miss a DDL statement if it is to ensure the consistency of its dictionary, LogMiner may start reading redo log files before your requested starting time or SCN (as specified with <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>) when the <code dir="ltr">DDL_DICT_TRACKING</code> option is enabled. The actual time or SCN at which LogMiner starts reading redo log files is referred to as the <span class="bold">required starting time</span> or the <span class="bold">required starting SCN</span>.</p>
<p>No missing redo log files (based on sequence numbers) are allowed from the required starting time or the required starting SCN.</p>
<p>LogMiner determines where it will start reading redo log data as follows:</p>
<ul>
<li>
<p>After the dictionary is loaded, the first time that you call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>, LogMiner begins reading as determined by one of the following, whichever causes it to begin earlier:</p>
<ul>
<li>
<p>Your requested starting time or SCN value</p>
</li>
<li>
<p>The commit SCN of the dictionary dump</p>
</li>
</ul>
</li>
<li>
<p>On subsequent calls to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>, LogMiner begins reading as determined for one of the following, whichever causes it to begin earliest:</p>
<ul>
<li>
<p>Your requested starting time or SCN value</p>
</li>
<li>
<p>The start of the earliest DDL transaction where the <code dir="ltr">COMMIT</code> statement has not yet been read by LogMiner</p>
</li>
<li>
<p>The highest SCN read by LogMiner</p>
</li>
</ul>
</li>
</ul>
<p>The following scenario helps illustrate this:</p>
<p>Suppose you create a redo log file list containing five redo log files. Assume that a dictionary is contained in the first redo file, and the changes that you have indicated you want to see (using <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>) are recorded in the third redo log file. You then do the following:</p>
<ol>
<li>
<p>Call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>. LogMiner will read:</p>
<ol>
<li>
<p>The first log file to load the dictionary</p>
</li>
<li>
<p>The second redo log file to pick up any possible DDLs contained within it</p>
</li>
<li>
<p>The third log file to retrieve the data of interest</p>
</li>
</ol>
</li>
<li>
<p>Call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> again with the same requested range.</p>
<p>LogMiner will begin with redo log file 3; it no longer needs to read redo log file 2, because it has already processed any DDL statements contained within it.</p>
</li>
<li>
<p>Call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> again, this time specifying parameters that require data to be read from redo log file 5.</p>
<p>LogMiner will start reading from redo log file 4 to pick up any DDL statements that may be contained within it.</p>
</li>
</ol>
<p>Query the <code dir="ltr">REQUIRED_START_DATE</code> or the <code dir="ltr">REQUIRED_START_SCN</code> columns of the <code dir="ltr"><a id="sthref1662"></a><a id="sthref1663"></a>V$LOGMNR_PARAMETERS</code> view to see where LogMiner will actually start reading. Regardless of where LogMiner starts reading, only rows in your requested range will be returned from the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1014119"></a>
<div id="SUTIL1594" class="sect1">
<h2 class="sect1">Accessing LogMiner Operational Information in Views</h2>
<p>LogMiner operational information (as opposed to redo data) is contained in the following views<a id="sthref1664"></a>. You can use SQL to query them as you would any other view.</p>
<ul>
<li>
<p><code dir="ltr"><a id="sthref1665"></a><a id="sthref1666"></a>V$LOGMNR_DICTIONARY</code></p>
<p><a id="sthref1667"></a>Shows information about a LogMiner dictionary file that was created using the <code dir="ltr">STORE_IN_FLAT_FILE</code> option to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>. The information shown includes information about the database from which the LogMiner dictionary was created.</p>
</li>
<li>
<p><code dir="ltr"><a id="sthref1668"></a><a id="sthref1669"></a>V$LOGMNR_LOGS</code></p>
<p><a id="sthref1670"></a>Shows information about specified redo log files, as described in <a href="#i1010534">"Querying V$LOGMNR_LOGS"</a>.</p>
</li>
<li>
<p><code dir="ltr"><a id="sthref1671"></a><a id="sthref1672"></a>V$LOGMNR_PARAMETERS</code></p>
<p><a id="sthref1673"></a>Shows information about optional LogMiner parameters, including starting and ending system change numbers (SCNs) and starting and ending times.</p>
</li>
<li>
<p><a id="sthref1674"></a><a id="sthref1675"></a><a id="sthref1676"></a><code dir="ltr">V$DATABASE</code>, <code dir="ltr">DBA_LOG_GROUPS</code>, <code dir="ltr">ALL_LOG_GROUPS</code>, <code dir="ltr">USER_LOG_GROUPS</code>, <code dir="ltr">DBA_LOG_GROUP_COLUMN</code>S, <code dir="ltr">ALL_LOG_GROUP_COLUMNS</code>, <code dir="ltr">USER_LOG_GROUP_COLUMN</code>S</p>
<p>Shows information about the current settings for supplemental logging, as described in <a href="#i1014108">"Querying Views for Supplemental Logging Settings"</a>.</p>
</li>
</ul>
<a id="i1010534"></a>
<div id="SUTIL1595" class="sect2">
<h3 class="sect2">Querying V$LOGMNR_LOGS</h3>
<p><a id="sthref1677"></a><a id="sthref1678"></a><a id="sthref1679"></a>You can query the <code dir="ltr">V$LOGMNR_LOGS</code> view to determine which redo log files have been manually or automatically added to the list of redo log files for LogMiner to analyze. This view contains one row for each redo log file. It provides valuable information about each of the redo log files including file name, sequence #, SCN and time ranges, and whether it contains all or part of the LogMiner dictionary.</p>
<p>After a successful call to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>, the <code dir="ltr">STATUS</code> column of the <code dir="ltr">V$LOGMNR_LOGS</code> view contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">0</code></p>
<p>Indicates that the redo log file will be processed during a query of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
</li>
<li>
<p><code dir="ltr">1</code></p>
<p>Indicates that this will be the first redo log file to be processed by LogMiner during a select operation against the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
</li>
<li>
<p><code dir="ltr">2</code></p>
<p>Indicates that the redo log file has been pruned and therefore will not be processed by LogMiner during a query of the <code dir="ltr">V$LOGMNR_CONTENTS</code> view. It has been pruned because it is not needed to satisfy your requested time or SCN range.</p>
</li>
<li>
<p><code dir="ltr">4</code></p>
<p>Indicates that a redo log file (based on sequence number) is missing from the LogMiner redo log file list.</p>
</li>
</ul>
<p>The <code dir="ltr">V$LOGMNR_LOGS</code> view contains a row for each redo log file that is missing from the list, as follows:</p>
<ul>
<li>
<p>The <code dir="ltr">FILENAME</code> column will contain the consecutive range of sequence numbers and total SCN range gap.</p>
<p>For example: 'Missing log file(s) for thread number 1, sequence number(s) 100 to 102'.</p>
</li>
<li>
<p>The <code dir="ltr">INFO</code> column will contain the string 'MISSING_LOGFILE'.</p>
</li>
</ul>
<p>Information about files missing from the redo log file list can be useful for the following reasons:</p>
<ul>
<li>
<p>The <code dir="ltr">DDL_DICT_TRACKING</code> and <code dir="ltr">CONTINUOUS_MINE</code> options that can be specified when you call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> will not allow redo log files to be missing from the LogMiner redo log file list for the requested time or SCN range. If a call to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> fails, then you can query the <code dir="ltr">STATUS</code> column in the <code dir="ltr">V$LOGMNR_LOGS</code> view to determine which redo log files are missing from the list. You can then find and manually add these redo log files and attempt to call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> again.</p>
</li>
<li>
<p>Although all other options that can be specified when you call <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> allow files to be missing from the LogMiner redo log file list, you may not want to have missing files. You can query the <code dir="ltr">V$LOGMNR_LOGS</code> view before querying the <code dir="ltr">V$LOGMNR_CONTENTS</code> view to ensure that all required files are in the list. If the list is left with missing files and you query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, then a row is returned in <code dir="ltr">V$LOGMNR_CONTENTS</code> with the following column values:</p>
<ul>
<li>
<p>In the <code dir="ltr">OPERATION</code> column, a value of 'MISSING_SCN'</p>
</li>
<li>
<p>In the <code dir="ltr">STATUS</code> column, a value of <code dir="ltr">1291</code></p>
</li>
<li>
<p>In the <code dir="ltr">INFO</code> column, a string indicating the missing SCN range (for example, 'Missing SCN 100 - 200')</p>
</li>
</ul>
</li>
</ul>
</div>
<!-- class="sect2" -->
<a id="i1014108"></a>
<div id="SUTIL1596" class="sect2">
<h3 class="sect2">Querying Views for Supplemental Logging Settings</h3>
<p>You can query several views to determine the current settings for supplemental logging, as described in the following list:</p>
<ul>
<li>
<p><code dir="ltr">V$DATABASE</code> view</p>
<ul>
<li>
<p><code dir="ltr">SUPPLEMENTAL_LOG_DATA_FK</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">NO</code> - if database-level identification key logging with the <code dir="ltr">FOREIGN KEY</code> option is not enabled</p>
</li>
<li>
<p><code dir="ltr">YES</code> - if database-level identification key logging with the <code dir="ltr">FOREIGN KEY</code> option is enabled</p>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">SUPPLEMENTAL_LOG_DATA_ALL</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">NO</code> - if database-level identification key logging with the <code dir="ltr">ALL</code> option is not enabled</p>
</li>
<li>
<p><code dir="ltr">YES</code> - if database-level identification key logging with the <code dir="ltr">ALL</code> option is enabled</p>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">SUPPLEMENTAL_LOG_DATA_UI</code> column</p>
<ul>
<li>
<p><code dir="ltr">NO</code> - if database-level identification key logging with the <code dir="ltr">UNIQUE</code> option is not enabled</p>
</li>
<li>
<p><code dir="ltr">YES</code> - if database-level identification key logging with the <code dir="ltr">UNIQUE</code> option is enabled</p>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">SUPPLEMENTAL_LOG_DATA_MIN</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">NO</code> - if no database-level supplemental logging is enabled</p>
</li>
<li>
<p><code dir="ltr">IMPLICIT</code> - if minimal supplemental logging is enabled because database-level identification key logging options is enabled</p>
</li>
<li>
<p><code dir="ltr">YES</code> - if minimal supplemental logging is enabled because the SQL <code dir="ltr">ALTER</code> <code dir="ltr">DATABASE</code> <code dir="ltr">ADD</code> <code dir="ltr">SUPPLEMENTAL</code> <code dir="ltr">LOG</code> <code dir="ltr">DATA</code> statement was issued</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">DBA_LOG_GROUPS</code>, <code dir="ltr">ALL_LOG_GROUPS</code>, and <code dir="ltr">USER_LOG_GROUPS</code> views</p>
<ul>
<li>
<p><code dir="ltr">ALWAYS</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">ALWAYS</code> - indicates that the columns in this log group will be supplementally logged if any column in the associated row is updated</p>
</li>
<li>
<p><code dir="ltr">CONDITIONAL</code> - indicates that the columns in this group will be supplementally logged only if a column in the log group is updated</p>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">GENERATED</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">GENERATED NAME</code> - if the LOG_GROUP name was system-generated</p>
</li>
<li>
<p><code dir="ltr">USER NAME</code> - if the LOG_GROUP name was user-defined</p>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">LOG_GROUP_TYPES</code> column</p>
<p>This column contains one of the following values to indicate the type of logging defined for this log group. <code dir="ltr">USER LOG GROUP</code> indicates that the log group was user-defined (as opposed to system-generated).</p>
<ul>
<li>
<p>ALL COLUMN LOGGING</p>
</li>
<li>
<p>FOREIGN KEY LOGGING</p>
</li>
<li>
<p>PRIMARY KEY LOGGING</p>
</li>
<li>
<p>UNIQUE KEY LOGGING</p>
</li>
<li>
<p>USER LOG GROUP</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code dir="ltr">DBA_LOG_GROUP_COLUMNS</code>, <code dir="ltr">ALL_LOG_GROUP_COLUMNS</code>, and <code dir="ltr">USER_LOG_GROUP_COLUMNS</code> views</p>
<ul>
<li>
<p>The <code dir="ltr">LOGGING_PROPERTY</code> column</p>
<p>This column contains one of the following values:</p>
<ul>
<li>
<p><code dir="ltr">LOG</code> - indicates that this column in the log group will be supplementally logged</p>
</li>
<li>
<p><code dir="ltr">NO</code> <code dir="ltr">LOG</code> - indicates that this column in the log group will not be supplementally logged</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1009063"></a>
<div id="SUTIL1597" class="sect1">
<h2 class="sect1">Steps in a Typical LogMiner Session</h2>
<p>This section describes the steps<a id="sthref1680"></a> in a typical LogMiner session. Each step is described in its own subsection.</p>
<ol>
<li>
<p><a href="#i1006298">Enable Supplemental Logging</a></p>
</li>
<li>
<p><a href="#i1006321">Extract a LogMiner Dictionary</a> (unless you plan to use the online catalog)</p>
</li>
<li>
<p><a href="#i1006346">Specify Redo Log Files for Analysis</a></p>
</li>
<li>
<p><a href="#i1006391">Start LogMiner</a></p>
</li>
<li>
<p><a href="#CCHGFHDE">Query V$LOGMNR_CONTENTS</a></p>
</li>
<li>
<p><a href="#i1006510">End the LogMiner Session</a></p>
</li>
</ol>
<p>To run LogMiner, you use the <code dir="ltr">DBMS_LOGMNR</code> PL/SQL package. Additionally, you might also use the <code dir="ltr">DBMS_LOGMNR_D</code> package if you choose to extract a LogMiner dictionary rather than use the online catalog.</p>
<p>The <code dir="ltr">DBMS_LOGMNR</code> package contains the procedures used to initialize and run LogMiner, including interfaces to specify names of redo log files, filter criteria, and session characteristics. The <code dir="ltr">DBMS_LOGMNR_D</code> package queries the database dictionary tables of the current database to create a LogMiner dictionary file.</p>
<p>The LogMiner PL/SQL packages are owned by the <code dir="ltr">SYS</code> schema. Therefore, if you are not connected as user <code dir="ltr">SYS</code>, then:</p>
<ul>
<li>
<p>You must include <code dir="ltr">SYS</code> in your call. For example:</p>
<pre dir="ltr">
EXECUTE SYS.DBMS_LOGMNR.END_LOGMNR;
</pre></li>
<li>
<p>You must have been granted the <code dir="ltr">EXECUTE_CATALOG_ROLE</code> role.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<ul>
<li>
<p><a class="olink ARPLS" href="../../appdev.112/e40758/toc.htm"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for details about syntax and parameters for these LogMiner packages</p>
</li>
<li>
<p><a class="olink ADFNS" href="../../appdev.112/e41502/toc.htm"><span class="italic">Oracle Database Advanced Application Developer's Guide</span></a> for information about executing PL/SQL procedures</p>
</li>
</ul>
</div>
</li>
</ul>
<a id="i1006298"></a>
<div id="SUTIL1598" class="sect2">
<h3 class="sect2">Enable Supplemental Logging</h3>
<p>Enable the type of supplemental logging you want to use. At the very least, you must enable minimal supplemental logging, as follows:</p>
<pre dir="ltr">
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
</pre>
<p>See <a href="#i1021068">"Supplemental Logging"</a> for more information.</p>
</div>
<!-- class="sect2" -->
<a id="i1006321"></a>
<div id="SUTIL1599" class="sect2">
<h3 class="sect2">Extract a LogMiner Dictionary</h3>
<p>To use LogMiner, you must supply it with a dictionary by doing one of the following:</p>
<ul>
<li>
<p>Specify use of the online catalog by using the <code dir="ltr">DICT_FROM_ONLINE_CATALOG</code> option when you start LogMiner. See <a href="#i1014720">"Using the Online Catalog"</a>.</p>
</li>
<li>
<p>Extract database dictionary information to the redo log files. See <a href="#i1014735">"Extracting a LogMiner Dictionary to the Redo Log Files"</a>.</p>
</li>
<li>
<p>Extract database dictionary information to a flat file. See <a href="#i1014763">"Extracting the LogMiner Dictionary to a Flat File"</a>.</p>
</li>
</ul>
</div>
<!-- class="sect2" -->
<a id="i1006346"></a>
<div id="SUTIL1600" class="sect2">
<h3 class="sect2">Specify Redo Log Files for Analysis</h3>
<p>Before you can start LogMiner, you must specify the redo log files that you want to analyze<a id="sthref1681"></a>. To do so, execute the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> procedure, as demonstrated in the following steps. You can add and remove redo log files in any order.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you will be mining in the database instance that is generating the redo log files, then you only need to specify the <code dir="ltr">CONTINUOUS_MINE</code> option and one of the following when you start LogMiner:
<ul>
<li>
<p>The <code dir="ltr">STARTSCN</code> parameter</p>
</li>
<li>
<p>The <code dir="ltr">STARTTIME</code> parameter</p>
</li>
</ul>
<p>For more information, see <a href="#i1005688">"Redo Log File Options"</a>.</p>
</div>
<ol>
<li>
<p>Use SQL*Plus to start an Oracle instance, with the database either mounted or unmounted. For example, enter the <code dir="ltr">STARTUP</code> statement at the SQL prompt:</p>
<pre dir="ltr">
STARTUP
</pre></li>
<li>
<p>Create a list of redo log files. Specify the <code dir="ltr">NEW</code> option of the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> PL/SQL procedure to signal that this is the beginning of a new list. For example, enter the following to specify the <code dir="ltr">/oracle/logs/log1.f</code> redo log file:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/oracle/logs/log1.f', -
   OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre></li>
<li>
<p>If desired, add more redo log files by specifying the <code dir="ltr">ADDFILE</code> option of the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> PL/SQL procedure. For example, enter the following to add the <code dir="ltr">/oracle/logs/log2.f</code> redo log file:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/oracle/logs/log2.f', -
   OPTIONS =&gt; DBMS_LOGMNR.ADDFILE);
</pre>
<p>The <code dir="ltr">OPTIONS</code> parameter is optional when you are adding additional redo log files. For example, you could simply enter the following:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME=&gt;'/oracle/logs/log2.f');
</pre></li>
<li>
<p>If desired, remove redo log files by using the <code dir="ltr">DBMS_LOGMNR.REMOVE_LOGFILE</code> PL/SQL procedure. For example, enter the following to remove the <code dir="ltr">/oracle/logs/log2.f</code> redo log file:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.REMOVE_LOGFILE( -
   LOGFILENAME =&gt; '/oracle/logs/log2.f');
</pre></li>
</ol>
</div>
<!-- class="sect2" -->
<a id="i1006391"></a>
<div id="SUTIL1601" class="sect2">
<h3 class="sect2">Start LogMiner</h3>
<p><a id="i1006394"></a><a id="sthref1682"></a>After you have created a LogMiner dictionary file and specified which redo log files to analyze, you must start LogMiner. Take the following steps:</p>
<ol>
<li>
<p>Execute the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure to start LogMiner.</p>
<p>Oracle recommends that you specify a LogMiner dictionary option. If you do not, then LogMiner cannot translate internal object identifiers and datatypes to object names and external data formats. Therefore, it would return internal object IDs and present data as binary data. Additionally, the <code dir="ltr">MINE_VALUE</code> and <code dir="ltr">COLUMN_PRESENT</code> functions cannot be used without a dictionary.</p>
<p>If you are specifying the name of a flat file LogMiner dictionary, then you must supply a fully qualified file name for the dictionary file. For example, to start LogMiner using <code dir="ltr">/oracle/database/dictionary.ora</code>, issue the following statement:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   DICTFILENAME =&gt;'/oracle/database/dictionary.ora');
</pre>
<p>If you are not specifying a flat file dictionary name, then use the <code dir="ltr">OPTIONS</code> parameter to specify either the <code dir="ltr">DICT_FROM_REDO_LOGS</code> or <code dir="ltr">DICT_FROM_ONLINE_CATALOG</code> option.</p>
<p>If you specify <code dir="ltr">DICT_FROM_REDO_LOGS</code>, then LogMiner expects to find a dictionary in the redo log files that you specified with the <code dir="ltr">DBMS_LOGMNR.ADD_LOGFILE</code> procedure. To determine which redo log files contain a dictionary, look at the <code dir="ltr">V$ARCHIVED_LOG</code> view. See <a href="#i1014735">"Extracting a LogMiner Dictionary to the Redo Log Files"</a> for an example.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you add additional redo log files after LogMiner has been started, you must restart LogMiner. LogMiner will not retain options that were included in the previous call to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>; you must respecify the options you want to use. However, LogMiner will retain the dictionary specification from the previous call if you do not specify a dictionary in the current call to <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code>.</div>
<p>For more information about the <code dir="ltr">DICT_FROM_ONLINE_CATALOG</code> option, see <a href="#i1014720">"Using the Online Catalog"</a>.</p>
</li>
<li>
<p>Optionally, you can filter your query by time or by SCN. See <a href="#i1016328">"Filtering Data by Time"</a> or <a href="#i1016339">"Filtering Data by SCN"</a>.</p>
</li>
<li>
<p>You can also use the <code dir="ltr">OPTIONS</code> parameter to specify additional characteristics of your LogMiner session. For example, you might decide to use the online catalog as your LogMiner dictionary and to have only committed transactions shown in the <code dir="ltr">V$LOGMNR_CONTENTS</code> view, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(OPTIONS =&gt; -
   DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
   DBMS_LOGMNR.COMMITTED_DATA_ONLY);
</pre>
<p>For more information about <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> options, see <a class="olink ARPLS022" href="../../appdev.112/e40758/d_logmnr.htm#ARPLS022"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a>.</p>
<p>You can execute the <code dir="ltr">DBMS_LOGMNR</code>.<code dir="ltr">START_LOGMNR</code> procedure multiple times, specifying different options each time. This can be useful, for example, if you did not get the desired results from a query of <code dir="ltr">V$LOGMNR_CONTENTS</code>, and want to restart LogMiner with different options. Unless you need to respecify the LogMiner dictionary, you do not need to add redo log files if they were already added with a previous call to <code dir="ltr">DBMS_LOGMNR</code>.<code dir="ltr">START_LOGMNR</code>.</p>
</li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CCHGFHDE"></a>
<div id="SUTIL1602" class="sect2">
<h3 class="sect2">Query V$LOGMNR_CONTENTS</h3>
<p>At this point, LogMiner is started and you can perform queries against the <code dir="ltr">V$LOGMNR_CONTENTS</code> view. See <a href="#i1016539">"Filtering and Formatting Data Returned to V$LOGMNR_CONTENTS"</a> for examples of this.</p>
</div>
<!-- class="sect2" -->
<a id="i1006510"></a>
<div id="SUTIL1603" class="sect2">
<h3 class="sect2">End the LogMiner Session</h3>
<p>To properly end<a id="sthref1683"></a> a LogMiner session, use the <code dir="ltr">DBMS_LOGMNR.END_LOGMNR</code> PL/SQL procedure, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR;
</pre>
<p>This procedure closes all the redo log files and allows all the database and system resources allocated by LogMiner to be released.</p>
<p>If this procedure is not executed, then LogMiner retains all its allocated resources until the end of the Oracle session in which it was invoked. It is particularly important to use this procedure to end the LogMiner session if either the <code dir="ltr">DDL_DICT_TRACKING</code> option or the <code dir="ltr">DICT_FROM_REDO_LOGS</code> option was used.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1028311"></a>
<div id="SUTIL1604" class="sect1">
<h2 class="sect1">Examples Using LogMiner</h2>
<p>This section provides several examples of using LogMiner in each of the following general categories:</p>
<ul>
<li>
<p><a href="#i1028506">Examples of Mining by Explicitly Specifying the Redo Log Files of Interest</a></p>
</li>
<li>
<p><a href="#i1027809">Examples of Mining Without Specifying the List of Redo Log Files Explicitly</a></p>
</li>
<li>
<p><a href="#i1028785">Example Scenarios</a></p>
<div class="infobox-note">
<p class="notep1">Note:</p>
All examples in this section assume that minimal supplemental logging has been enabled:
<pre dir="ltr">
SQL&gt; ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
</pre>
<p>See <a href="#i1021068">"Supplemental Logging"</a> for more information.</p>
<p>All examples, except <a href="#i1028800">"Example 2: Mining the Redo Log Files in a Given SCN Range"</a> and the <a href="#i1028785">"Example Scenarios"</a>, assume that the <code dir="ltr">NLS_DATE_FORMAT</code> parameter has been set as follows:</p>
<pre dir="ltr">
SQL&gt;  ALTER SESSION SET NLS_DATE_FORMAT = 'dd-mon-yyyy hh24:mi:ss';
</pre>
<p>Because LogMiner displays date data using the setting for the <code dir="ltr">NLS_DATE_FORMAT</code> parameter that is active for the user session, this step is optional. However, setting the parameter explicitly lets you predict the date format.</p>
</div>
</li>
</ul>
<a id="i1028506"></a>
<div id="SUTIL1605" class="sect2">
<h3 class="sect2">Examples of Mining by Explicitly Specifying the Redo Log Files of Interest</h3>
<p>The following examples demonstrate how to use LogMiner when you know which redo log files contain the data of interest. This section contains the following list of examples; these examples are best read sequentially, because each example builds on the example or examples that precede it:</p>
<ul>
<li>
<p><a href="#i1041492">Example 1: Finding All Modifications in the Last Archived Redo Log File</a></p>
</li>
<li>
<p><a href="#i1028820">Example 2: Grouping DML Statements into Committed Transactions</a></p>
</li>
<li>
<p><a href="#i1028825">Example 3: Formatting the Reconstructed SQL</a></p>
</li>
<li>
<p><a href="#i1028830">Example 4: Using the LogMiner Dictionary in the Redo Log Files</a></p>
</li>
<li>
<p><a href="#i1028835">Example 5: Tracking DDL Statements in the Internal Dictionary</a></p>
</li>
<li>
<p><a href="#i1028846">Example 6: Filtering Output by Time Range</a></p>
</li>
</ul>
<p>The SQL output formatting may be different on your display than that shown in these examples.</p>
<a id="i1041492"></a>
<div id="SUTIL1606" class="sect3">
<h4 class="sect3">Example 1: Finding All Modifications in the Last Archived Redo Log File</h4>
<p>The easiest way to examine the modification history of a database is to mine at the source database and use the online catalog to translate the redo log files. This example shows how to do the simplest analysis using LogMiner.</p>
<p>This example finds all modifications that are contained in the last archived redo log generated by the database (assuming that the database is not an Oracle Real Application Clusters (Oracle RAC) database).</p>
<dl>
<dd><a id="SUTIL3899"></a><a id="sthref1684"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine which redo log file was most recently archived.</dt>
<dd>
<p>This example assumes that you know you want to mine the redo log file that was most recently archived.</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG
   WHERE FIRST_TIME = (SELECT MAX(FIRST_TIME) FROM V$ARCHIVED_LOG);

NAME                            
-------------------------------------------
/usr/oracle/data/db1arch_1_16_482701534.dbf
</pre></dd>
<dd><a id="SUTIL3900"></a><a id="sthref1685"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Specify the list of redo log files to be analyzed.</dt>
<dd>
<p>Specify the redo log file that was returned by the query in Step 1. The list will consist of one redo log file.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
  LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_16_482701534.dbf', -
  OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre></dd>
<dd><a id="SUTIL3901"></a><a id="sthref1686"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner and specify the dictionary to use.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG);
</pre></dd>
<dd><a id="SUTIL3902"></a><a id="sthref1687"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>Note that there are four transactions (two of them were committed within the redo log file being analyzed, and two were not). The output shows the DML statements in the order in which they were executed; thus transactions interleave among themselves.</p>
<pre dir="ltr">
SELECT username AS USR, (XIDUSN || '.' || XIDSLT || '.' ||  XIDSQN) AS XID, 
   SQL_REDO, SQL_UNDO FROM V$LOGMNR_CONTENTS WHERE username IN ('HR', 'OE');

USR    XID          SQL_REDO                        SQL_UNDO
----   ---------  ----------------------------------------------------
HR     1.11.1476  set transaction read write;

HR     1.11.1476  insert into "HR"."EMPLOYEES"(     delete from "HR"."EMPLOYEES" 
                  "EMPLOYEE_ID","FIRST_NAME",       where "EMPLOYEE_ID" = '306'
                  "LAST_NAME","EMAIL",              and "FIRST_NAME" = 'Nandini'
                  "PHONE_NUMBER","HIRE_DATE",       and "LAST_NAME" = 'Shastry'
                  "JOB_ID","SALARY",                and "EMAIL" = 'NSHASTRY'
                  "COMMISSION_PCT","MANAGER_ID",    and "PHONE_NUMBER" = '1234567890'
                  "DEPARTMENT_ID") values           and "HIRE_DATE" = TO_DATE('10-JAN-2003
                  ('306','Nandini','Shastry',       13:34:43', 'dd-mon-yyyy hh24:mi:ss') 
                  'NSHASTRY', '1234567890',         and "JOB_ID" = 'HR_REP' and 
                  TO_DATE('10-jan-2003 13:34:43',   "SALARY" = '120000' and 
                  'dd-mon-yyyy hh24:mi:ss'),         "COMMISSION_PCT" = '.05' and
                  'HR_REP','120000', '.05',         "DEPARTMENT_ID" = '10' and
                  '105','10');                      ROWID = 'AAAHSkAABAAAY6rAAO';
     
OE     1.1.1484   set transaction read write;

OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  update "OE"."PRODUCT_INFORMATION" 
                  set "WARRANTY_PERIOD" =            set "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+05-00') where      TO_YMINTERVAL('+01-00') where
                  "PRODUCT_ID" = '1799' and          "PRODUCT_ID" = '1799' and
                  "WARRANTY_PERIOD" =                "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+01-00') and        TO_YMINTERVAL('+05-00') and
                  ROWID = 'AAAHTKAABAAAY9mAAB';      ROWID = 'AAAHTKAABAAAY9mAAB'; 
                                                                                
OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  update "OE"."PRODUCT_INFORMATION"
                  set "WARRANTY_PERIOD" =            set "WARRANTY_PERIOD" =
                  TO_YMINTERVAL('+05-00') where      TO_YMINTERVAL('+01-00') where
                  "PRODUCT_ID" = '1801' and          "PRODUCT_ID" = '1801' and
                  "WARRANTY_PERIOD" =                "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+01-00') and        TO_YMINTERVAL('+05-00') and
                  ROWID = 'AAAHTKAABAAAY9mAAC';      ROWID ='AAAHTKAABAAAY9mAAC';

HR     1.11.1476  insert into "HR"."EMPLOYEES"(     delete from "HR"."EMPLOYEES"
                  "EMPLOYEE_ID","FIRST_NAME",       "EMPLOYEE_ID" = '307' and 
                  "LAST_NAME","EMAIL",              "FIRST_NAME" = 'John' and
                  "PHONE_NUMBER","HIRE_DATE",       "LAST_NAME" = 'Silver' and
                  "JOB_ID","SALARY",                "EMAIL" = 'JSILVER' and 
                  "COMMISSION_PCT","MANAGER_ID",    "PHONE_NUMBER" = '5551112222'
                  "DEPARTMENT_ID") values           and "HIRE_DATE" = TO_DATE('10-jan-2003
                  ('307','John','Silver',           13:41:03', 'dd-mon-yyyy hh24:mi:ss') 
                   'JSILVER', '5551112222',         and "JOB_ID" ='105' and "DEPARTMENT_ID" 
                  TO_DATE('10-jan-2003 13:41:03',   = '50' and ROWID = 'AAAHSkAABAAAY6rAAP'; 
                  'dd-mon-yyyy hh24:mi:ss'),
                  'SH_CLERK','110000', '.05',
                  '105','50');                

OE     1.1.1484   commit;

HR     1.15.1481   set transaction read write;

HR     1.15.1481  delete from "HR"."EMPLOYEES"      insert into "HR"."EMPLOYEES"(
                  where "EMPLOYEE_ID" = '205' and   "EMPLOYEE_ID","FIRST_NAME",
                  "FIRST_NAME" = 'Shelley' and      "LAST_NAME","EMAIL","PHONE_NUMBER",
                  "LAST_NAME" = 'Higgins' and       "HIRE_DATE", "JOB_ID","SALARY",
                  "EMAIL" = 'SHIGGINS' and          "COMMISSION_PCT","MANAGER_ID",
                  "PHONE_NUMBER" = '515.123.8080'   "DEPARTMENT_ID") values
                  and "HIRE_DATE" = TO_DATE(        ('205','Shelley','Higgins',
                  '07-jun-1994 10:05:01',           and     'SHIGGINS','515.123.8080',
                  'dd-mon-yyyy hh24:mi:ss')         TO_DATE('07-jun-1994 10:05:01',
                  and "JOB_ID" = 'AC_MGR'           'dd-mon-yyyy hh24:mi:ss'),
                  and "SALARY"= '12000'            'AC_MGR','12000',NULL,'101','110'); 
                  and "COMMISSION_PCT" IS NULL 
                  and "MANAGER_ID" 
                  = '101' and "DEPARTMENT_ID" = 
                  '110' and ROWID = 
                  'AAAHSkAABAAAY6rAAM';


OE     1.8.1484   set transaction read write;

OE     1.8.1484   update "OE"."PRODUCT_INFORMATION"  update "OE"."PRODUCT_INFORMATION"
                  set "WARRANTY_PERIOD" =            set "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+12-06') where      TO_YMINTERVAL('+20-00') where
                  "PRODUCT_ID" = '2350' and          "PRODUCT_ID" = '2350' and
                  "WARRANTY_PERIOD" =                "WARRANTY_PERIOD" =
                  TO_YMINTERVAL('+20-00') and        TO_YMINTERVAL('+20-00') and
                  ROWID = 'AAAHTKAABAAAY9tAAD';       ROWID ='AAAHTKAABAAAY9tAAD'; 

HR     1.11.1476  commit;
</pre></dd>
<dd><a id="SUTIL3903"></a><a id="sthref1688"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
SQL&gt; EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028820"></a>
<div id="SUTIL1607" class="sect3">
<h4 class="sect3">Example 2: Grouping DML Statements into Committed Transactions</h4>
<p>As shown in the first example, <a href="#i1041492">"Example 1: Finding All Modifications in the Last Archived Redo Log File"</a>, LogMiner displays all modifications it finds in the redo log files that it analyzes by default, regardless of whether the transaction has been committed or not. In addition, LogMiner shows modifications in the same order in which they were executed. Because DML statements that belong to the same transaction are not grouped together, visual inspection of the output can be difficult. Although you can use SQL to group transactions, LogMiner provides an easier way. In this example, the latest archived redo log file will again be analyzed, but it will return only committed transactions.</p>
<dl>
<dd><a id="SUTIL3904"></a><a id="sthref1689"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine which redo log file was most recently archived by the database.</dt>
<dd>
<p>This example assumes that you know you want to mine the redo log file that was most recently archived.</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG
   WHERE FIRST_TIME = (SELECT MAX(FIRST_TIME) FROM V$ARCHIVED_LOG);

NAME                            
-------------------------------------------
/usr/oracle/data/db1arch_1_16_482701534.dbf
</pre></dd>
<dd><a id="SUTIL3905"></a><a id="sthref1690"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Specify the list of redo log files to be analyzed.</dt>
<dd>
<p>Specify the redo log file that was returned by the query in Step 1. The list will consist of one redo log file.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_16_482701534.dbf', -
   OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre></dd>
<dd><a id="SUTIL3906"></a><a id="sthref1691"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner by specifying the dictionary to use and the <code dir="ltr">COMMITTED_DATA_ONLY</code> option.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
   DBMS_LOGMNR.COMMITTED_DATA_ONLY); 
</pre></dd>
<dd><a id="SUTIL3907"></a><a id="sthref1692"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>Although transaction 1.11.1476 was started before transaction 1.1.1484 (as revealed in <a href="#i1041492">"Example 1: Finding All Modifications in the Last Archived Redo Log File"</a>), it committed after transaction 1.1.1484 committed. In this example, therefore, transaction 1.1.1484 is shown in its entirety before transaction 1.11.1476. The two transactions that did not commit within the redo log file being analyzed are not returned.</p>
<pre dir="ltr">
SELECT username AS USR, (XIDUSN || '.' || XIDSLT || '.' ||  XIDSQN) AS XID, SQL_REDO, 
   SQL_UNDO FROM V$LOGMNR_CONTENTS WHERE username IN ('HR', 'OE');
;
USR    XID          SQL_REDO                        SQL_UNDO
----   ---------    ------------------------------- ---------------------------------
     
OE     1.1.1484   set transaction read write;

OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  update "OE"."PRODUCT_INFORMATION" 
                  set "WARRANTY_PERIOD" =            set "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+05-00') where      TO_YMINTERVAL('+01-00') where
                  "PRODUCT_ID" = '1799' and          "PRODUCT_ID" = '1799' and
                  "WARRANTY_PERIOD" =                "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+01-00') and        TO_YMINTERVAL('+05-00') and
                  ROWID = 'AAAHTKAABAAAY9mAAB';      ROWID = 'AAAHTKAABAAAY9mAAB'; 
                                                                                
OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  update "OE"."PRODUCT_INFORMATION"
                  set "WARRANTY_PERIOD" =            set "WARRANTY_PERIOD" =
                  TO_YMINTERVAL('+05-00') where      TO_YMINTERVAL('+01-00') where
                  "PRODUCT_ID" = '1801' and          "PRODUCT_ID" = '1801' and
                  "WARRANTY_PERIOD" =                "WARRANTY_PERIOD" = 
                  TO_YMINTERVAL('+01-00') and        TO_YMINTERVAL('+05-00') and
                  ROWID = 'AAAHTKAABAAAY9mAAC';      ROWID ='AAAHTKAABAAAY9mAAC';

OE     1.1.1484   commit;
                            
HR     1.11.1476  set transaction read write;

HR     1.11.1476  insert into "HR"."EMPLOYEES"(     delete from "HR"."EMPLOYEES" 
                  "EMPLOYEE_ID","FIRST_NAME",       where "EMPLOYEE_ID" = '306'
                  "LAST_NAME","EMAIL",              and "FIRST_NAME" = 'Nandini'
                  "PHONE_NUMBER","HIRE_DATE",       and "LAST_NAME" = 'Shastry'
                  "JOB_ID","SALARY",                and "EMAIL" = 'NSHASTRY'
                  "COMMISSION_PCT","MANAGER_ID",    and "PHONE_NUMBER" = '1234567890'
                  "DEPARTMENT_ID") values           and "HIRE_DATE" = TO_DATE('10-JAN-2003
                  ('306','Nandini','Shastry',       13:34:43', 'dd-mon-yyyy hh24:mi:ss') 
                  'NSHASTRY', '1234567890',         and "JOB_ID" = 'HR_REP' and 
                  TO_DATE('10-jan-2003 13:34:43',   "SALARY" = '120000' and 
                  'dd-mon-yyy hh24:mi:ss'),         "COMMISSION_PCT" = '.05' and
                  'HR_REP','120000', '.05',         "DEPARTMENT_ID" = '10' and
                  '105','10');                      ROWID = 'AAAHSkAABAAAY6rAAO';

HR     1.11.1476  insert into "HR"."EMPLOYEES"(     delete from "HR"."EMPLOYEES"
                  "EMPLOYEE_ID","FIRST_NAME",       "EMPLOYEE_ID" = '307' and 
                  "LAST_NAME","EMAIL",              "FIRST_NAME" = 'John' and
                  "PHONE_NUMBER","HIRE_DATE",       "LAST_NAME" = 'Silver' and
                  "JOB_ID","SALARY",                "EMAIL" = 'JSILVER' and 
                  "COMMISSION_PCT","MANAGER_ID",    "PHONE_NUMBER" = '5551112222'
                  "DEPARTMENT_ID") values           and "HIRE_DATE" = TO_DATE('10-jan-2003
                  ('307','John','Silver',           13:41:03', 'dd-mon-yyyy hh24:mi:ss') 
                   'JSILVER', '5551112222',         and "JOB_ID" ='105' and "DEPARTMENT_ID" 
                  TO_DATE('10-jan-2003 13:41:03',   = '50' and ROWID = 'AAAHSkAABAAAY6rAAP'; 
                  'dd-mon-yyyy hh24:mi:ss'),
                  'SH_CLERK','110000', '.05',
                  '105','50');                

HR     1.11.1476  commit;
</pre></dd>
<dd><a id="SUTIL3908"></a><a id="sthref1693"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028825"></a>
<div id="SUTIL1608" class="sect3">
<h4 class="sect3">Example 3: Formatting the Reconstructed SQL</h4>
<p>As shown in <a href="#i1028820">"Example 2: Grouping DML Statements into Committed Transactions"</a>, using the <code dir="ltr">COMMITTED_DATA_ONLY</code> option with the dictionary in the online redo log file is an easy way to focus on committed transactions. However, one aspect remains that makes visual inspection difficult: the association between the column names and their respective values in an <code dir="ltr">INSERT</code> statement are not apparent. This can be addressed by specifying the <code dir="ltr">PRINT_PRETTY_SQL</code> option. Note that specifying this option will make some of the reconstructed SQL statements nonexecutable.</p>
<dl>
<dd><a id="SUTIL3909"></a><a id="sthref1694"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine which redo log file was most recently archived.</dt>
<dd>
<p>This example assumes that you know you want to mine the redo log file that was most recently archived.</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG
   WHERE FIRST_TIME = (SELECT MAX(FIRST_TIME) FROM V$ARCHIVED_LOG);

NAME                            
-------------------------------------------
/usr/oracle/data/db1arch_1_16_482701534.dbf
</pre></dd>
<dd><a id="SUTIL3910"></a><a id="sthref1695"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Specify the list of redo log files to be analyzed.</dt>
<dd>
<p>Specify the redo log file that was returned by the query in Step 1. The list will consist of one redo log file.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_16_482701534.dbf', -
   OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre></dd>
<dd><a id="SUTIL3911"></a><a id="sthref1696"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner by specifying the dictionary to use and the <code dir="ltr">COMMITTED_DATA_ONLY</code> and <code dir="ltr">PRINT_PRETTY_SQL</code> options.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
              DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
              DBMS_LOGMNR.PRINT_PRETTY_SQL);
</pre>
<p>The <code dir="ltr">DBMS_LOGMNR.PRINT_PRETTY_SQL</code> option changes only the format of the reconstructed SQL, and therefore is useful for generating reports for visual inspection.</p>
</dd>
<dd><a id="SUTIL3912"></a><a id="sthref1697"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view for SQL_REDO statements.</dt>
<dd>
<pre dir="ltr">
SELECT username AS USR, (XIDUSN || '.' || XIDSLT || '.' ||  XIDSQN) AS XID, SQL_REDO 
   FROM V$LOGMNR_CONTENTS;

USR    XID          SQL_REDO                     
----   ---------  -----------------------------------------------------

OE     1.1.1484   set transaction read write;

OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  
                    set 
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') 
                    where
                      "PRODUCT_ID" = '1799' and          
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+01-00') and        
                      ROWID = 'AAAHTKAABAAAY9mAAB';  
                                                                                
OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"
                    set 
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') 
                    where
                      "PRODUCT_ID" = '1801' and
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+01-00') and   
                      ROWID = 'AAAHTKAABAAAY9mAAC'; 

OE     1.1.1484   commit;
                            
HR     1.11.1476  set transaction read write;

HR     1.11.1476  insert into "HR"."EMPLOYEES"
                   values
                     "EMPLOYEE_ID" = 306,
                     "FIRST_NAME" = 'Nandini',
                     "LAST_NAME" = 'Shastry',
                     "EMAIL" = 'NSHASTRY',
                     "PHONE_NUMBER" = '1234567890',
                     "HIRE_DATE" = TO_DATE('10-jan-2003 13:34:43', 
                     'dd-mon-yyyy hh24:mi:ss',
                     "JOB_ID" = 'HR_REP',
                     "SALARY" = 120000,
                     "COMMISSION_PCT" = .05,
                     "MANAGER_ID" = 105,
                     "DEPARTMENT_ID" = 10;

HR     1.11.1476   insert into "HR"."EMPLOYEES"
                    values
                       "EMPLOYEE_ID" = 307,
                       "FIRST_NAME" = 'John',
                       "LAST_NAME" = 'Silver',
                       "EMAIL" = 'JSILVER',
                       "PHONE_NUMBER" = '5551112222',
                       "HIRE_DATE" = TO_DATE('10-jan-2003 13:41:03',
                       'dd-mon-yyyy hh24:mi:ss'),
                       "JOB_ID" = 'SH_CLERK',
                       "SALARY" = 110000,
                       "COMMISSION_PCT" = .05,
                       "MANAGER_ID" = 105,
                       "DEPARTMENT_ID" = 50;
HR     1.11.1476    commit;
</pre></dd>
<dd><a id="SUTIL3913"></a><a id="sthref1698"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view for reconstructed SQL_UNDO statements.</dt>
<dd>
<pre dir="ltr">
SELECT username AS USR, (XIDUSN || '.' || XIDSLT || '.' ||  XIDSQN) AS XID, SQL_UNDO 
   FROM V$LOGMNR_CONTENTS;

USR   XID        SQL_UNDO                     
----   ---------  -----------------------------------------------------

     
OE     1.1.1484   set transaction read write;

OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"  
                    set 
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+01-00') 
                    where
                      "PRODUCT_ID" = '1799' and          
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and        
                      ROWID = 'AAAHTKAABAAAY9mAAB';  
                                                                                
OE     1.1.1484   update "OE"."PRODUCT_INFORMATION"
                    set 
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+01-00') 
                    where
                      "PRODUCT_ID" = '1801' and
                      "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and   
                      ROWID = 'AAAHTKAABAAAY9mAAC'; 

OE     1.1.1484   commit;
                            
HR     1.11.1476  set transaction read write;

HR     1.11.1476  delete from "HR"."EMPLOYEES"
                  where
                     "EMPLOYEE_ID" = 306 and
                     "FIRST_NAME" = 'Nandini' and
                     "LAST_NAME" = 'Shastry' and
                     "EMAIL" = 'NSHASTRY' and
                     "PHONE_NUMBER" = '1234567890' and
                     "HIRE_DATE" = TO_DATE('10-jan-2003 13:34:43',
                     'dd-mon-yyyy hh24:mi:ss') and
                     "JOB_ID" = 'HR_REP' and 
                     "SALARY" = 120000 and
                     "COMMISSION_PCT" = .05 and
                     "MANAGER_ID" = 105 and
                     "DEPARTMENT_ID" = 10 and
                     ROWID = 'AAAHSkAABAAAY6rAAO';

HR     1.11.1476   delete from "HR"."EMPLOYEES"
                   where
                       "EMPLOYEE_ID" = 307 and
                       "FIRST_NAME" = 'John' and
                       "LAST_NAME" = 'Silver' and
                       "EMAIL" = 'JSILVER' and
                       "PHONE_NUMBER" = '555122122' and
                       "HIRE_DATE" = TO_DATE('10-jan-2003 13:41:03',
                       'dd-mon-yyyy hh24:mi:ss') and
                       "JOB_ID" = 'SH_CLERK' and
                       "SALARY" = 110000 and
                       "COMMISSION_PCT" = .05 and
                       "MANAGER_ID" = 105 and
                       "DEPARTMENT_ID" = 50 and
                       ROWID = 'AAAHSkAABAAAY6rAAP'; 
HR     1.11.1476    commit;
</pre></dd>
<dd><a id="SUTIL3914"></a><a id="sthref1699"></a></dd>
<dt class="seghead">Step 6&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028830"></a>
<div id="SUTIL1609" class="sect3">
<h4 class="sect3">Example 4: Using the LogMiner Dictionary in the Redo Log Files</h4>
<p>This example shows how to use the dictionary that has been extracted to the redo log files. When you use the dictionary in the online catalog, you must mine the redo log files in the same database that generated them. Using the dictionary contained in the redo log files enables you to mine redo log files in a different database.</p>
<dl>
<dd><a id="SUTIL3915"></a><a id="sthref1700"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine which redo log file was most recently archived by the database.</dt>
<dd>
<p>This example assumes that you know you want to mine the redo log file that was most recently archived.</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE# FROM V$ARCHIVED_LOG
   WHERE FIRST_TIME = (SELECT MAX(FIRST_TIME) FROM V$ARCHIVED_LOG);

NAME                                           SEQUENCE#
--------------------------------------------   --------------
/usr/oracle/data/db1arch_1_210_482701534.dbf   210
</pre></dd>
<dd><a id="SUTIL3916"></a><a id="sthref1701"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Find the redo log files containing the dictionary.</dt>
<dd>
<p>The dictionary may be contained in more than one redo log file. Therefore, you need to determine which redo log files contain the start and end of the dictionary. Query the <code dir="ltr">V$ARCHIVED_LOG</code> view, as follows:</p>
<ol>
<li>
<p>Find a redo log file that contains the end of the dictionary extract. This redo log file must have been created before the redo log file that you want to analyze, but should be as recent as possible.</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE#, DICTIONARY_BEGIN d_beg, DICTIONARY_END d_end
   FROM V$ARCHIVED_LOG
   WHERE SEQUENCE# = (SELECT MAX (SEQUENCE#) FROM V$ARCHIVED_LOG
   WHERE DICTIONARY_END = 'YES' and SEQUENCE# &lt;= 210);

NAME                                           SEQUENCE#    D_BEG  D_END
--------------------------------------------   ----------   -----  ------
/usr/oracle/data/db1arch_1_208_482701534.dbf   208          NO     YES
</pre></li>
<li>
<p>Find the redo log file that contains the start of the data dictionary extract that matches the end of the dictionary found in the previous step:</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE#, DICTIONARY_BEGIN d_beg, DICTIONARY_END d_end
   FROM V$ARCHIVED_LOG
   WHERE SEQUENCE# = (SELECT MAX (SEQUENCE#) FROM V$ARCHIVED_LOG
   WHERE DICTIONARY_BEGIN = 'YES' and SEQUENCE# &lt;= 208);

NAME                                           SEQUENCE#    D_BEG  D_END
--------------------------------------------   ----------   -----  ------
/usr/oracle/data/db1arch_1_207_482701534.dbf   207          YES     NO
</pre></li>
<li>
<p>Specify the list of the redo log files of interest. Add the redo log files that contain the start and end of the dictionary and the redo log file that you want to analyze. You can add the redo log files in any order.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_210_482701534.dbf', -
       OPTIONS =&gt; DBMS_LOGMNR.NEW);
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_208_482701534.dbf');
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_207_482701534.dbf');
</pre></li>
<li>
<p>Query the <code dir="ltr">V$LOGMNR_LOGS</code> view to display the list of redo log files to be analyzed, including their timestamps.</p>
<p>In the output, LogMiner flags a missing redo log file. LogMiner lets you proceed with mining, provided that you do not specify an option that requires the missing redo log file for proper functioning.</p>
</li>
</ol>
<pre dir="ltr">
SQL&gt; SELECT FILENAME AS name, LOW_TIME, HIGH_TIME FROM V$LOGMNR_LOGS;
 NAME                                  LOW_TIME              HIGH_TIME
-------------------------------------   --------------------  --------------------
/usr/data/db1arch_1_207_482701534.dbf   10-jan-2003 12:01:34  10-jan-2003 13:32:46

/usr/data/db1arch_1_208_482701534.dbf   10-jan-2003 13:32:46  10-jan-2003 15:57:03

Missing logfile(s) for thread number 1, 10-jan-2003 15:57:03  10-jan-2003 15:59:53 
sequence number(s) 209 to 209

/usr/data/db1arch_1_210_482701534.dbf   10-jan-2003 15:59:53  10-jan-2003 16:07:41
</pre></dd>
<dd><a id="SUTIL3917"></a><a id="sthref1702"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner by specifying the dictionary to use and the <code dir="ltr">COMMITTED_DATA_ONLY</code> and <code dir="ltr">PRINT_PRETTY_SQL</code> options.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_REDO_LOGS + -
              DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
              DBMS_LOGMNR.PRINT_PRETTY_SQL);
</pre></dd>
<dd><a id="SUTIL3918"></a><a id="sthref1703"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>To reduce the number of rows returned by the query, exclude from the query all DML statements done in the <code dir="ltr">SYS</code> or <code dir="ltr">SYSTEM</code> schemas. (This query specifies a timestamp to exclude transactions that were involved in the dictionary extraction.)</p>
<p>The output shows three transactions: two DDL transactions and one DML transaction. The DDL transactions, 1.2.1594 and 1.18.1602, create the table <code dir="ltr">oe.product_tracking</code> and create a trigger on table <code dir="ltr">oe.product_information</code>, respectively. In both transactions, the DML statements done to the system tables (tables owned by <code dir="ltr">SYS</code>) are filtered out because of the query predicate.</p>
<p>The DML transaction, 1.9.1598, updates the <code dir="ltr">oe.product_information</code> table. The update operation in this transaction is fully translated. However, the query output also contains some untranslated reconstructed SQL statements. Most likely, these statements were done on the <code dir="ltr">oe.product_tracking</code> table that was created after the data dictionary was extracted to the redo log files.</p>
<p>(The next example shows how to run LogMiner with the <code dir="ltr">DDL_DICT_TRACKING</code> option so that all SQL statements are fully translated; no binary data is returned.)</p>
<pre dir="ltr">
SELECT USERNAME AS usr, SQL_REDO FROM V$LOGMNR_CONTENTS 
   WHERE SEG_OWNER IS NULL OR SEG_OWNER NOT IN ('SYS', 'SYSTEM') AND
   TIMESTAMP &gt; '10-jan-2003 15:59:53';

USR             XID         SQL_REDO
---             --------    -----------------------------------
SYS             1.2.1594    set transaction read write;
SYS             1.2.1594    create table oe.product_tracking (product_id number not null,
                            modified_time date,
                            old_list_price number(8,2),
                            old_warranty_period interval year(2) to month);
SYS             1.2.1594    commit;

SYS             1.18.1602   set transaction read write;
SYS             1.18.1602   create or replace trigger oe.product_tracking_trigger
                            before update on oe.product_information
                            for each row
                            when (new.list_price &lt;&gt; old.list_price or
                                  new.warranty_period &lt;&gt; old.warranty_period)
                            declare
                            begin
                            insert into oe.product_tracking values 
                               (:old.product_id, sysdate,
                                :old.list_price, :old.warranty_period);
                            end;
SYS             1.18.1602   commit;

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 100
                              where
                                "PRODUCT_ID" = 1729 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 80 and
                                ROWID = 'AAAHTKAABAAAY9yAAA';

OE              1.9.1598    insert into "UNKNOWN"."OBJ# 33415"
                              values
                                "COL 1" = HEXTORAW('c2121e'),
                                "COL 2" = HEXTORAW('7867010d110804'),
                                "COL 3" = HEXTORAW('c151'),
                                "COL 4" = HEXTORAW('800000053c');

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 92
                              where
                                "PRODUCT_ID" = 2340 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 72 and
                                ROWID = 'AAAHTKAABAAAY9zAAA';

OE              1.9.1598    insert into "UNKNOWN"."OBJ# 33415"
                              values
                                "COL 1" = HEXTORAW('c21829'),
                                "COL 2" = HEXTORAW('7867010d110808'),
                                "COL 3" = HEXTORAW('c149'),
                                "COL 4" = HEXTORAW('800000053c');

OE              1.9.1598     commit;
</pre></dd>
<dd><a id="SUTIL3919"></a><a id="sthref1704"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;Issue additional queries, if desired.</dt>
<dd>
<p>Display all the DML statements that were executed as part of the <code dir="ltr">CREATE TABLE</code> DDL statement. This includes statements executed by users and internally by Oracle.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you choose to reapply statements displayed by a query such as the one shown here, then reapply DDL statements only. Do not reapply DML statements that were executed internally by Oracle, or you risk corrupting your database. In the following output, the only statement that you should use in a reapply operation is the <code dir="ltr">CREATE TABLE OE.PRODUCT_TRACKING</code> statement.</div>
<pre dir="ltr">
SELECT SQL_REDO FROM V$LOGMNR_CONTENTS
   WHERE XIDUSN  = 1 and XIDSLT = 2 and XIDSQN = 1594;

SQL_REDO
--------------------------------------------------------------------------------
set transaction read write;

insert into "SYS"."OBJ$"
 values
    "OBJ#" = 33415,
    "DATAOBJ#" = 33415,
    "OWNER#" = 37,
    "NAME" = 'PRODUCT_TRACKING',
    "NAMESPACE" = 1,
    "SUBNAME" IS NULL,
    "TYPE#" = 2,
    "CTIME" = TO_DATE('13-jan-2003 14:01:03', 'dd-mon-yyyy hh24:mi:ss'),
    "MTIME" = TO_DATE('13-jan-2003 14:01:03', 'dd-mon-yyyy hh24:mi:ss'),
    "STIME" = TO_DATE('13-jan-2003 14:01:03', 'dd-mon-yyyy hh24:mi:ss'),
    "STATUS" = 1,
    "REMOTEOWNER" IS NULL,
    "LINKNAME" IS NULL,
    "FLAGS" = 0,
    "OID$" IS NULL,
    "SPARE1" = 6,
    "SPARE2" = 1,
    "SPARE3" IS NULL,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "SPARE6" IS NULL;

insert into "SYS"."TAB$"
 values
    "OBJ#" = 33415,
    "DATAOBJ#" = 33415,
    "TS#" = 0,
    "FILE#" = 1,
    "BLOCK#" = 121034,
    "BOBJ#" IS NULL,
    "TAB#" IS NULL,
    "COLS" = 5,
    "CLUCOLS" IS NULL,
    "PCTFREE$" = 10,
    "PCTUSED$" = 40,
    "INITRANS" = 1,
    "MAXTRANS" = 255,
    "FLAGS" = 1,
    "AUDIT$" = '--------------------------------------',
    "ROWCNT" IS NULL,
    "BLKCNT" IS NULL,
    "EMPCNT" IS NULL,
    "AVGSPC" IS NULL,
    "CHNCNT" IS NULL,
    "AVGRLN" IS NULL,
    "AVGSPC_FLB" IS NULL,
    "FLBCNT" IS NULL,
    "ANALYZETIME" IS NULL,
    "SAMPLESIZE" IS NULL,
    "DEGREE" IS NULL,
    "INSTANCES" IS NULL,
    "INTCOLS" = 5,
    "KERNELCOLS" = 5,
    "PROPERTY" = 536870912,
    "TRIGFLAG" = 0,
    "SPARE1" = 178,
    "SPARE2" IS NULL,
    "SPARE3" IS NULL,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "SPARE6" = TO_DATE('13-jan-2003 14:01:05', 'dd-mon-yyyy hh24:mi:ss'),

insert into "SYS"."COL$"
 values
    "OBJ#" = 33415,
    "COL#" = 1,
    "SEGCOL#" = 1,
    "SEGCOLLENGTH" = 22,
    "OFFSET" = 0,
    "NAME" = 'PRODUCT_ID',
    "TYPE#" = 2,
    "LENGTH" = 22,
    "FIXEDSTORAGE" = 0,
    "PRECISION#" IS NULL,
    "SCALE" IS NULL,
    "NULL$" = 1,
    "DEFLENGTH" IS NULL,
    "SPARE6" IS NULL,
    "INTCOL#" = 1,
    "PROPERTY" = 0,
    "CHARSETID" = 0,
    "CHARSETFORM" = 0,
    "SPARE1" = 0,
    "SPARE2" = 0,
    "SPARE3" = 0,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "DEFAULT$" IS NULL;

insert into "SYS"."COL$"
 values
    "OBJ#" = 33415,
    "COL#" = 2,
    "SEGCOL#" = 2,
    "SEGCOLLENGTH" = 7,
    "OFFSET" = 0,
    "NAME" = 'MODIFIED_TIME',
    "TYPE#" = 12,
    "LENGTH" = 7,
    "FIXEDSTORAGE" = 0,
    "PRECISION#" IS NULL,
    "SCALE" IS NULL,
    "NULL$" = 0,
    "DEFLENGTH" IS NULL,
    "SPARE6" IS NULL,
    "INTCOL#" = 2,
    "PROPERTY" = 0,
    "CHARSETID" = 0,
    "CHARSETFORM" = 0,
    "SPARE1" = 0,
    "SPARE2" = 0,
    "SPARE3" = 0,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "DEFAULT$" IS NULL;

insert into "SYS"."COL$"
 values
    "OBJ#" = 33415,
    "COL#" = 3,
    "SEGCOL#" = 3,
    "SEGCOLLENGTH" = 22,
    "OFFSET" = 0,
    "NAME" = 'OLD_LIST_PRICE',
    "TYPE#" = 2,
    "LENGTH" = 22,
    "FIXEDSTORAGE" = 0,
    "PRECISION#" = 8,
    "SCALE" = 2,
    "NULL$" = 0,
    "DEFLENGTH" IS NULL,
    "SPARE6" IS NULL,
    "INTCOL#" = 3,
    "PROPERTY" = 0,
    "CHARSETID" = 0,
    "CHARSETFORM" = 0,
    "SPARE1" = 0,
    "SPARE2" = 0,
    "SPARE3" = 0,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "DEFAULT$" IS NULL;

insert into "SYS"."COL$"
 values
    "OBJ#" = 33415,
    "COL#" = 4,
    "SEGCOL#" = 4,
    "SEGCOLLENGTH" = 5,
    "OFFSET" = 0,
    "NAME" = 'OLD_WARRANTY_PERIOD',
    "TYPE#" = 182,
    "LENGTH" = 5,
    "FIXEDSTORAGE" = 0,
    "PRECISION#" = 2,
    "SCALE" = 0,
    "NULL$" = 0,
    "DEFLENGTH" IS NULL,
    "SPARE6" IS NULL,
    "INTCOL#" = 4,
    "PROPERTY" = 0,
    "CHARSETID" = 0,
    "CHARSETFORM" = 0,
    "SPARE1" = 0,
    "SPARE2" = 2,
    "SPARE3" = 0,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "DEFAULT$" IS NULL;

insert into "SYS"."CCOL$"
 values
    "OBJ#" = 33415,
    "CON#" = 2090,
    "COL#" = 1,
    "POS#" IS NULL,
    "INTCOL#" = 1,
    "SPARE1" = 0,
    "SPARE2" IS NULL,
    "SPARE3" IS NULL,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "SPARE6" IS NULL;

insert into "SYS"."CDEF$"
 values
    "OBJ#" = 33415,
    "CON#" = 2090,
    "COLS" = 1,
    "TYPE#" = 7,
    "ROBJ#" IS NULL,
    "RCON#" IS NULL,
    "RRULES" IS NULL,
    "MATCH#" IS NULL,
    "REFACT" IS NULL,
    "ENABLED" = 1,
    "CONDLENGTH" = 24,
    "SPARE6" IS NULL,
    "INTCOLS" = 1,
    "MTIME" = TO_DATE('13-jan-2003 14:01:08', 'dd-mon-yyyy hh24:mi:ss'),
    "DEFER" = 12,
    "SPARE1" = 6,
    "SPARE2" IS NULL,
    "SPARE3" IS NULL,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "CONDITION" = '"PRODUCT_ID" IS NOT NULL';

create table oe.product_tracking (product_id number not null,
  modified_time date,
  old_product_description varchar2(2000),
  old_list_price number(8,2),
  old_warranty_period interval year(2) to month);

update "SYS"."SEG$"
  set
    "TYPE#" = 5,
    "BLOCKS" = 5,
    "EXTENTS" = 1,
    "INIEXTS" = 5,
    "MINEXTS" = 1,
    "MAXEXTS" = 121,
    "EXTSIZE" = 5,
    "EXTPCT" = 50,
    "USER#" = 37,
    "LISTS" = 0,
    "GROUPS" = 0,
    "CACHEHINT" = 0,
    "HWMINCR" = 33415,
    "SPARE1" = 1024
  where
    "TS#" = 0 and
    "FILE#" = 1 and
    "BLOCK#" = 121034 and
    "TYPE#" = 3 and
    "BLOCKS" = 5 and
    "EXTENTS" = 1 and
    "INIEXTS" = 5 and
    "MINEXTS" = 1 and
    "MAXEXTS" = 121 and
    "EXTSIZE" = 5 and
    "EXTPCT" = 50 and
    "USER#" = 37 and
    "LISTS" = 0 and
    "GROUPS" = 0 and
    "BITMAPRANGES" = 0 and
    "CACHEHINT" = 0 and
    "SCANHINT" = 0 and
    "HWMINCR" = 33415 and
    "SPARE1" = 1024 and
    "SPARE2" IS NULL and
    ROWID = 'AAAAAIAABAAAdMOAAB';

insert into "SYS"."CON$"
 values
    "OWNER#" = 37,
    "NAME" = 'SYS_C002090',
    "CON#" = 2090,
    "SPARE1" IS NULL,
    "SPARE2" IS NULL,
    "SPARE3" IS NULL,
    "SPARE4" IS NULL,
    "SPARE5" IS NULL,
    "SPARE6" IS NULL;

commit;
</pre></dd>
<dd><a id="SUTIL3920"></a><a id="sthref1705"></a></dd>
<dt class="seghead">Step 6&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028835"></a>
<div id="SUTIL1610" class="sect3">
<h4 class="sect3">Example 5: Tracking DDL Statements in the Internal Dictionary</h4>
<p>By using the <code dir="ltr">DBMS_LOGMNR.DDL_DICT_TRACKING</code> option, this example ensures that the LogMiner internal dictionary is updated with the DDL statements encountered in the redo log files.</p>
<dl>
<dd><a id="SUTIL3921"></a><a id="sthref1706"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine which redo log file was most recently archived by the database.</dt>
<dd>
<p>This example assumes that you know you want to mine the redo log file that was most recently archived.</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE# FROM V$ARCHIVED_LOG 
   WHERE FIRST_TIME = (SELECT MAX(FIRST_TIME) FROM V$ARCHIVED_LOG);

NAME                                           SEQUENCE#
--------------------------------------------   --------------
/usr/oracle/data/db1arch_1_210_482701534.dbf   210
</pre></dd>
<dd><a id="SUTIL3922"></a><a id="sthref1707"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Find the dictionary in the redo log files.</dt>
<dd>
<p>Because the dictionary may be contained in more than one redo log file, you need to determine which redo log files contain the start and end of the data dictionary. Query the <code dir="ltr">V$ARCHIVED_LOG</code> view, as follows:</p>
<ol>
<li>
<p>Find a redo log that contains the end of the data dictionary extract. This redo log file must have been created before the redo log files that you want to analyze, but should be as recent as possible.</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE#, DICTIONARY_BEGIN d_beg, DICTIONARY_END d_end
   FROM V$ARCHIVED_LOG
   WHERE SEQUENCE# = (SELECT MAX (SEQUENCE#) FROM V$ARCHIVED_LOG
   WHERE DICTIONARY_END = 'YES' and SEQUENCE# &lt; 210);


NAME                                           SEQUENCE#    D_BEG  D_END
--------------------------------------------   ----------   -----  ------
/usr/oracle/data/db1arch_1_208_482701534.dbf   208          NO     YES
</pre></li>
<li>
<p>Find the redo log file that contains the start of the data dictionary extract that matches the end of the dictionary found by the previous SQL statement:</p>
<pre dir="ltr">
SELECT NAME, SEQUENCE#, DICTIONARY_BEGIN d_beg, DICTIONARY_END d_end
   FROM V$ARCHIVED_LOG
   WHERE SEQUENCE# = (SELECT MAX (SEQUENCE#) FROM V$ARCHIVED_LOG
   WHERE DICTIONARY_BEGIN = 'YES' and SEQUENCE# &lt;= 208);

NAME                                           SEQUENCE#    D_BEG  D_END
--------------------------------------------   ----------   -----  ------
/usr/oracle/data/db1arch_1_208_482701534.dbf   207          YES     NO
</pre></li>
</ol>
</dd>
<dd><a id="SUTIL3923"></a><a id="sthref1708"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Ensure that you have a complete list of redo log files.</dt>
<dd>
<p>To successfully apply DDL statements encountered in the redo log files, ensure that all files are included in the list of redo log files to mine. The missing log file corresponding to sequence# 209 must be included in the list. Determine the names of the redo log files that you need to add to the list by issuing the following query:</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG
   WHERE SEQUENCE# &gt;= 207 AND SEQUENCE# &lt;= 210 
   ORDER BY SEQUENCE# ASC;

NAME                                           
--------------------------------------------   
/usr/oracle/data/db1arch_1_207_482701534.dbf  
/usr/oracle/data/db1arch_1_208_482701534.dbf  
/usr/oracle/data/db1arch_1_209_482701534.dbf  
/usr/oracle/data/db1arch_1_210_482701534.dbf  
</pre></dd>
<dd><a id="SUTIL3924"></a><a id="sthref1709"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Specify the list of the redo log files of interest.</dt>
<dd>
<p>Include the redo log files that contain the beginning and end of the dictionary, the redo log file that you want to mine, and any redo log files required to create a list without gaps. You can add the redo log files in any order.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_210_482701534.dbf', -
</pre>
<pre dir="ltr">
       OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_209_482701534.dbf');
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_208_482701534.dbf');
EXECUTE DBMS_LOGMNR.ADD_LOGFILE(-
   LOGFILENAME =&gt; '/usr/oracle/data/db1arch_1_207_482701534.dbf');
</pre></dd>
<dd><a id="SUTIL3925"></a><a id="sthref1710"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner by specifying the dictionary to use and the <code dir="ltr">DDL_DICT_TRACKING</code>, <code dir="ltr">COMMITTED_DATA_ONLY</code>, and <code dir="ltr">PRINT_PRETTY_SQL</code> options.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_REDO_LOGS + -
              DBMS_LOGMNR.DDL_DICT_TRACKING + -
              DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
              DBMS_LOGMNR.PRINT_PRETTY_SQL);
</pre></dd>
<dd><a id="SUTIL3926"></a><a id="sthref1711"></a></dd>
<dt class="seghead">Step 6&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>To reduce the number of rows returned, exclude from the query all DML statements done in the <code dir="ltr">SYS</code> or <code dir="ltr">SYSTEM</code> schemas. (This query specifies a timestamp to exclude transactions that were involved in the dictionary extraction.)</p>
<p>The query returns all the reconstructed SQL statements correctly translated and the insert operations on the <code dir="ltr">oe.product_tracking</code> table that occurred because of the trigger execution.</p>
<pre dir="ltr">
SELECT USERNAME AS usr,(XIDUSN || '.' || XIDSLT || '.' || XIDSQN) as XID, SQL_REDO FROM  
   V$LOGMNR_CONTENTS 
   WHERE SEG_OWNER IS NULL OR SEG_OWNER NOT IN ('SYS', 'SYSTEM') AND
   TIMESTAMP &gt; '10-jan-2003 15:59:53';

USR             XID         SQL_REDO
-----------     --------    -----------------------------------
SYS             1.2.1594    set transaction read write;
SYS             1.2.1594    create table oe.product_tracking (product_id number not null,
                            modified_time date,
                            old_list_price number(8,2),
                            old_warranty_period interval year(2) to month);
SYS             1.2.1594    commit;

SYS             1.18.1602   set transaction read write;
SYS             1.18.1602   create or replace trigger oe.product_tracking_trigger
                            before update on oe.product_information
                            for each row
                            when (new.list_price &lt;&gt; old.list_price or
                                  new.warranty_period &lt;&gt; old.warranty_period)
                            declare
                            begin
                            insert into oe.product_tracking values 
                               (:old.product_id, sysdate,
                                :old.list_price, :old.warranty_period);
                            end;
SYS             1.18.1602   commit;

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 100
                              where
                                "PRODUCT_ID" = 1729 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 80 and
                                ROWID = 'AAAHTKAABAAAY9yAAA';
OE              1.9.1598    insert into "OE"."PRODUCT_TRACKING"
                              values
                                "PRODUCT_ID" = 1729,
                                "MODIFIED_TIME" = TO_DATE('13-jan-2003 16:07:03', 
                                'dd-mon-yyyy hh24:mi:ss'),
                                "OLD_LIST_PRICE" = 80,
                                "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00');

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 92
                              where
                                "PRODUCT_ID" = 2340 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 72 and
                                ROWID = 'AAAHTKAABAAAY9zAAA';

OE              1.9.1598    insert into "OE"."PRODUCT_TRACKING"
                              values
                                "PRODUCT_ID" = 2340,
                                "MODIFIED_TIME" = TO_DATE('13-jan-2003 16:07:07', 
                                'dd-mon-yyyy hh24:mi:ss'),
                                "OLD_LIST_PRICE" = 72,
                                "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00');

OE              1.9.1598     commit;
</pre></dd>
<dd><a id="SUTIL3927"></a><a id="sthref1712"></a></dd>
<dt class="seghead">Step 7&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028846"></a>
<div id="SUTIL1611" class="sect3">
<h4 class="sect3">Example 6: Filtering Output by Time Range</h4>
<p>In the previous two examples, rows were filtered by specifying a timestamp-based predicate (timestamp &gt; '10-jan-2003 15:59:53') in the query. However, a more efficient way to filter out redo records based on timestamp values is by specifying the time range in the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure call, as shown in this example.</p>
<dl>
<dd><a id="SUTIL3928"></a><a id="sthref1713"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Create a list of redo log files to mine.</dt>
<dd>
<p>Suppose you want to mine redo log files generated since a given time. The following procedure creates a list of redo log files based on a specified time. The subsequent SQL <code dir="ltr">EXECUTE</code> statement calls the procedure and specifies the starting time as 2 p.m. on Jan-13-2003.</p>
<pre dir="ltr">
--
-- my_add_logfiles
-- Add all archived logs generated after a specified start_time.
--
CREATE OR REPLACE PROCEDURE my_add_logfiles (in_start_time  IN DATE) AS
  CURSOR  c_log IS 
    SELECT NAME FROM V$ARCHIVED_LOG 
      WHERE FIRST_TIME &gt;= in_start_time;

count      pls_integer := 0;
my_option  pls_integer := DBMS_LOGMNR.NEW;

BEGIN
  FOR c_log_rec IN c_log
  LOOP
    DBMS_LOGMNR.ADD_LOGFILE(LOGFILENAME =&gt; c_log_rec.name, 
                            OPTIONS =&gt; my_option);
    my_option := DBMS_LOGMNR.ADDFILE;
    DBMS_OUTPUT.PUT_LINE('Added logfile ' || c_log_rec.name);
  END LOOP;
END;
/

EXECUTE my_add_logfiles(in_start_time =&gt; '13-jan-2003 14:00:00');
</pre></dd>
<dd><a id="SUTIL3929"></a><a id="sthref1714"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_LOGS to see the list of redo log files.</dt>
<dd>
<p>This example includes the size of the redo log files in the output.</p>
<pre dir="ltr">
SELECT FILENAME name, LOW_TIME start_time, FILESIZE bytes 
    FROM V$LOGMNR_LOGS;

NAME                                START_TIME            BYTES
----------------------------------- --------------------  ----------------
/usr/orcl/arch1_310_482932022.dbf    13-jan-2003 14:02:35  23683584
/usr/orcl/arch1_311_482932022.dbf    13-jan-2003 14:56:35  2564096
/usr/orcl/arch1_312_482932022.dbf    13-jan-2003 15:10:43  23683584
/usr/orcl/arch1_313_482932022.dbf    13-jan-2003 15:17:52  23683584
/usr/orcl/arch1_314_482932022.dbf    13-jan-2003 15:23:10  23683584
/usr/orcl/arch1_315_482932022.dbf    13-jan-2003 15:43:22  23683584
/usr/orcl/arch1_316_482932022.dbf    13-jan-2003 16:03:10  23683584
/usr/orcl/arch1_317_482932022.dbf    13-jan-2003 16:33:43  23683584
/usr/orcl/arch1_318_482932022.dbf    13-jan-2003 17:23:10  23683584
</pre></dd>
<dd><a id="SUTIL3930"></a><a id="sthref1715"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Adjust the list of redo log files.</dt>
<dd>
<p>Suppose you realize that you want to mine just the redo log files generated between 3 p.m. and 4 p.m.</p>
<p>You could use the query predicate (<code dir="ltr">timestamp &gt; '13-jan-2003 15:00:00' and timestamp &lt; '13-jan-2003 16:00:00'</code>) to accomplish this. However, the query predicate is evaluated on each row returned by LogMiner, and the internal mining engine does not filter rows based on the query predicate. Thus, although you only wanted to get rows out of redo log files <code dir="ltr">arch1_311_482932022.dbf</code> to <code dir="ltr">arch1_315_482932022.dbf,</code> your query would result in mining all redo log files registered to the LogMiner session.</p>
<p>Furthermore, although you could use the query predicate and manually remove the redo log files that do not fall inside the time range of interest, the simplest solution is to specify the time range of interest in the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure call.</p>
<p>Although this does not change the list of redo log files, LogMiner will mine only those redo log files that fall in the time range specified.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   STARTTIME =&gt; '13-jan-2003 15:00:00', -
   ENDTIME   =&gt; '13-jan-2003 16:00:00', -
   OPTIONS   =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
                DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
                DBMS_LOGMNR.PRINT_PRETTY_SQL);
</pre></dd>
<dd><a id="SUTIL3931"></a><a id="sthref1716"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<pre dir="ltr">
SELECT TIMESTAMP, (XIDUSN || '.' || XIDSLT || '.' || XIDSQN) AS XID,
</pre>
<pre dir="ltr">
 SQL_REDO FROM V$LOGMNR_CONTENTS WHERE SEG_OWNER = 'OE';

TIMESTAMP              XID          SQL_REDO
---------------------  -----------  --------------------------------
13-jan-2003 15:29:31   1.17.2376    update "OE"."PRODUCT_INFORMATION"
                                      set
                                        "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00')
                                      where
                                        "PRODUCT_ID" = 3399 and
                                        "WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00') and
                                        ROWID = 'AAAHTKAABAAAY9TAAE';
13-jan-2003 15:29:34   1.17.2376      insert into "OE"."PRODUCT_TRACKING"
                                        values
                                        "PRODUCT_ID" = 3399,
                                        "MODIFIED_TIME" = TO_DATE('13-jan-2003 15:29:34', 
                                        'dd-mon-yyyy hh24:mi:ss'),
                                        "OLD_LIST_PRICE" = 815,
                                        "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00');

13-jan-2003 15:52:43   1.15.1756      update "OE"."PRODUCT_INFORMATION"
                                        set
                                          "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00')
                                        where
                                          "PRODUCT_ID" = 1768 and
                                          "WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00') and
                                          ROWID = 'AAAHTKAABAAAY9UAAB';

13-jan-2003 15:52:43   1.15.1756      insert into "OE"."PRODUCT_TRACKING"
                                        values
                                        "PRODUCT_ID" = 1768,
                                        "MODIFIED_TIME" = TO_DATE('13-jan-2003 16:52:43', 
                                        'dd-mon-yyyy hh24:mi:ss'),
                                        "OLD_LIST_PRICE" = 715,
                                        "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00');
</pre></dd>
<dd><a id="SUTIL3932"></a><a id="sthref1717"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="i1027809"></a>
<div id="SUTIL1612" class="sect2">
<h3 class="sect2">Examples of Mining Without Specifying the List of Redo Log Files Explicitly</h3>
<p>The previous set of examples explicitly specified the redo log file or files to be mined. However, if you are mining in the same database that generated the redo log files, then you can mine the appropriate list of redo log files by just specifying the time (or SCN) range of interest. To mine a set of redo log files without explicitly specifying them, use the <code dir="ltr">DBMS_LOGMNR.CONTINUOUS_MINE</code> option to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure, and specify either a time range or an SCN range of interest.</p>
<p>This section contains the following list of examples; these examples are best read in sequential order, because each example builds on the example or examples that precede it:</p>
<ul>
<li>
<p><a href="#i1028804">Example 1: Mining Redo Log Files in a Given Time Range</a></p>
</li>
<li>
<p><a href="#i1028800">Example 2: Mining the Redo Log Files in a Given SCN Range</a></p>
</li>
<li>
<p><a href="#i1028809">Example 3: Using Continuous Mining to Include Future Values in a Query</a></p>
</li>
</ul>
<p>The SQL output formatting may be different on your display than that shown in these examples.</p>
<a id="i1028804"></a>
<div id="SUTIL1613" class="sect3">
<h4 class="sect3">Example 1: Mining Redo Log Files in a Given Time Range</h4>
<p>This example is similar to <a href="#i1028830">"Example 4: Using the LogMiner Dictionary in the Redo Log Files"</a>, except the list of redo log files are not specified explicitly. This example assumes that you want to use the data dictionary extracted to the redo log files.</p>
<dl>
<dd><a id="SUTIL3933"></a><a id="sthref1718"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine the timestamp of the redo log file that contains the start of the data dictionary.</dt>
<dd>
<pre dir="ltr">
SELECT NAME, FIRST_TIME FROM V$ARCHIVED_LOG
</pre>
<pre dir="ltr">
    WHERE SEQUENCE# = (SELECT MAX(SEQUENCE#) FROM V$ARCHIVED_LOG 
    WHERE DICTIONARY_BEGIN = 'YES');

NAME                                          FIRST_TIME
--------------------------------------------  --------------------
/usr/oracle/data/db1arch_1_207_482701534.dbf  10-jan-2003 12:01:34
</pre></dd>
<dd><a id="SUTIL3934"></a><a id="sthref1719"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Display all the redo log files that have been generated so far.</dt>
<dd>
<p>This step is not required, but is included to demonstrate that the <code dir="ltr">CONTINUOUS_MINE</code> option works as expected, as will be shown in Step 4.</p>
<pre dir="ltr">
SELECT FILENAME name FROM V$LOGMNR_LOGS
   WHERE LOW_TIME &gt; '10-jan-2003 12:01:34';

NAME
----------------------------------------------
/usr/oracle/data/db1arch_1_207_482701534.dbf
/usr/oracle/data/db1arch_1_208_482701534.dbf
/usr/oracle/data/db1arch_1_209_482701534.dbf
/usr/oracle/data/db1arch_1_210_482701534.dbf
</pre></dd>
<dd><a id="SUTIL3935"></a><a id="sthref1720"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<p>Start LogMiner by specifying the dictionary to use and the <code dir="ltr">COMMITTED_DATA_ONLY</code>, <code dir="ltr">PRINT_PRETTY_SQL,</code> <code dir="ltr">and CONTINUOUS_MINE</code> options.</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
   STARTTIME =&gt; '10-jan-2003 12:01:34', -
     ENDTIME =&gt; SYSDATE, -
     OPTIONS =&gt; DBMS_LOGMNR.DICT_FROM_REDO_LOGS + -
                DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
                DBMS_LOGMNR.PRINT_PRETTY_SQL + -
                    DBMS_LOGMNR.CONTINUOUS_MINE);
</pre></dd>
<dd><a id="SUTIL3936"></a><a id="sthref1721"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_LOGS view.</dt>
<dd>
<p>This step shows that the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure with the <code dir="ltr">CONTINUOUS_MINE</code> option includes all of the redo log files that have been generated so far, as expected. (Compare the output in this step to the output in Step 2.)</p>
<pre dir="ltr">
SELECT FILENAME name FROM V$LOGMNR_LOGS;

NAME
------------------------------------------------------
/usr/oracle/data/db1arch_1_207_482701534.dbf
/usr/oracle/data/db1arch_1_208_482701534.dbf
/usr/oracle/data/db1arch_1_209_482701534.dbf
/usr/oracle/data/db1arch_1_210_482701534.dbf
</pre></dd>
<dd><a id="SUTIL3937"></a><a id="sthref1722"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>To reduce the number of rows returned by the query, exclude all DML statements done in the <code dir="ltr">SYS</code> or <code dir="ltr">SYSTEM</code> schema. (This query specifies a timestamp to exclude transactions that were involved in the dictionary extraction.)</p>
<p>Note that all reconstructed SQL statements returned by the query are correctly translated.</p>
<pre dir="ltr">
SELECT USERNAME AS usr,(XIDUSN || '.' || XIDSLT || '.' || XIDSQN) as XID, 
   SQL_REDO FROM V$LOGMNR_CONTENTS 
   WHERE SEG_OWNER IS NULL OR SEG_OWNER NOT IN ('SYS', 'SYSTEM') AND
   TIMESTAMP &gt; '10-jan-2003 15:59:53';

USR             XID         SQL_REDO
-----------     --------    -----------------------------------
SYS             1.2.1594    set transaction read write;
SYS             1.2.1594    create table oe.product_tracking (product_id number not null,
                            modified_time date,
                            old_list_price number(8,2),
                            old_warranty_period interval year(2) to month);
SYS             1.2.1594    commit;

SYS             1.18.1602   set transaction read write;
SYS             1.18.1602   create or replace trigger oe.product_tracking_trigger
                            before update on oe.product_information
                            for each row
                            when (new.list_price &lt;&gt; old.list_price or
                                  new.warranty_period &lt;&gt; old.warranty_period)
                            declare
                            begin
                            insert into oe.product_tracking values 
                               (:old.product_id, sysdate,
                                :old.list_price, :old.warranty_period);
                            end;
SYS             1.18.1602   commit;

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 100
                              where
                                "PRODUCT_ID" = 1729 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 80 and
                                ROWID = 'AAAHTKAABAAAY9yAAA';
OE              1.9.1598    insert into "OE"."PRODUCT_TRACKING"
                              values
                                "PRODUCT_ID" = 1729,
                                "MODIFIED_TIME" = TO_DATE('13-jan-2003 16:07:03', 
                                'dd-mon-yyyy hh24:mi:ss'),
                                "OLD_LIST_PRICE" = 80,
                                "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00');

OE              1.9.1598    update "OE"."PRODUCT_INFORMATION"
                              set
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+08-00'),
                                "LIST_PRICE" = 92
                              where
                                "PRODUCT_ID" = 2340 and
                                "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00') and
                                "LIST_PRICE" = 72 and
                                ROWID = 'AAAHTKAABAAAY9zAAA';

OE              1.9.1598    insert into "OE"."PRODUCT_TRACKING"
                              values
                                "PRODUCT_ID" = 2340,
                                "MODIFIED_TIME" = TO_DATE('13-jan-2003 16:07:07', 
                                'dd-mon-yyyy hh24:mi:ss'),
                                "OLD_LIST_PRICE" = 72,
                                "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00');

OE              1.9.1598     commit;
</pre></dd>
<dd><a id="SUTIL3938"></a><a id="sthref1723"></a></dd>
<dt class="seghead">Step 6&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028800"></a>
<div id="SUTIL1614" class="sect3">
<h4 class="sect3">Example 2: Mining the Redo Log Files in a Given SCN Range</h4>
<p>This example shows how to specify an SCN range of interest and mine the redo log files that satisfy that range. You can use LogMiner to see all committed DML statements whose effects have not yet been made permanent in the data files.</p>
<p>Note that in this example (unlike the other examples) it is not assumed that you have set the <code dir="ltr">NLS_DATE_FORMAT</code> parameter.</p>
<dl>
<dd><a id="SUTIL3939"></a><a id="sthref1724"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Determine the SCN of the last checkpoint taken.</dt>
<dd>
<pre dir="ltr">
SELECT CHECKPOINT_CHANGE#, CURRENT_SCN FROM V$DATABASE;
</pre>
<pre dir="ltr">
CHECKPOINT_CHANGE#  CURRENT_SCN
------------------  ---------------
          56453576         56454208
</pre></dd>
<dd><a id="SUTIL3940"></a><a id="sthref1725"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Start LogMiner and specify the CONTINUOUS_MINE option.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
</pre>
<pre dir="ltr">
   STARTSCN =&gt; 56453576, -
   ENDSCN   =&gt; 56454208, -
   OPTIONS  =&gt; DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + -
               DBMS_LOGMNR.COMMITTED_DATA_ONLY + -
               DBMS_LOGMNR.PRINT_PRETTY_SQL + -
               DBMS_LOGMNR.CONTINUOUS_MINE);
</pre></dd>
<dd><a id="SUTIL3941"></a><a id="sthref1726"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Display the list of archived redo log files added by LogMiner.</dt>
<dd>
<pre dir="ltr">
SELECT FILENAME name, LOW_SCN, NEXT_SCN FROM V$LOGMNR_LOGS;
</pre>
<pre dir="ltr">
NAME                                           LOW_SCN   NEXT_SCN
--------------------------------------------   --------  --------
/usr/oracle/data/db1arch_1_215_482701534.dbf   56316771  56453579
</pre>
<p>Note that the redo log file that LogMiner added does not contain the whole SCN range. When you specify the <code dir="ltr">CONTINUOUS_MINE</code> option, LogMiner adds only archived redo log files when you call the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure. LogMiner will add the rest of the SCN range contained in the online redo log files automatically, as needed during the query execution. Use the following query to determine whether the redo log file added is the latest archived redo log file produced.</p>
<pre dir="ltr">
SELECT NAME FROM V$ARCHIVED_LOG 
   WHERE SEQUENCE# = (SELECT MAX(SEQUENCE#) FROM V$ARCHIVED_LOG);

NAME
-------------------------------------------- 
/usr/oracle/data/db1arch_1_215_482701534.dbf 
</pre></dd>
<dd><a id="SUTIL3942"></a><a id="sthref1727"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view for changes made to the user tables.</dt>
<dd>
<p>The following query does not return the <code dir="ltr">SET TRANSACTION READ WRITE</code> and <code dir="ltr">COMMIT</code> statements associated with transaction 1.6.1911 because these statements do not have a segment owner (<code dir="ltr">SEG_OWNER</code>) associated with them.</p>
<p>Note that the default <code dir="ltr">NLS_DATE_FORMAT</code>, 'DD-MON-RR', is used to display the column <code dir="ltr">MODIFIED_TIME</code> of type <code dir="ltr">DATE</code>.</p>
<pre dir="ltr">
SELECT SCN, (XIDUSN || '.' || XIDSLT || '.' ||  XIDSQN) as XID, SQL_REDO 
    FROM V$LOGMNR_CONTENTS
    WHERE SEG_OWNER NOT IN ('SYS', 'SYSTEM');


SCN        XID        SQL_REDO
---------- ---------- -------------
56454198   1.6.1911   update "OE"."PRODUCT_INFORMATION"
                        set
                          "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00')
                        where
                          "PRODUCT_ID" = 2430 and
                          "WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00') and
                          ROWID = 'AAAHTKAABAAAY9AAAC';

56454199   1.6.1911   insert into "OE"."PRODUCT_TRACKING"
                        values
                          "PRODUCT_ID" = 2430,
                          "MODIFIED_TIME" = TO_DATE('17-JAN-03', 'DD-MON-RR'),
                          "OLD_LIST_PRICE" = 175,
                          "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00');

56454204   1.6.1911    update "OE"."PRODUCT_INFORMATION"
                         set
                           "WARRANTY_PERIOD" = TO_YMINTERVAL('+05-00')
                         where
                           "PRODUCT_ID" = 2302 and
                           "WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00') and
                           ROWID = 'AAAHTKAABAAAY9QAAA';
56454206   1.6.1911    insert into "OE"."PRODUCT_TRACKING"
                         values
                           "PRODUCT_ID" = 2302,
                           "MODIFIED_TIME" = TO_DATE('17-JAN-03', 'DD-MON-RR'),
                           "OLD_LIST_PRICE" = 150,
                           "OLD_WARRANTY_PERIOD" = TO_YMINTERVAL('+02-00');
</pre></dd>
<dd><a id="SUTIL3943"></a><a id="sthref1728"></a></dd>
<dt class="seghead">Step 5&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre>
<pre dir="ltr">
</pre></dd>
</dl>
</div>
<!-- class="sect3" -->
<a id="i1028809"></a>
<div id="SUTIL1615" class="sect3">
<h4 class="sect3">Example 3: Using Continuous Mining to Include Future Values in a Query</h4>
<p>To specify that a query not finish until some future time occurs or SCN is reached, use the <code dir="ltr">CONTINUOUS_MINE</code> option and set either the <code dir="ltr">ENDTIME</code> or <code dir="ltr">ENDSCN</code> option in your call to the <code dir="ltr">DBMS_LOGMNR.START_LOGMNR</code> procedure to a time in the future or to an SCN value that has not yet been reached.</p>
<p>This examples assumes that you want to monitor all changes made to the table <code dir="ltr">hr.employees</code> from now until 5 hours from now, and that you are using the dictionary in the online catalog.</p>
<dl>
<dd><a id="SUTIL3944"></a><a id="sthref1729"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Start LogMiner.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR(-
</pre>
<pre dir="ltr">
   STARTTIME =&gt; SYSDATE, -
   ENDTIME   =&gt; SYSDATE + 5/24, -
   OPTIONS   =&gt; DBMS_LOGMNR.CONTINUOUS_MINE  + -
                DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG);
</pre></dd>
<dd><a id="SUTIL3945"></a><a id="sthref1730"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Query the V$LOGMNR_CONTENTS view.</dt>
<dd>
<p>This select operation will not complete until it encounters the first redo log file record that is generated after the time range of interest (5 hours from now). You can end the select operation prematurely by entering Ctrl+C.</p>
<p>This example specifies the <code dir="ltr">SET</code> <code dir="ltr">ARRAYSIZE</code> statement so that rows are displayed as they are entered in the redo log file. If you do not specify the <code dir="ltr">SET ARRAYSIZE</code> statement, then rows are not returned until the SQL internal buffer is full.</p>
<pre dir="ltr">
SET ARRAYSIZE 1;
SELECT USERNAME AS usr, SQL_REDO FROM V$LOGMNR_CONTENTS
   WHERE  SEG_OWNER = 'HR' AND TABLE_NAME = 'EMPLOYEES';
</pre></dd>
<dd><a id="SUTIL3946"></a><a id="sthref1731"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;End the LogMiner session.</dt>
<dd>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.END_LOGMNR();
</pre>
<pre dir="ltr">
</pre></dd>
</dl>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="i1028785"></a>
<div id="SUTIL1616" class="sect2">
<h3 class="sect2">Example Scenarios</h3>
<p>The examples in this section demonstrate how to use LogMiner for typical scenarios. This section includes the following examples:</p>
<ul>
<li>
<p><a href="#i1040290">Scenario 1: Using LogMiner to Track Changes Made by a Specific User</a></p>
</li>
<li>
<p><a href="#i1028674">Scenario 2: Using LogMiner to Calculate Table Access Statistics</a></p>
</li>
</ul>
<a id="i1040290"></a>
<div id="SUTIL1617" class="sect3">
<h4 class="sect3">Scenario 1: Using LogMiner to Track Changes Made by a Specific User</h4>
<p>This example shows how to see all changes made to the database in a specific time range by a single user: <code dir="ltr">joedevo</code>. Connect to the database and then take the following steps:</p>
<ol>
<li>
<p>Create the LogMiner dictionary file.</p>
<p>To use LogMiner to analyze <code dir="ltr">joedevo</code>'s data, you must either create a LogMiner dictionary file before any table definition changes are made to tables that <code dir="ltr">joedevo</code> uses or use the online catalog at LogMiner startup. See <a href="#i1006321">"Extract a LogMiner Dictionary"</a> for examples of creating LogMiner dictionaries. This example uses a LogMiner dictionary that has been extracted to the redo log files.</p>
</li>
<li>
<p>Add redo log files.</p>
<p>Assume that <code dir="ltr">joedevo</code> has made some changes to the database. You can now specify the names of the redo log files that you want to analyze, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; 'log1orc1.ora', -
   OPTIONS =&gt; DBMS_LOGMNR.NEW);
</pre>
<p>If desired, add additional redo log files, as follows:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.ADD_LOGFILE( -
   LOGFILENAME =&gt; 'log2orc1.ora', -
   OPTIONS =&gt; DBMS_LOGMNR.ADDFILE);
</pre></li>
<li>
<p>Start LogMiner and limit the search to the specified time range:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   DICTFILENAME =&gt; 'orcldict.ora', -
   STARTTIME =&gt; TO_DATE('01-Jan-1998 08:30:00','DD-MON-YYYY HH:MI:SS'), -
   ENDTIME =&gt; TO_DATE('01-Jan-1998 08:45:00', 'DD-MON-YYYY HH:MI:SS'));
</pre></li>
<li>
<p>Query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view.</p>
<p>At this point, the <code dir="ltr">V$LOGMNR_CONTENTS</code> view is available for queries. You decide to find all of the changes made by user <code dir="ltr">joedevo</code> to the <code dir="ltr">salary</code> table. Execute the following <code dir="ltr">SELECT</code> statement:</p>
<pre dir="ltr">
SELECT SQL_REDO, SQL_UNDO FROM V$LOGMNR_CONTENTS 
   WHERE USERNAME = 'joedevo' AND SEG_NAME = 'salary';
</pre>
<p>For both the <code dir="ltr">SQL_REDO</code> and <code dir="ltr">SQL_UNDO</code> columns, two rows are returned (the format of the data display will be different on your screen). You discover that <code dir="ltr">joedevo</code> requested two operations: he deleted his old salary and then inserted a new, higher salary. You now have the data necessary to undo this operation.</p>
<pre dir="ltr">
SQL_REDO                              SQL_UNDO
--------                              --------
delete from SALARY                    insert into SALARY(NAME, EMPNO, SAL)
where EMPNO = 12345                    values ('JOEDEVO', 12345, 500)
and NAME='JOEDEVO'
and SAL=500;

insert into SALARY(NAME, EMPNO, SAL)  delete from SALARY
values('JOEDEVO',12345, 2500)         where EMPNO = 12345
                                      and NAME = 'JOEDEVO'
2 rows selected                       and SAL = 2500;
</pre></li>
<li>
<p>End the LogMiner session.</p>
<p>Use the <code dir="ltr">DBMS_LOGMNR.END_LOGMNR</code> procedure to finish the LogMiner session properly:</p>
<pre dir="ltr">
DBMS_LOGMNR.END_LOGMNR( );
</pre></li>
</ol>
</div>
<!-- class="sect3" -->
<a id="i1028674"></a>
<div id="SUTIL1618" class="sect3">
<h4 class="sect3">Scenario 2: Using LogMiner to Calculate Table Access Statistics</h4>
<p>In this example, assume you manage a direct marketing database and want to determine how productive the customer contacts have been in generating revenue for a 2-week period in January. Assume that you have already created the LogMiner dictionary and added the redo log files that you want to search (as demonstrated in the previous example). Take the following steps:</p>
<ol>
<li>
<p>Start LogMiner and specify a range of times:</p>
<pre dir="ltr">
EXECUTE DBMS_LOGMNR.START_LOGMNR( -
   STARTTIME =&gt; TO_DATE('07-Jan-2003 08:30:00','DD-MON-YYYY HH:MI:SS'), -
   ENDTIME =&gt; TO_DATE('21-Jan-2003 08:45:00','DD-MON-YYYY HH:MI:SS'), -
   DICTFILENAME =&gt; '/usr/local/dict.ora');
</pre></li>
<li>
<p>Query the <code dir="ltr">V$LOGMNR_CONTENTS</code> view to determine which tables were modified in the time range you specified, as shown in the following example. (This query filters out system tables that traditionally have a <code dir="ltr">$</code> in their name.)</p>
<pre dir="ltr">
SELECT SEG_OWNER, SEG_NAME, COUNT(*) AS Hits FROM
   V$LOGMNR_CONTENTS WHERE SEG_NAME NOT LIKE '%$' GROUP BY
   SEG_OWNER, SEG_NAME ORDER BY Hits DESC;
</pre></li>
<li>
<p>The following data is displayed. (The format of your display may be different.)</p>
<pre dir="ltr">
SEG_OWNER          SEG_NAME          Hits
---------          --------          ----
CUST               ACCOUNT            384
UNIV               EXECDONOR          325
UNIV               DONOR              234
UNIV               MEGADONOR           32
HR                 EMPLOYEES           12
SYS                DONOR               12
</pre>
<p><a id="i1028698"></a>The values in the <code dir="ltr">Hits</code> column show the number of times that the named table had an insert, delete, or update operation performed on it during the 2-week period specified in the query. In this example, the <code dir="ltr">cust.account</code> table was modified the most during the specified 2-week period, and the <code dir="ltr">hr.employees</code> and <code dir="ltr">sys.donor</code> tables were modified the least during the same time period.</p>
</li>
<li>
<p>End the LogMiner session.</p>
<p>Use the <code dir="ltr">DBMS_LOGMNR.END_LOGMNR</code> procedure to finish the LogMiner session properly:</p>
<pre dir="ltr">
DBMS_LOGMNR.END_LOGMNR( );
</pre></li>
</ol>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1031090"></a>
<div id="SUTIL1619" class="sect1">
<h2 class="sect1">Supported Datatypes, Storage Attributes, and Database and Redo Log File Versions</h2>
<p>The following sections provide information about datatype and storage attribute support and the releases of the database and redo log files supported:</p>
<ul>
<li>
<p><a href="#i1031015">Supported Datatypes and Table Storage Attributes</a></p>
</li>
<li>
<p><a href="#i1031042">Unsupported Datatypes and Table Storage Attributes</a></p>
</li>
<li>
<p><a href="#i1031050">Supported Databases and Redo Log File Versions</a></p>
</li>
</ul>
<a id="i1031015"></a>
<div id="SUTIL102" class="sect2">
<h3 class="sect2">Supported Datatypes and Table Storage Attributes</h3>
<p><a id="sthref1732"></a><a id="sthref1733"></a>LogMiner supports the following datatypes and table storage attributes. As described in information following this list, some datatypes are supported only in certain releases.</p>
<ul>
<li>
<p><code dir="ltr">CHAR</code></p>
</li>
<li>
<p><code dir="ltr">NCHAR</code></p>
</li>
<li>
<p><code dir="ltr">VARCHAR2</code> and <code dir="ltr">VARCHAR</code></p>
</li>
<li>
<p><code dir="ltr">NVARCHAR2</code></p>
</li>
<li>
<p><code dir="ltr">NUMBER</code></p>
</li>
<li>
<p><code dir="ltr">DATE</code></p>
</li>
<li>
<p><code dir="ltr">TIMESTAMP</code></p>
</li>
<li>
<p><code dir="ltr">TIMESTAMP WITH TIME ZONE</code></p>
</li>
<li>
<p><code dir="ltr">TIMESTAMP WITH LOCAL TIME ZONE</code></p>
</li>
<li>
<p><code dir="ltr">INTERVAL YEAR TO MONTH</code></p>
</li>
<li>
<p><code dir="ltr">INTERVAL DAY TO SECOND</code></p>
</li>
<li>
<p><code dir="ltr">RAW</code></p>
</li>
<li>
<p><code dir="ltr">CLOB</code></p>
</li>
<li>
<p><code dir="ltr">NCLOB</code></p>
</li>
<li>
<p><code dir="ltr">BLOB</code></p>
</li>
<li>
<p><code dir="ltr">LONG</code></p>
</li>
<li>
<p><code dir="ltr">LONG RAW</code></p>
</li>
<li>
<p><code dir="ltr">BINARY_FLOAT</code></p>
</li>
<li>
<p><code dir="ltr">BINARY_DOUBLE</code></p>
</li>
<li>
<p>Index-organized tables (IOTs), including those with overflows or LOB columns</p>
</li>
<li>
<p>Function-based indexes</p>
</li>
<li>
<p>Tables using basic table compression and OLTP table compression</p>
</li>
<li>
<p><code dir="ltr">XMLType</code> data stored in <code dir="ltr">CLOB</code> format</p>
</li>
<li>
<p><code dir="ltr">XMLType</code> data stored in object-relational format. The contents of the <code dir="ltr">SQL_REDO</code> column for the XML data-related operations is never valid SQL or PL/SQL.</p>
</li>
<li>
<p><code dir="ltr">XMLType</code> data stored as binary XML. The contents of the <code dir="ltr">SQL_REDO</code> column for the XML data-related operations is never valid SQL or PL/SQL.</p>
</li>
<li>
<p>Hybrid Columnar Compression (Support depends on the underlying storage system. See <a class="olink CNCPT89198" href="../../server.112/e40540/tablecls.htm#CNCPT89198"><span class="italic">Oracle Database Concepts</span></a> for more information about Hybrid Columnar Compression. Compatibility must be set to 11.2.)</p>
</li>
</ul>
<p>Support for multibyte <code dir="ltr">CLOB</code>s is available only for redo logs generated by a database with compatibility set to a value of 10.1 or higher.</p>
<p>Support for LOB and LONG datatypes is available only for redo logs generated by a database with compatibility set to a value of 9.2.0.0 or higher.</p>
<p>Support for index-organized tables without overflow segment or with no LOB columns in them is available only for redo logs generated by a database with compatibility set to 10.0.0.0 or higher. Support for index-organized tables with overflow segment or with LOB columns is available only for redo logs generated by a database with compatibility set to 10.2.0.0 or higher.</p>
<p>Support for <code dir="ltr">XMLType</code> data stored as binary XML is available only on Oracle Database 11<span class="italic">g</span> Release 2 (11.2.0.3) or higher with a redo compatibility setting of 11.2.0.3 or higher.</p>
<p>Support for <code dir="ltr">XMLType</code> data stored in object-relational format is available only on Oracle Database 11<span class="italic">g</span> Release 2 (11.2.0.3) or higher with a redo compatibility setting of 11.2.0.3 or higher.</p>
</div>
<!-- class="sect2" -->
<a id="i1031042"></a>
<div id="SUTIL1620" class="sect2">
<h3 class="sect2">Unsupported Datatypes and Table Storage Attributes</h3>
<p><a id="sthref1734"></a><a id="sthref1735"></a>LogMiner does not support the following data types and table storage attributes. If a table contains columns having any of these unsupported data types, then the entire table is ignored by LogMiner.</p>
<ul>
<li>
<p><code dir="ltr">BFILE</code> datatype</p>
</li>
<li>
<p>Simple and nested abstract datatypes (<code dir="ltr">ADT</code>s)</p>
</li>
<li>
<p>Collections (nested tables and <code dir="ltr">VARRAY</code>s)</p>
</li>
<li>
<p>Object refs</p>
</li>
<li>
<p>SecureFiles (unless database compatibility is set to 11.2 or higher)</p>
</li>
</ul>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#CHDFHCGJ">"SecureFiles LOB Considerations"</a></div>
</div>
<!-- class="sect2" -->
<a id="i1031050"></a>
<div id="SUTIL1621" class="sect2">
<h3 class="sect2">Supported Databases and Redo Log File Versions</h3>
<p>LogMiner <a id="sthref1736"></a><a id="sthref1737"></a><a id="sthref1738"></a>runs only on databases of release 8.1 or later, but you can use it to analyze redo log files from release 8.0 databases. However, the information that LogMiner is able to retrieve from a redo log file depends on the version of the log, not the release of the database in use. For example, redo log files for Oracle9<span class="italic">i</span> can be augmented to capture additional information when supplemental logging is enabled. This allows LogMiner functionality to be used to its fullest advantage. Redo log files created with older releases of Oracle will not have that additional data and may therefore have limitations on the operations and datatypes supported by LogMiner.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#i1009063">"Steps in a Typical LogMiner Session"</a> and <a href="#i1021068">"Supplemental Logging"</a></div>
</div>
<!-- class="sect2" -->
<a id="CHDFHCGJ"></a>
<div id="SUTIL3764" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">SecureFiles LOB Considerations</h3>
<p>SecureFiles LOBs are supported when database compatibility is set to 11.2 or higher. Only <code dir="ltr">SQL_REDO</code> columns can be filled in for SecureFiles LOB columns; <code dir="ltr">SQL_UNDO</code> columns are not filled in.</p>
<p>Transparent data encryption and data compression can be enabled on SecureFiles LOB columns at the primary database.</p>
<p>De-duplication of SecureFiles LOB columns, fragment-based operations on SecureFiles LOB columns, and SecureFiles Database File System (DBFS) operations are not supported. Specifically, the following operations contained within the <code dir="ltr">DBMS_LOB</code> PL/SQL package are not supported on SecureFiles LOB columns:</p>
<p><code dir="ltr">FRAGMENT_DELETE</code>, <code dir="ltr">FRAGMENT_INSERT</code>, <code dir="ltr">FRAGMENT_MOVE</code>, <code dir="ltr">FRAGMENT_REPLACE</code>, <code dir="ltr">COPY_FROM_DBFS_LINK</code>, <code dir="ltr">MOVE_TO_DBFS_LINK</code>, <code dir="ltr">SET_DBFS_LINK</code>, <code dir="ltr">COPY_DBFS_LINK</code>, and <code dir="ltr">SETCONTENTTYPE</code>.</p>
<p>If LogMiner encounters redo generated by any of these operations, then it generates rows with the <code dir="ltr">OPERATION</code> column set to <code dir="ltr">UNSUPPORTED</code>. No <code dir="ltr">SQL_REDO</code> or <code dir="ltr">SQL_UNDO</code> will be generated for these redo records.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" --></div>
<!-- class="chapter" --></div>
<!-- class="ind" -->
<!-- Start Footer -->
</div>
<!-- add extra wrapper close div-->
<footer><!--
<hr />
<table class="cellalignment1387">
<tr>
<td class="cellalignment1396">
<table class="cellalignment1392">
<tr>
<td class="cellalignment1391"><a href="dbnewid.htm"><img width="24" height="24" src="../../dcommon/gifs/leftnav.gif" alt="Go to previous page" /><br />
<span class="icon">Previous</span></a></td>
<td class="cellalignment1391"><a href="metadata_api.htm"><img width="24" height="24" src="../../dcommon/gifs/rightnav.gif" alt="Go to next page" /><br />
<span class="icon">Next</span></a></td>
</tr>
</table>
</td>
<td class="cellalignment-copyrightlogo"><img width="144" height="18" src="../../dcommon/gifs/oracle.gif" alt="Oracle" /><br />
Copyright&nbsp;&copy;&nbsp;1996, 2018,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved.<br />
<a href="../../dcommon/html/cpyr.htm">Legal Notices</a></td>
<td class="cellalignment1398">
<table class="cellalignment1390">
<tr>
<td class="cellalignment1391"><a href="../../index.htm"><img width="24" height="24" src="../../dcommon/gifs/doclib.gif" alt="Go to Documentation Home" /><br />
<span class="icon">Home</span></a></td>
<td class="cellalignment1391"><a href="../../nav/portal_booklist.htm"><img width="24" height="24" src="../../dcommon/gifs/booklist.gif" alt="Go to Book List" /><br />
<span class="icon">Book List</span></a></td>
<td class="cellalignment1391"><a href="toc.htm"><img width="24" height="24" src="../../dcommon/gifs/toc.gif" alt="Go to Table of Contents" /><br />
<span class="icon">Contents</span></a></td>
<td class="cellalignment1391"><a href="index.htm"><img width="24" height="24" src="../../dcommon/gifs/index.gif" alt="Go to Index" /><br />
<span class="icon">Index</span></a></td>
<td class="cellalignment1391"><a href="../../nav/mindx.htm"><img width="24" height="24" src="../../dcommon/gifs/masterix.gif" alt="Go to Master Index" /><br />
<span class="icon">Master Index</span></a></td>
<td class="cellalignment1391"><a href="../../dcommon/html/feedback.htm"><img width="24" height="24" src="../../dcommon/gifs/feedbck2.gif" alt="Go to Feedback page" /><br />
<span class="icon">Contact Us</span></a></td>
</tr>
</table>
</td>
</tr>
</table>
--></footer>
<noscript>
<p>Scripting on this page enhances content navigation, but does not change the content in any way.</p>
</noscript>
</body>
</html>
