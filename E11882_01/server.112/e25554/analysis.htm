<!DOCTYPE html>
<html lang="en" >
<head>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta charset="utf-8">
<title>SQL for Analysis and Reporting</title>
<meta name="generator" content="Oracle DARB XHTML Converter (Mode = document) - Version 5.1.2 Build 070" />
<meta name="dcterms.created" content="2013-07-04T23:23:23Z" />
<meta name="robots" content="all" />
<meta name="dcterms.title" content="Database Data Warehousing Guide" />
<meta name="dcterms.identifier" content="E25554-02" />
<meta name="dcterms.isVersionOf" content="DWHSG" />
<meta name="dcterms.rights" content="Copyright&nbsp;&copy;&nbsp;2001, 2013,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved." />
<link rel="Start" href="../../index.htm" title="Home" type="text/html" />
<link rel="Copyright" href="../../dcommon/html/cpyr.htm" title="Copyright" type="text/html" />

<script type="application/javascript"  src="../../dcommon/js/headfoot.js"></script>
<script type="application/javascript"  src="../../nav/js/doccd.js"></script>
<link rel="Contents" href="toc.htm" title="Contents" type="text/html" />
<link rel="Index" href="index.htm" title="Index" type="text/html" />
<link rel="Glossary" href="glossary.htm" title="Glossary" type="text/html" />
<link rel="Prev" href="aggreg.htm" title="Previous" type="text/html" />
<link rel="Next" href="sqlmodel.htm" title="Next" type="text/html" />
<link rel="alternate" href="../e25554.pdf" title="PDF version" type="application/pdf" />
<link rel="schema.dcterms" href="http://purl.org/dc/terms/" />
<link rel="stylesheet" href="../../dcommon/css/fusiondoc.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/header.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/footer.css">
<link rel="stylesheet" type="text/css"  href="../../dcommon/css/fonts.css">
<link rel="stylesheet" href="../../dcommon/css/foundation.css">
<link rel="stylesheet" href="../../dcommon/css/codemirror.css">
<link rel="stylesheet" type="text/css" title="Default" href="../../nav/css/html5.css">
<link rel="stylesheet" href="../../dcommon/css/respond-480-tablet.css">
<link rel="stylesheet" href="../../dcommon/css/respond-768-laptop.css">
<link rel="stylesheet" href="../../dcommon/css/respond-1140-deskop.css">
<script type="application/javascript" src="../../dcommon/js/modernizr.js"></script>
<script type="application/javascript" src="../../dcommon/js/codemirror.js"></script>
<script type="application/javascript" src="../../dcommon/js/jquery.js"></script>
<script type="application/javascript" src="../../dcommon/js/foundation.min.js"></script>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-552992c80ef99c8d" async="async"></script>
<script type="application/javascript" src="../../dcommon/js/jqfns.js"></script>
<script type="application/javascript" src="../../dcommon/js/ohc-inline-videos.js"></script>
<!-- Add fancyBox -->
<link rel="stylesheet" href="../../dcommon/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<!-- Optionally add helpers - button, thumbnail and/or media -->
<link rel="stylesheet"  href="../../dcommon/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5"  type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<link rel="stylesheet"  href="../../dcommon/fancybox/helpers/jquery.fancybox-thumbs.css?v=1.0.7"  type="text/css" media="screen" />
<script type="text/javascript" src="../../dcommon/fancybox/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
</head>
<body>
<a href="#BEGIN" class="accessibility-top skipto" tabindex="0">Go to main content</a><header><!--
<div class="zz-skip-header"><a id="top" href="#BEGIN">Go to main content</a>--></header>
<div class="row" id="CONTENT">
<div class="IND large-9 medium-8 columns" dir="ltr">
<a id="BEGIN" name="BEGIN"></a>
<span id="PAGE" style="display:none;">31/35</span> <!-- End Header --><a id="g1030987"></a><a id="DWHSG021"></a><a id="i1007779"></a>
<h1 class="chapter"><span class="secnum">22</span> SQL for Analysis and Reporting</h1>
<p>The following topics provide information about how to improve analytical SQL queries in a data warehouse:</p>
<ul>
<li>
<p><a href="#i1006229">Overview of SQL for Analysis and Reporting</a></p>
</li>
<li>
<p><a href="#BCFDFACB">Ranking, Windowing, and Reporting Functions</a></p>
</li>
<li>
<p><a href="#BCFIABEE">Advanced Aggregates for Analysis</a></p>
</li>
<li>
<p><a href="#BCFHHHHF">Pivoting Operations</a></p>
</li>
<li>
<p><a href="#i1014934">Data Densification for Reporting</a></p>
</li>
<li>
<p><a href="#CIHIGJII">Time Series Calculations on Densified Data</a></p>
</li>
<li>
<p><a href="#BCFBEEAG">Miscellaneous Analysis and Reporting Capabilities</a></p>
</li>
</ul>
<a id="i1006229"></a><a id="DWHSG8659"></a>
<div class="sect1">
<h2 class="sect1">Overview of SQL for Analysis and Reporting</h2>
<p>Oracle has enhanced SQL's analytical processing capabilities by introducing a new family of analytic SQL functions. These analytic functions enable you to calculate:</p>
<ul>
<li>
<p>Rankings and percentiles</p>
</li>
<li>
<p>Moving window calculations</p>
</li>
<li>
<p>Lag/lead analysis</p>
</li>
<li>
<p>First/last analysis</p>
</li>
<li>
<p>Linear regression statistics</p>
</li>
</ul>
<p>Ranking functions include cumulative distributions, percent rank, and N-tiles. Moving window calculations allow you to find moving and cumulative aggregations, such as sums and averages. Lag/lead analysis enables direct inter-row references so you can calculate period-to-period changes. First/last analysis enables you to find the first or last value in an ordered group.</p>
<p>Other enhancements to SQL include the <code>CASE</code> expression and partitioned outer join. <code>CASE</code> expressions provide if-then logic useful in many situations. Partitioned outer join is an extension to ANSI outer join syntax that allows users to selectively densify certain dimensions while keeping others sparse. This allows reporting tools to selectively densify dimensions, for example, the ones that appear in their cross-tabular reports while keeping others sparse.</p>
<p>To enhance performance, analytic functions can be parallelized: multiple processes can simultaneously execute all of these statements. These capabilities make calculations easier and more efficient, thereby enhancing database performance, scalability, and simplicity.</p>
<p>Analytic functions are classified as described in <a href="#i1014372">Table 22-1</a>.</p>
<div class="tblformalwidemax">
<p class="titleintable"><a id="DWHSG8660"></a><a id="sthref1103"></a><a id="sthref1104"></a>Table 22-1 <a id="i1014372"></a><span class="bolditalic">Analytic Functions and Their Uses&nbsp;</span></p>
<table class="cellalignment1383" title="Analytic Functions and Their Uses&nbsp;" summary="Analytic Functions and Their Uses" dir="ltr">
<thead>
<tr class="cellalignment1372">
<th class="cellalignment1382" id="r1c1-t2">Type</th>
<th class="cellalignment1382" id="r1c2-t2">Used For</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r2c1-t2" headers="r1c1-t2">
<p>Ranking</p>
</td>
<td class="cellalignment1378" headers="r2c1-t2 r1c2-t2">
<p>Calculating ranks, percentiles, and n-tiles of the values in a result set.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r3c1-t2" headers="r1c1-t2">
<p>Windowing</p>
</td>
<td class="cellalignment1378" headers="r3c1-t2 r1c2-t2">
<p>Calculating cumulative and moving aggregates. Works with these functions: <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>VARIANCE</code>, <code>STDDEV</code>, <code>FIRST_VALUE</code>, <code>LAST_VALUE</code>, and new statistical functions. Note that the <code>DISTINCT</code> keyword is not supported in windowing functions except for <code>MAX</code> and <code>MIN</code>.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r4c1-t2" headers="r1c1-t2">
<p>Reporting</p>
</td>
<td class="cellalignment1378" headers="r4c1-t2 r1c2-t2">
<p>Calculating shares, for example, market share. Works with these functions: <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code> (with/without <code>DISTINCT</code>), <code>VARIANCE</code>, <code>STDDEV</code>, <code>RATIO_TO_REPORT</code>, and new statistical functions. Note that the <code>DISTINCT</code> keyword may be used in those reporting functions that support <code>DISTINCT</code> in aggregate mode.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r5c1-t2" headers="r1c1-t2">
<p><code>LAG</code>/<code>LEAD</code></p>
</td>
<td class="cellalignment1378" headers="r5c1-t2 r1c2-t2">
<p>Finding a value in a row a specified number of rows from a current row.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r6c1-t2" headers="r1c1-t2">
<p><code>FIRST</code>/<code>LAST</code></p>
</td>
<td class="cellalignment1378" headers="r6c1-t2 r1c2-t2">
<p>First or last value in an ordered group.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r7c1-t2" headers="r1c1-t2">
<p>Linear Regression</p>
</td>
<td class="cellalignment1378" headers="r7c1-t2 r1c2-t2">
<p>Calculating linear regression and other statistics (slope, intercept, and so on).</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r8c1-t2" headers="r1c1-t2">
<p>Inverse Percentile</p>
</td>
<td class="cellalignment1378" headers="r8c1-t2 r1c2-t2">
<p>The value in a data set that corresponds to a specified percentile.</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r9c1-t2" headers="r1c1-t2">
<p>Hypothetical Rank and Distribution</p>
</td>
<td class="cellalignment1378" headers="r9c1-t2 r1c2-t2">
<p>The rank or percentile that a row would have if inserted into a specified data set.</p>
</td>
</tr>
</tbody>
</table>
<br /></div>
<!-- class="tblformalwidemax" -->
<p>To perform these operations, the analytic functions add several new elements to SQL processing. These elements build on existing SQL to allow flexible and powerful calculation expressions. With just a few exceptions, the analytic functions have these new elements. The processing flow is represented in <a href="#i1014377">Figure 22-1</a>.</p>
<div class="figure">
<p class="titleinfigure"><a id="i1014377"></a><a id="DWHSG8661"></a>Figure 22-1 Processing Order</p>
<img width="464" height="72" src="./dwhsg021.gif" alt="Description of Figure 22-1 follows" /><br />
<a id="sthref1105" href="./img_text/dwhsg021.htm">Description of "Figure 22-1 Processing Order"</a><br />
<br /></div>
<!-- class="figure" -->
<p>The essential <a id="sthref1106"></a>concepts used in analytic functions are:</p>
<ul>
<li>
<p>Processing order</p>
<p>Query processing using analytic functions takes place in three stages. First, all joins, <code>WHERE</code>, <code>GROUP</code> <code>BY</code> and <code>HAVING</code> clauses are performed. Second, the result set is made available to the analytic functions, and all their calculations take place. Third, if the query has an <code>ORDER</code> <code>BY</code> clause at its end, the <code>ORDER</code> <code>BY</code> is processed to allow for precise output ordering. The processing order is shown in <a href="#i1014377">Figure 22-1</a>.</p>
</li>
<li>
<p>Result set partitions</p>
<p>The analytic functions allow users to divide query result sets into groups of rows called partitions. Note that the term <span class="bold">partitions</span> used with analytic functions is unrelated to the table partitions feature. Throughout this chapter, the term partitions refers to only the meaning related to analytic functions. Partitions are created after the groups defined with <code>GROUP</code> <code>BY</code> clauses, so they are available to any aggregate results such as sums and averages. Partition divisions may be based upon any desired columns or expressions. A query result set may be partitioned into just one partition holding all the rows, a few large partitions, or many small partitions holding just a few rows each.</p>
</li>
<li>
<p>Window</p>
<p>For each row in a partition, you can define a sliding window of data. This window determines the range of rows used to perform the calculations for the current row. Window sizes can be based on either a physical number of rows or a logical interval such as time. The window has a starting row and an ending row. Depending on its definition, the window may move at one or both ends. For instance, a window defined for a cumulative sum function would have its starting row fixed at the first row of its partition, and its ending row would slide from the starting point all the way to the last row of the partition. In contrast, a window defined for a moving average would have both its starting and end points slide so that they maintain a constant physical or logical range.</p>
<p>A window can be set as large as all the rows in a partition or just a sliding window of one row within a partition. When a window is near a border, the function returns results for only the available rows, rather than warning you that the results are not what you want.</p>
<p>When using window functions, the current row is included during calculations, so you should only specify (<span class="italic">n</span>-1) when you are dealing with <span class="italic">n</span> items.</p>
</li>
<li>
<p>Current row</p>
<p>Each calculation performed with an analytic function is based on a current row within a partition. The current row serves as the reference point determining the start and end of the window. For instance, a centered moving average calculation could be defined with a window that holds the current row, the six preceding rows, and the following six rows. This would create a sliding window of 13 rows, as shown in <a href="#i1006330">Figure 22-2</a>.</p>
</li>
</ul>
<div class="figure">
<p class="titleinfigure"><a id="i1006330"></a><a id="DWHSG8662"></a>Figure 22-2 Sliding Window Example</p>
<img width="378" height="245" src="./dwhsg022.gif" alt="Description of Figure 22-2 follows" /><br />
<a id="sthref1107" href="./img_text/dwhsg022.htm">Description of "Figure 22-2 Sliding Window Example"</a><br />
<br /></div>
<!-- class="figure" --></div>
<!-- class="sect1" -->
<a id="BCFDFACB"></a><a id="DWHSG0201"></a>
<div class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1">Ranking, Windowing, and Reporting Functions</h2>
<p>This section illustrates the basic analytic functions for ranking, windowing, and reporting.</p>
<a id="DWHSG8663"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1108"></a>
<h3 class="sect2">Ranking</h3>
<p><a id="sthref1109"></a><a id="sthref1110"></a>A ranking function computes the rank of a record compared to other records in the data set based on the values of a set of measures. The types of ranking function are:</p>
<a id="DWHSG8664"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1111"></a>
<h4 class="sect3">RANK and DENSE_RANK Functions</h4>
<p><a id="sthref1112"></a><a id="sthref1113"></a><a id="sthref1114"></a><a id="sthref1115"></a>The <code>RANK</code> and <code>DENSE_RANK</code> functions allow you to rank items in a group, for example, finding the top three products sold in California last year. There are two functions that perform ranking, as shown by the following syntax:</p>
<pre>
RANK ( ) OVER ( [query_partition_clause] order_by_clause )
DENSE_RANK ( ) OVER ( [query_partition_clause] order_by_clause )
</pre>
<p>The difference between <code>RANK</code> and <code>DENSE_RANK</code> is that <code>DENSE_RANK</code> leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using <code>DENSE_RANK</code> and had three people tie for second place, you would say that all three were in second place and that the next person came in third. The <code>RANK</code> function would also give three people in second place, but the next person would be in fifth place.</p>
<p>The following are some relevant points about <code>RANK</code>:</p>
<ul>
<li>
<p>Ascending is the default sort order, which you may want to change to descending.</p>
</li>
<li>
<p>The expressions in the optional <code>PARTITION</code> <code>BY</code> clause divide the query result set into groups within which the <code>RANK</code> function operates. That is, <code>RANK</code> gets reset whenever the group changes. In effect, the value expressions of the <code>PARTITION</code> <code>BY</code> clause define the reset boundaries.</p>
</li>
<li>
<p>If the <code>PARTITION</code> <code>BY</code> clause is missing, then ranks are computed over the entire query result set.</p>
</li>
<li>
<p>The <code>ORDER</code> <code>BY</code> clause specifies the measures (&lt;<code>value</code> <code>expression</code>&gt;) on which ranking is done and defines the order in which rows are sorted in each group (or partition). Once the data is sorted within each partition, ranks are given to each row starting from 1.</p>
</li>
<li>
<p>The <code>NULLS</code> <code>FIRST</code> | <code>NULLS</code> <code>LAST</code> clause indicates the position of <code>NULLs</code> in the ordered sequence, either first or last in the sequence. The order of the sequence would make <code>NULLs</code> compare either high or low with respect to non-<code>NULL</code> values. If the sequence were in ascending order, then <code>NULLS</code> <code>FIRST</code> implies that <code>NULLs</code> are smaller than all other non-<code>NULL</code> values and <code>NULLS</code> <code>LAST</code> implies they are larger than non-<code>NULL</code> values. It is the opposite for descending order. See the example in <a href="#BCFFHDBI">"Treatment of NULLs"</a>.</p>
</li>
<li>
<p>If the <code>NULLS</code> <code>FIRST</code> | <code>NULLS</code> <code>LAST</code> clause is omitted, then the ordering of the null values depends on the <code>ASC</code> or <code>DESC</code> arguments. Null values are considered larger than any other values. If the ordering sequence is <code>ASC</code>, then nulls will appear last; nulls will appear first otherwise. Nulls are considered equal to other nulls and, therefore, the order in which nulls are presented is non-deterministic.</p>
</li>
</ul>
<a id="DWHSG8665"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" --><a id="sthref1116"></a>
<h5 class="sect4">Ranking Order</h5>
<p>The following example shows how the <code>[ASC | DESC]</code> option changes the ranking order.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8666"></a><a id="sthref1117"></a>Example 22-1 Ranking Order</p>
<pre>
SELECT channel_desc, TO_CHAR(SUM(amount_sold), '9,999,999,999') SALES$,
   RANK() OVER (ORDER BY SUM(amount_sold)) AS default_rank,
   RANK() OVER (ORDER BY SUM(amount_sold) DESC NULLS LAST) AS custom_rank
FROM sales, products, customers, times, channels, countries
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND customers.country_id = countries.country_id AND sales.time_id=times.time_id
  AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-09', '2000-10')
  AND country_iso_code='US'
GROUP BY channel_desc;

CHANNEL_DESC         SALES$         DEFAULT_RANK CUSTOM_RANK
-------------------- -------------- ------------ -----------
Direct Sales              1,320,497            3           1
Partners                    800,871            2           2
Internet                    261,278            1           3
</pre>
<p>While the data in this result is ordered on the measure <code>SALES$</code>, in general, it is not guaranteed by the <code>RANK</code> function that the data will be sorted on the measures. If you want the data to be sorted on <code>SALES$</code> in your result, you must specify it explicitly with an <code>ORDER</code> <code>BY</code> clause, at the end of the <code>SELECT</code> statement.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect4" -->
<a id="DWHSG8667"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" --><a id="sthref1118"></a>
<h5 class="sect4">Ranking on Multiple Expressions</h5>
<p>Ranking functions must resolve ties between values in the set. If the first expression cannot resolve ties, the second expression is used to resolve ties and so on. For example, here is a query ranking three of the sales channels over two months based on their dollar sales, breaking ties with the unit sales. (Note that the <code>TRUNC</code> function is used here only to create tie values for this query.)</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8668"></a><a id="sthref1119"></a>Example 22-2 Ranking On Multiple Expressions</p>
<pre>
SELECT channel_desc, calendar_month_desc, TO_CHAR(TRUNC(SUM(amount_sold),-5),
  '9,999,999,999') SALES$, TO_CHAR(SUM(quantity_sold), '9,999,999,999') 
  SALES_Count, RANK() OVER (ORDER BY TRUNC(SUM(amount_sold), -5) 
  DESC, SUM(quantity_sold) DESC) AS col_rank
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-09', '2000-10')
  AND channels.channel_desc&lt;&gt;'Tele Sales'
GROUP BY channel_desc, calendar_month_desc;

CHANNEL_DESC         CALENDAR SALES$         SALES_COUNT     COL_RANK
-------------------- -------- -------------- -------------- ---------
Direct Sales         2000-10       1,200,000         12,584          1
Direct Sales         2000-09       1,200,000         11,995          2
Partners             2000-10         600,000          7,508          3
Partners             2000-09         600,000          6,165          4
Internet             2000-09         200,000          1,887          5
Internet             2000-10         200,000          1,450          6
</pre>
<p>The <code>sales_count</code> column breaks the ties for three pairs of values.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect4" -->
<a id="DWHSG8669"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" --><a id="sthref1120"></a>
<h5 class="sect4">RANK and DENSE_RANK Difference</h5>
<p>The difference between <code>RANK</code> and <code>DENSE_RANK</code> functions is illustrated in <a href="#BCFCJCEA">Example 22-3</a>.</p>
<div class="example">
<p class="titleinexample"><a id="BCFCJCEA"></a><a id="DWHSG8670"></a>Example 22-3 RANK and DENSE_RANK</p>
<pre>
SELECT channel_desc, calendar_month_desc,
   TO_CHAR(TRUNC(SUM(amount_sold),-5), '9,999,999,999') SALES$,
      RANK() OVER (ORDER BY TRUNC(SUM(amount_sold),-5) DESC) AS RANK,
DENSE_RANK() OVER (ORDER BY TRUNC(SUM(amount_sold),-5) DESC) AS DENSE_RANK
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id
  AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-09', '2000-10')
  AND channels.channel_desc&lt;&gt;'Tele Sales'
GROUP BY channel_desc, calendar_month_desc;

CHANNEL_DESC         CALENDAR SALES$              RANK DENSE_RANK
-------------------- -------- -------------- --------- ----------
Direct Sales         2000-09       1,200,000         1          1
Direct Sales         2000-10       1,200,000         1          1
Partners             2000-09         600,000         3          2
Partners             2000-10         600,000         3          2
Internet             2000-09         200,000         5          3
Internet             2000-10         200,000         5          3
</pre>
<p>Note that, in the case of <code>DENSE_RANK</code>, the largest rank value gives the number of distinct values in the data set.</p>
</div>
<!-- class="example" -->
<a id="DWHSG8671"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" --><a id="sthref1121"></a>
<h5 class="sect4">Per Group Ranking</h5>
<p>The <code>RANK</code> function can be made to operate within groups, that is, the rank gets reset whenever the group changes. This is accomplished with the <code>PARTITION</code> <code>BY</code> clause. The group expressions in the <code>PARTITION</code> <code>BY</code> subclause divide the data set into groups within which <code>RANK</code> operates. For example, to rank products within each channel by their dollar sales, you could issue the following statement.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8672"></a><a id="sthref1122"></a>Example 22-4 Per Group Ranking Example 1</p>
<pre>
SELECT channel_desc, calendar_month_desc, TO_CHAR(SUM(amount_sold),
 '9,999,999,999') SALES$, RANK() OVER (PARTITION BY channel_desc
  ORDER BY SUM(amount_sold) DESC) AS RANK_BY_CHANNEL
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-08', '2000-09', '2000-10', '2000-11')
  AND channels.channel_desc IN ('Direct Sales', 'Internet')
GROUP BY channel_desc, calendar_month_desc;

CHANNEL_DESC         CALENDAR SALES$         RANK_BY_CHANNEL
-------------------- -------- -------------- ---------------
Direct Sales         2000-08       1,236,104               1
Direct Sales         2000-10       1,225,584               2
Direct Sales         2000-09       1,217,808               3
Direct Sales         2000-11       1,115,239               4
Internet             2000-11         284,742               1
Internet             2000-10         239,236               2
Internet             2000-09         228,241               3
Internet             2000-08         215,107               4
 
8 rows selected.
</pre>
<p>A single query block can contain more than one ranking function, each partitioning the data into different groups (that is, reset on different boundaries). The groups can be mutually exclusive. The following query ranks products based on their dollar sales within each month (<code>rank_of_product_per_region</code>) and within each channel (<code>rank_of_product_total</code>).</p>
</div>
<!-- class="example" -->
<div class="example">
<p class="titleinexample"><a id="DWHSG8673"></a><a id="sthref1123"></a>Example 22-5 Per Group Ranking Example 2</p>
<pre>
SELECT channel_desc, calendar_month_desc, TO_CHAR(SUM(amount_sold),
  '9,999,999,999') SALES$, RANK() OVER (PARTITION BY calendar_month_desc
  ORDER BY SUM(amount_sold) DESC) AS RANK_WITHIN_MONTH, RANK() OVER (PARTITION
 BY channel_desc ORDER BY SUM(amount_sold) DESC) AS RANK_WITHIN_CHANNEL 
FROM sales, products, customers, times, channels, countries
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND customers.country_id = countries.country_id AND sales.time_id=times.time_id
  AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-08', '2000-09', '2000-10', '2000-11')
  AND channels.channel_desc IN ('Direct Sales', 'Internet')
GROUP BY channel_desc, calendar_month_desc;

CHANNEL_DESC  CALENDAR    SALES$     RANK_WITHIN_MONTH  RANK_WITHIN_CHANNEL
------------- --------    ---------  -----------------  -------------------
Direct Sales   2000-08    1,236,104                  1                    1
Internet       2000-08      215,107                  2                    4
Direct Sales   2000-09    1,217,808                  1                    3
Internet       2000-09      228,241                  2                    3
Direct Sales   2000-10    1,225,584                  1                    2
Internet       2000-10      239,236                  2                    2
Direct Sales   2000-11    1,115,239                  1                    4
Internet       2000-11      284,742                  2                    1
</pre></div>
<!-- class="example" --></div>
<!-- class="sect4" -->
<a id="DWHSG8674"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" --><a id="sthref1124"></a>
<h5 class="sect4">Per Cube and Rollup Group Ranking</h5>
<p>Analytic functions, <code>RANK</code> for example, can be reset based on the groupings provided by a <code>CUBE</code>, <code>ROLLUP</code>, or <code>GROUPING</code> <code>SETS</code> operator. It is useful to assign ranks to the groups created by <code>CUBE</code>, <code>ROLLUP</code>, and <code>GROUPING</code> <code>SETS</code> queries. See <a href="aggreg.htm#g1023942">Chapter 21, "SQL for Aggregation in Data Warehouses"</a> for further information about the <code>GROUPING</code> function.</p>
<p>A sample <code>CUBE</code> and <code>ROLLUP</code> query is the following:</p>
<pre>
SELECT channel_desc, country_iso_code, 
  TO_CHAR(SUM(amount_sold), '9,999,999,999')
 SALES$, RANK() OVER (PARTITION BY GROUPING_ID(channel_desc, country_iso_code)
    ORDER BY SUM(amount_sold) DESC) AS RANK_PER_GROUP
FROM sales, customers, times, channels, countries
WHERE sales.time_id=times.time_id AND sales.cust_id=customers.cust_id
  AND sales.channel_id = channels.channel_id AND channels.channel_desc
   IN ('Direct Sales', 'Internet') AND times.calendar_month_desc='2000-09'
   AND country_iso_code IN ('GB', 'US', 'JP')
GROUP BY CUBE(channel_desc, country_iso_code);

CHANNEL_DESC         CO SALES$         RANK_PER_GROUP
-------------------- -- -------------- --------------
Direct Sales         GB      1,217,808              1
Direct Sales         JP      1,217,808              1
Direct Sales         US      1,217,808              1
Internet             GB        228,241              4
Internet             JP        228,241              4
Internet             US        228,241              4
Direct Sales                 3,653,423              1
Internet                       684,724              2
                     GB      1,446,049              1
                     JP      1,446,049              1
                     US      1,446,049              1
                             4,338,147              1
</pre>
<a id="BCFFHDBI"></a><a id="DWHSG8675"></a>
<div class="sect4"><!-- infolevel="all" infotype="General" -->
<h5 class="sect4">Treatment of NULLs</h5>
<p><code>NULLs</code> are treated like normal values. Also, for rank computation, a <code>NULL</code> value is assumed to be equal to another <code>NULL</code> value. Depending on the <code>ASC</code> | <code>DESC</code> options provided for measures and the <code>NULLS</code> <code>FIRST</code> | <code>NULLS</code> <code>LAST</code> clause, <code>NULLs</code> will either sort low or high and hence, are given ranks appropriately. The following example shows how <code>NULLs</code> are ranked in different cases:</p>
<pre>
SELECT  times.time_id time,  sold,
  RANK() OVER (ORDER BY (sold) DESC NULLS LAST) AS NLAST_DESC,
  RANK() OVER (ORDER BY (sold) DESC NULLS FIRST) AS NFIRST_DESC,
  RANK() OVER (ORDER BY (sold) ASC NULLS FIRST) AS NFIRST,
  RANK() OVER (ORDER BY (sold) ASC NULLS LAST) AS NLAST
FROM
  (
   SELECT time_id, SUM(sales.amount_sold)  sold
   FROM sales, products, customers, countries
   WHERE sales.prod_id=products.prod_id
    AND customers.country_id = countries.country_id 
    AND sales.cust_id=customers.cust_id 
    AND prod_name IN ('Envoy Ambassador', 'Mouse Pad') AND country_iso_code ='GB'
   GROUP BY  time_id)
 v, times
WHERE v.time_id (+) = times.time_id 
  AND calendar_year=1999
  AND calendar_month_number=1
ORDER BY sold  DESC NULLS LAST;

TIME            SOLD NLAST_DESC NFIRST_DESC     NFIRST      NLAST
--------- ---------- ---------- ----------- ---------- ----------
25-JAN-99    3097.32          1          18         31         14
17-JAN-99    1791.77          2          19         30         13
30-JAN-99     127.69          3          20         29         12
28-JAN-99     120.34          4          21         28         11
23-JAN-99      86.12          5          22         27         10
20-JAN-99      79.07          6          23         26          9
13-JAN-99       56.1          7          24         25          8
07-JAN-99      42.97          8          25         24          7
08-JAN-99      33.81          9          26         23          6
10-JAN-99      22.76         10          27         21          4
02-JAN-99      22.76         10          27         21          4
26-JAN-99      19.84         12          29         20          3
16-JAN-99      11.27         13          30         19          2
14-JAN-99       9.52         14          31         18          1
09-JAN-99                    15           1          1         15
12-JAN-99                    15           1          1         15
31-JAN-99                    15           1          1         15
11-JAN-99                    15           1          1         15
19-JAN-99                    15           1          1         15
03-JAN-99                    15           1          1         15
15-JAN-99                    15           1          1         15
21-JAN-99                    15           1          1         15
24-JAN-99                    15           1          1         15
04-JAN-99                    15           1          1         15
06-JAN-99                    15           1          1         15
27-JAN-99                    15           1          1         15
18-JAN-99                    15           1          1         15
01-JAN-99                    15           1          1         15
22-JAN-99                    15           1          1         15
29-JAN-99                    15           1          1         15
05-JAN-99                    15           1          1         15
</pre></div>
<!-- class="sect4" --></div>
<!-- class="sect4" --></div>
<!-- class="sect4" --></div>
<!-- class="sect3" -->
<a id="DWHSG8676"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1125"></a>
<h4 class="sect3">Bottom N Ranking</h4>
<p>Bottom N is similar to top N except for the ordering sequence within the rank expression. Using the previous example, you can order <code>SUM(s_amount)</code> ascending instead of descending.</p>
</div>
<!-- class="sect3" -->
<a id="DWHSG8677"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1126"></a>
<h4 class="sect3">CUME_DIST Function</h4>
<p><a id="sthref1127"></a><a id="sthref1128"></a>The <code>CUME_DIST</code> function (defined as the inverse of percentile in some statistical books) computes the position of a specified value relative to a set of values. The order can be ascending or descending. Ascending is the default. The range of values for <code>CUME_DIST</code> is from greater than 0 to 1. To compute the <code>CUME_DIST</code> of a value x in a set S of size N, you use the formula:</p>
<pre>
CUME_DIST(x) =  number of values in S coming before 
   and including x in the specified order/ N
</pre>
<pre>
</pre>
<p>Its syntax is:</p>
<pre>
CUME_DIST ( ) OVER ( [query_partition_clause] order_by_clause )
</pre>
<p>The semantics of various options in the <code>CUME_DIST</code> function are similar to those in the <code>RANK</code> function. The default order is ascending, implying that the lowest value gets the lowest <code>CUME_DIST</code> (as all other values come later than this value in the order). <code>NULLs</code> are treated the same as they are in the <code>RANK</code> function. They are counted toward both the numerator and the denominator as they are treated like non-<code>NULL</code> values. The following example finds cumulative distribution of sales by channel within each month:</p>
<pre>
SELECT calendar_month_desc AS MONTH, channel_desc,
     TO_CHAR(SUM(amount_sold) , '9,999,999,999') SALES$,
     CUME_DIST() OVER (PARTITION BY calendar_month_desc ORDER BY
         SUM(amount_sold) ) AS CUME_DIST_BY_CHANNEL
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2000-09', '2000-07','2000-08')
GROUP BY calendar_month_desc, channel_desc;

MONTH    CHANNEL_DESC         SALES$         CUME_DIST_BY_CHANNEL
-------- -------------------- -------------- --------------------
2000-07  Internet                    140,423           .333333333
2000-07  Partners                    611,064           .666666667
2000-07  Direct Sales              1,145,275                    1
2000-08  Internet                    215,107           .333333333
2000-08  Partners                    661,045           .666666667
2000-08  Direct Sales              1,236,104                    1
2000-09  Internet                    228,241           .333333333
2000-09  Partners                    666,172           .666666667
2000-09  Direct Sales              1,217,808                    1
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8678"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1129"></a>
<h4 class="sect3">PERCENT_RANK Function</h4>
<p><code><a id="sthref1130"></a><a id="sthref1131"></a>PERCENT_RANK</code> is similar to <code>CUME_DIST</code>, but it uses rank values rather than row counts in its numerator. Therefore, it returns the percent rank of a value relative to a group of values. The function is available in many popular spreadsheets. <code>PERCENT_RANK</code> of a row is calculated as:</p>
<pre>
(rank of row in its partition - 1) / (number of rows in the partition - 1)
</pre>
<p><code>PERCENT_RANK</code> returns values in the range zero to one. The row(s) with a rank of 1 will have a <code>PERCENT_RANK</code> of zero. Its syntax is:</p>
<pre>
PERCENT_RANK () OVER ([query_partition_clause] order_by_clause)
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8679"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1132"></a>
<h4 class="sect3">NTILE Function</h4>
<p><code><a id="sthref1133"></a><a id="sthref1134"></a>NTILE</code> allows easy calculation of tertiles, quartiles, deciles and other common summary statistics. This function divides an ordered partition into a specified number of groups called <span class="bold">buckets</span> and assigns a bucket number to each row in the partition. <code>NTILE</code> is a very useful calculation because it lets users divide a data set into fourths, thirds, and other groupings.</p>
<p>The buckets are calculated so that each bucket has exactly the same number of rows assigned to it or at most 1 row more than the others. For instance, if you have 100 rows in a partition and ask for an <code>NTILE</code> function with four buckets, 25 rows will be assigned a value of 1, 25 rows will have value 2, and so on. These buckets are referred to as equiheight buckets.</p>
<p>If the number of rows in the partition does not divide evenly (without a remainder) into the number of buckets, then the number of rows assigned for each bucket will differ by one at most. The extra rows will be distributed one for each bucket starting from the lowest bucket number. For instance, if there are 103 rows in a partition which has an <code>NTILE(5)</code> function, the first 21 rows will be in the first bucket, the next 21 in the second bucket, the next 21 in the third bucket, the next 20 in the fourth bucket and the final 20 in the fifth bucket.</p>
<p>The <code>NTILE</code> function has the following syntax:</p>
<pre>
NTILE (expr) OVER ([query_partition_clause] order_by_clause)
</pre>
<p>In this, the N in <code>NTILE(N)</code> can be a constant (for example, 5) or an expression.</p>
<p>This function, like <code>RANK</code> and <code>CUME_DIST</code>, has a <code>PARTITION</code> <code>BY</code> clause for per group computation, an <code>ORDER</code> <code>BY</code> clause for specifying the measures and their sort order, and <code>NULLS</code> <code>FIRST</code> | <code>NULLS</code> <code>LAST</code> clause for the specific treatment of <code>NULLs</code>. For example, the following is an example assigning each month's sales total into one of four buckets:</p>
<pre>
SELECT calendar_month_desc AS MONTH , TO_CHAR(SUM(amount_sold),
 '9,999,999,999')
 SALES$, NTILE(4) OVER (ORDER BY SUM(amount_sold)) AS TILE4
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_year=2000 AND prod_category= 'Electronics'
GROUP BY calendar_month_desc;

MONTH    SALES$              TILE4
-------- -------------- ----------
2000-02         242,416          1
2000-01         257,286          1
2000-03         280,011          1
2000-06         315,951          2
2000-05         316,824          2
2000-04         318,106          2
2000-07         433,824          3
2000-08         477,833          3
2000-12         553,534          3
2000-10         652,225          4
2000-11         661,147          4
2000-09         691,449          4
</pre>
<p><code>NTILE</code> <code>ORDER</code> <code>BY</code> statements must be fully specified to yield reproducible results. Equal values can get distributed across adjacent buckets. To ensure deterministic results, you must order on a unique key.</p>
</div>
<!-- class="sect3" -->
<a id="DWHSG8680"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1135"></a>
<h4 class="sect3">ROW_NUMBER Function</h4>
<p><a id="sthref1136"></a><a id="sthref1137"></a>The <code>ROW_NUMBER</code> function assigns a unique number (sequentially, starting from 1, as defined by <code>ORDER</code> <code>BY</code>) to each row within the partition. It has the following syntax:</p>
<pre>
ROW_NUMBER ( ) OVER ( [query_partition_clause] order_by_clause )
</pre>
<div class="example">
<p class="titleinexample"><a id="DWHSG8681"></a><a id="sthref1138"></a>Example 22-6 ROW_NUMBER</p>
<pre>
SELECT channel_desc, calendar_month_desc,
   TO_CHAR(TRUNC(SUM(amount_sold), -5), '9,999,999,999') SALES$,
   ROW_NUMBER() OVER (ORDER BY TRUNC(SUM(amount_sold), -6) DESC) AS ROW_NUMBER 
FROM sales, products, customers, times, channels
WHERE sales.prod_id=products.prod_id AND sales.cust_id=customers.cust_id
  AND sales.time_id=times.time_id AND sales.channel_id=channels.channel_id
  AND times.calendar_month_desc IN ('2001-09', '2001-10')
GROUP BY channel_desc, calendar_month_desc;

CHANNEL_DESC         CALENDAR SALES$         ROW_NUMBER
-------------------- -------- -------------- ----------
Direct Sales         2001-10       1,000,000          1
Direct Sales         2001-09       1,100,000          2
Internet             2001-09         500,000          3
Partners             2001-09         600,000          4
Partners             2001-10         600,000          5
Internet             2001-10         700,000          6
</pre>
<p>Note that there are three pairs of tie values in these results. Like <code>NTILE</code>, <code>ROW_NUMBER</code> is a non-deterministic function, so each tied value could have its row number switched. To ensure deterministic results, you must order on a unique key. Inmost cases, that will require adding a new tie breaker column to the query and using it in the <code>ORDER</code> <code>BY</code> specification.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="DWHSG0202"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1139"></a>
<h3 class="sect2">Windowing</h3>
<p><a id="sthref1140"></a><a id="sthref1141"></a>Windowing functions can be used to compute cumulative, moving, and centered aggregates. They return a value for each row in the table, which depends on other rows in the corresponding window. With windowing aggregate functions, you can calculate moving and cumulative versions of <code>SUM</code>, <code>AVERAGE</code>, <code>COUNT</code>, <code>MAX</code>, <code>MIN</code>, and many more functions. They can be used only in the <code>SELECT</code> and <code>ORDER</code> <code>BY</code> clauses of the query. Windowing aggregate functions include the convenient <code>FIRST_VALUE</code>, which returns the first value in the window; and <code>LAST_VALUE</code>, which returns the last value in the window. These functions provide access to more than one row of a table without a self-join. The syntax of the windowing functions is:</p>
<pre>
analytic_function([ arguments ])
   OVER (analytic_clause)

 where analytic_clause =
     [ query_partition_clause ]
     [ order_by_clause [ windowing_clause ] ]

and query_partition_clause =
    PARTITION BY
      { value_expr[, value_expr ]...
      }

 and windowing_clause =
     { ROWS | RANGE }
     { BETWEEN
       { UNBOUNDED PRECEDING
     | CURRENT ROW
     | value_expr { PRECEDING | FOLLOWING }
     } 
     AND
     { UNBOUNDED FOLLOWING
     | CURRENT ROW
     | value_expr { PRECEDING | FOLLOWING }
     }
   | { UNBOUNDED PRECEDING
     | CURRENT ROW
     | value_expr PRECEDING
     }
   }
</pre>
<p>Note that the <code>DISTINCT</code> keyword is not supported in windowing functions except for <code>MAX</code> and <code>MIN</code>.</p>
<div class="infoboxnotealso">
<p class="notep1"><span class="bold">See Also</span>:</p>
<a class="olink SQLRF20035" href="../../server.112/e41084/functions003.htm#SQLRF20035"><span class="italic">Oracle Database SQL Language Reference</span></a> for further information regarding syntax and restrictions</div>
<a id="DWHSG8682"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1142"></a>
<h4 class="sect3">Treatment of NULLs as Input to Window Functions</h4>
<p>Window functions' <code>NULL</code> semantics match the <code>NULL</code> semantics for SQL aggregate functions. Other semantics can be obtained by user-defined functions, or by using the <code>DECODE</code> or a <code>CASE</code> expression within the window function.</p>
</div>
<!-- class="sect3" -->
<a id="DWHSG8683"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1143"></a>
<h4 class="sect3">Windowing Functions with Logical Offset</h4>
<p>A logical offset can be specified with constants such as <code>RANGE 10 PRECEDING</code>, or an expression that evaluates to a constant, or by an interval specification like <code>RANGE</code> <code>INTERVAL</code> <code>N</code> <code>DAY</code>/<code>MONTH</code>/<code>YEAR</code> <code>PRECEDING</code> or an expression that evaluates to an interval.</p>
<p>With logical offset, there can only be one expression in the <code>ORDER</code> <code>BY</code> expression list in the function, with type compatible to <code>NUMERIC</code> if offset is numeric, or <code>DATE</code> if an interval is specified.</p>
<p>An analytic function that uses the <code>RANGE</code> keyword can use multiple sort keys in its <code>ORDER</code> <code>BY</code> clause if it specifies either of these two windows:</p>
<ul>
<li>
<p><code>RANGE</code> <code>BETWEEN</code> <code>UNBOUNDED</code> <code>PRECEDING</code> <code>AND</code> <code>CURRENT</code> <code>ROW</code>. The short form of this is <code>RANGE</code> <code>UNBOUNDED</code> <code>PRECEDING</code>, which can also be used.</p>
</li>
<li>
<p><code>RANGE</code> <code>BETWEEN</code> <code>CURRENT</code> <code>ROW</code> <code>AND</code> <code>UNBOUNDED</code> <code>FOLLOWING</code>.</p>
</li>
</ul>
<p>Window boundaries that do not meet these conditions can have only one sort key in the analytic function's <code>ORDER</code> <code>BY</code> clause.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8684"></a><a id="sthref1144"></a>Example 22-7 Cumulative Aggregate Function</p>
<p>The following is an example of cumulative <code>amount_sold</code> by customer ID by quarter in 2000:</p>
<pre>
SELECT c.cust_id, t.calendar_quarter_desc, TO_CHAR (SUM(amount_sold),
  '9,999,999,999.99') AS Q_SALES, TO_CHAR(SUM(SUM(amount_sold))
OVER (PARTITION BY c.cust_id ORDER BY c.cust_id, t.calendar_quarter_desc
ROWS UNBOUNDED
PRECEDING), '9,999,999,999.99') AS CUM_SALES
  FROM sales s, times t, customers c
  WHERE s.time_id=t.time_id AND s.cust_id=c.cust_id AND t.calendar_year=2000
    AND c.cust_id IN (2595, 9646, 11111)
  GROUP BY c.cust_id, t.calendar_quarter_desc
  ORDER BY c.cust_id, t.calendar_quarter_desc;

   CUST_ID CALENDA Q_SALES           CUM_SALES
---------- ------- ----------------- -----------------
      2595 2000-01            659.92            659.92
      2595 2000-02            224.79            884.71
      2595 2000-03            313.90          1,198.61
      2595 2000-04          6,015.08          7,213.69
      9646 2000-01          1,337.09          1,337.09
      9646 2000-02            185.67          1,522.76
      9646 2000-03            203.86          1,726.62
      9646 2000-04            458.29          2,184.91
     11111 2000-01             43.18             43.18
     11111 2000-02             33.33             76.51
     11111 2000-03            579.73            656.24
     11111 2000-04            307.58            963.82
</pre>
<p>In this example, the analytic function <code>SUM</code> defines, for each row, a window that starts at the beginning of the partition (<code>UNBOUNDED</code> <code>PRECEDING</code>) and ends, by default, at the current row.</p>
<p>Nested <code>SUM</code>s are needed in this example since we are performing a <code>SUM</code> over a value that is itself a <code>SUM</code>. Nested aggregations are used very often in analytic aggregate functions.</p>
</div>
<!-- class="example" -->
<div class="example">
<p class="titleinexample"><a id="DWHSG8685"></a><a id="sthref1145"></a>Example 22-8 Moving Aggregate Function</p>
<p>This example of a time-based window shows, for one customer, the moving average of sales for the current month and preceding two months:</p>
<pre>
SELECT c.cust_id, t.calendar_month_desc, TO_CHAR (SUM(amount_sold), 
      '9,999,999,999') AS SALES, TO_CHAR(AVG(SUM(amount_sold))
OVER (ORDER BY c.cust_id, t.calendar_month_desc ROWS 2 PRECEDING),
      '9,999,999,999') AS MOVING_3_MONTH_AVG
FROM sales s, times t, customers c
WHERE s.time_id=t.time_id AND s.cust_id=c.cust_id
   AND t.calendar_year=1999 AND c.cust_id IN (6510)
GROUP BY c.cust_id, t.calendar_month_desc
ORDER BY c.cust_id, t.calendar_month_desc; 

   CUST_ID CALENDAR SALES          MOVING_3_MONTH
---------- -------- -------------- --------------
      6510 1999-04             125            125
      6510 1999-05           3,395          1,760
      6510 1999-06           4,080          2,533
      6510 1999-07           6,435          4,637
      6510 1999-08           5,105          5,207
      6510 1999-09           4,676          5,405
      6510 1999-10           5,109          4,963
      6510 1999-11             802          3,529
</pre>
<p>Note that the first two rows for the three month moving average calculation in the output data are based on a smaller interval size than specified because the window calculation cannot reach past the data retrieved by the query. You must consider the different window sizes found at the borders of result sets. In other words, you may need to modify the query to include exactly what you want.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" -->
<a id="DWHSG8686"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1146"></a>
<h4 class="sect3">Centered Aggregate Function</h4>
<p>Calculating windowing aggregate functions centered around the current row is straightforward. This example computes for all customers a centered moving average of sales for one week in late December 1999. It finds an average of the sales total for the one day preceding the current row and one day following the current row including the current row as well.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8687"></a><a id="sthref1147"></a>Example 22-9 Centered Aggregate</p>
<pre>
SELECT t.time_id, TO_CHAR (SUM(amount_sold), '9,999,999,999')
AS SALES, TO_CHAR(AVG(SUM(amount_sold)) OVER
  (ORDER BY t.time_id
   RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND
   INTERVAL '1' DAY FOLLOWING), '9,999,999,999') AS CENTERED_3_DAY_AVG
   FROM sales s, times t
   WHERE s.time_id=t.time_id AND t.calendar_week_number IN (51)
     AND calendar_year=1999
   GROUP BY t.time_id
   ORDER BY t.time_id;

TIME_ID   SALES          CENTERED_3_DAY
--------- -------------- --------------
20-DEC-99        134,337        106,676
21-DEC-99         79,015        102,539
22-DEC-99         94,264         85,342
23-DEC-99         82,746         93,322
24-DEC-99        102,957         82,937
25-DEC-99         63,107         87,062
26-DEC-99         95,123         79,115
</pre>
<p>The starting and ending rows for each product's centered moving average calculation in the output data are based on just two days, since the window calculation cannot reach past the data retrieved by the query. Users must consider the different window sizes found at the borders of result sets: the query may need to be adjusted.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" -->
<a id="DWHSG8688"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1148"></a>
<h4 class="sect3">Windowing Aggregate Functions in the Presence of Duplicates</h4>
<p>The following example illustrates how window aggregate functions compute values when there are duplicates, that is, when multiple rows are returned for a single ordering value. The query retrieves the quantity sold to several customers during a specified time range. (Although we use an inline view to define our base data set, it has no special significance and can be ignored.) The query defines a moving window that runs from the date of the current row to 10 days earlier.Note that the <code>RANGE</code> keyword is used to define the windowing clause of this example. This means that the window can potentially hold many rows for each value in the range. In this case, there are three pairs of rows with duplicate date values.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8689"></a><a id="sthref1149"></a>Example 22-10 Windowing Aggregate Functions with Logical Offsets</p>
<pre>
SELECT time_id, daily_sum, SUM(daily_sum) OVER (ORDER BY time_id
RANGE BETWEEN INTERVAL '10' DAY PRECEDING AND CURRENT ROW)
AS current_group_sum
FROM (SELECT time_id, channel_id, SUM(s.quantity_sold)
AS daily_sum
FROM customers c, sales s, countries
WHERE c.cust_id=s.cust_id
  AND c.country_id = countries.country_id
  AND s.cust_id IN (638, 634, 753, 440 ) AND s.time_id BETWEEN '01-MAY-00'
  AND '13-MAY-00' GROUP BY time_id, channel_id);

TIME_ID    DAILY_SUM CURRENT_GROUP_SUM
--------- ---------- -----------------
06-MAY-00          7                 7   /* 7 */
10-MAY-00          1                 9   /* 7 + (1+1) */
10-MAY-00          1                 9   /* 7 + (1+1) */
11-MAY-00          2                15   /* 7 + (1+1) + (2+4) */
11-MAY-00          4                15   /* 7 + (1+1) + (2+4) */
12-MAY-00          1                16   /* 7 + (1+1) + (2+4) + 1 */
13-MAY-00          2                23   /* 7 + (1+1) + (2+4) + 1 + (5+2) */
13-MAY-00          5                23   /* 7 + (1+1) + (2+4) + 1 + (5+2) */
</pre>
<p>In the output of this example, all dates except May 6 and May 12 return two rows. Examine the commented numbers to the right of the output to see how the values are calculated. Note that each group in parentheses represents the values returned for a single day.</p>
<p>Note that this example applies only when you use the <code>RANGE</code> keyword rather than the <code>ROWS</code> keyword. It is also important to remember that with <code>RANGE</code>, you can only use 1 <code>ORDER</code> <code>BY</code> expression in the analytic function's <code>ORDER</code> <code>BY</code> clause. With the <code>ROWS</code> keyword, you can use multiple order by expressions in the analytic function's <code>ORDER</code> <code>BY</code> clause.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" -->
<a id="DWHSG8690"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1150"></a>
<h4 class="sect3">Varying Window Size for Each Row</h4>
<p>There are situations where it is useful to vary the size of a window for each row, based on a specified condition. For instance, you may want to make the window larger for certain dates and smaller for others. Assume that you want to calculate the moving average of stock price over three working days. If you have an equal number of rows for each day for all working days and no non-working days are stored, then you can use a physical window function. However, if the conditions noted are not met, you can still calculate a moving average by using an expression in the window size parameters.</p>
<p>Expressions in a window size specification can be made in several different sources. the expression could be a reference to a column in a table, such as a time table. It could also be a function that returns the appropriate boundary for the window based on values in the current row. The following statement for a hypothetical stock price database uses a user-defined function in its <code>RANGE</code> clause to set window size:</p>
<pre>
SELECT t_timekey, AVG(stock_price)
        OVER (ORDER BY t_timekey RANGE fn(t_timekey) PRECEDING) av_price
FROM stock, time WHERE st_timekey = t_timekey
ORDER BY t_timekey;
</pre>
<p>In this statement, <code>t_timekey</code> is a date field. Here, <span class="italic">fn</span> could be a PL/SQL function with the following specification:</p>
<p><code>fn(t_timekey)</code> returns</p>
<ul>
<li>
<p>4 if <code>t_timekey</code> is Monday, Tuesday</p>
</li>
<li>
<p>2 otherwise</p>
</li>
<li>
<p>If any of the previous days are holidays, it adjusts the count appropriately.</p>
</li>
</ul>
<p>Note that, when window is specified using a number in a window function with <code>ORDER</code> <code>BY</code> on a date column, then it is converted to mean the number of days. You could have also used the interval literal conversion function, as <code>NUMTODSINTERVAL(fn(t_timekey), 'DAY')</code> instead of just <code>fn(t_timekey)</code> to mean the same thing. You can also write a PL/SQL function that returns an <code>INTERVAL</code> data type value.</p>
</div>
<!-- class="sect3" -->
<a id="DWHSG8691"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1151"></a>
<h4 class="sect3">Windowing Aggregate Functions with Physical Offsets</h4>
<p>For windows expressed in rows, the ordering expressions should be unique to produce deterministic results. For example, the following query is not deterministic because <code>time_id</code> is not unique in this result set.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8692"></a><a id="sthref1152"></a>Example 22-11 Windowing Aggregate Functions With Physical Offsets</p>
<pre>
SELECT t.time_id, TO_CHAR(amount_sold, '9,999,999,999') AS INDIV_SALE,
       TO_CHAR(SUM(amount_sold) OVER (PARTITION BY t.time_id ORDER BY t.time_id
ROWS UNBOUNDED PRECEDING), '9,999,999,999') AS CUM_SALES
FROM sales s, times t, customers c
WHERE s.time_id=t.time_id AND s.cust_id=c.cust_id
  AND t.time_id IN 
   (TO_DATE('11-DEC-1999'), TO_DATE('12-DEC-1999')) 
  AND c.cust_id 
BETWEEN 6500 AND 6600
ORDER BY t.time_id;

TIME_ID   INDIV_SALE    CUM_SALES
--------- ----------    ---------
12-DEC-99         23           23
12-DEC-99          9           32
12-DEC-99         14           46
12-DEC-99         24           70
12-DEC-99         19           89
</pre>
<p>One way to handle this problem would be to add the <code>prod_id</code> column to the result set and order on both <code>time_id</code> and <code>prod_id</code>.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="DWHSG0203"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1153"></a>
<h3 class="sect2">Reporting</h3>
<p><a id="sthref1154"></a><a id="sthref1155"></a>After a query has been processed, aggregate values like the number of resulting rows or an average value in a column can be easily computed within a partition and made available to other reporting functions. Reporting aggregate functions return the same aggregate value for every row in a partition. Their behavior with respect to <code>NULLs</code> is the same as the SQL aggregate functions. The syntax is:</p>
<pre>
{SUM | AVG | MAX | MIN | COUNT | STDDEV | VARIANCE  ... } 
  ([ALL | DISTINCT] {<span class="italic">value expression1</span> [,...] | *})
   OVER ([PARTITION BY <span class="italic">value expression2</span>[,...]])
</pre>
<p>In addition, the following conditions apply:</p>
<ul>
<li>
<p>An asterisk (*) is only allowed in <code>COUNT(*)</code></p>
</li>
<li>
<p><code>DISTINCT</code> is supported only if corresponding aggregate functions allow it.</p>
</li>
<li>
<p><code><span class="codeinlineitalic">value expression1</span></code> and <code><span class="codeinlineitalic">value expression2</span></code> can be any valid expression involving column references or aggregates.</p>
</li>
<li>
<p>The <code>PARTITION</code> <code>BY</code> clause defines the groups on which the windowing functions would be computed. If the <code>PARTITION</code> <code>BY</code> clause is absent, then the function is computed over the whole query result set.</p>
</li>
</ul>
<p>Reporting functions can appear only in the <code>SELECT</code> clause or the <code>ORDER</code> <code>BY</code> clause. The major benefit of reporting functions is their ability to do multiple passes of data in a single query block and speed up query performance. Queries such as "Count the number of salesmen with sales more than 10% of city sales" do not require joins between separate query blocks.</p>
<p>For example, consider the question "For each product category, find the region in which it had maximum sales". The equivalent SQL query using the <code>MAX</code> reporting aggregate function would be:</p>
<pre>
SELECT prod_category, country_region, sales
FROM (SELECT SUBSTR(p.prod_category,1,8) AS prod_category, co.country_region,
 SUM(amount_sold) AS sales,
MAX(SUM(amount_sold)) OVER (PARTITION BY prod_category) AS MAX_REG_SALES
FROM sales s, customers c, countries co, products p
WHERE s.cust_id=c.cust_id AND c.country_id=co.country_id
   AND s.prod_id =p.prod_id AND s.time_id = TO_DATE('11-OCT-2001')
GROUP BY prod_category, country_region)
WHERE sales = MAX_REG_SALES;
</pre>
<p>The inner query with the reporting aggregate function <code>MAX(SUM(amount_sold))</code> returns:</p>
<pre>
PROD_CAT COUNTRY_REGION            SALES MAX_REG_SALES
-------- -------------------- ---------- -------------
Electron Americas                 581.92        581.92
Hardware Americas                 925.93        925.93
Peripher Americas                3084.48       4290.38
Peripher Asia                    2616.51       4290.38
Peripher Europe                  4290.38       4290.38
Peripher Oceania                  940.43       4290.38
Software Americas                 4445.7        4445.7
Software Asia                    1408.19        4445.7
Software Europe                  3288.83        4445.7
Software Oceania                  890.25        4445.7
</pre>
<p>The full query results are:</p>
<pre>
PROD_CAT COUNTRY_REGION            SALES
-------- -------------------- ----------
Electron Americas                 581.92
Hardware Americas                 925.93
Peripher Europe                  4290.38
Software Americas                 4445.7
</pre>
<div class="example">
<p class="titleinexample"><a id="DWHSG8693"></a><a id="sthref1156"></a>Example 22-12 Reporting Aggregate Example</p>
<p>Reporting aggregates combined with nested queries enable you to answer complex queries efficiently. For example, what if you want to know the best selling products in your most significant product subcategories? The following is a query which finds the 5 top-selling products for each product subcategory that contributes more than 20% of the sales within its product category:</p>
<pre>
SELECT SUBSTR(prod_category,1,8) AS CATEG, prod_subcategory, prod_id, SALES 
FROM (SELECT p.prod_category, p.prod_subcategory, p.prod_id,
      SUM(amount_sold) AS SALES,
      SUM(SUM(amount_sold)) OVER (PARTITION BY p.prod_category) AS CAT_SALES,
      SUM(SUM(amount_sold)) OVER 
         (PARTITION BY p.prod_subcategory) AS SUBCAT_SALES,
      RANK() OVER  (PARTITION BY p.prod_subcategory  
         ORDER BY SUM(amount_sold) ) AS RANK_IN_LINE
     FROM sales s, customers c, countries co, products p 
     WHERE s.cust_id=c.cust_id
       AND c.country_id=co.country_id AND  s.prod_id=p.prod_id
       AND s.time_id=to_DATE('11-OCT-2000')
     GROUP BY p.prod_category, p.prod_subcategory, p.prod_id
     ORDER BY prod_category, prod_subcategory) 
  WHERE SUBCAT_SALES&gt;0.2*CAT_SALES AND RANK_IN_LINE&lt;=5;
</pre></div>
<!-- class="example" -->
<a id="DWHSG8694"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1157"></a>
<h4 class="sect3">RATIO_TO_REPORT Function</h4>
<p><a id="sthref1158"></a><a id="sthref1159"></a>The <code>RATIO_TO_REPORT</code> function computes the ratio of a value to the sum of a set of values. If the expression <code>value</code> <code>expression</code> evaluates to <code>NULL</code>, <code>RATIO_TO_REPORT</code> also evaluates to <code>NULL</code>, but it is treated as zero for computing the sum of values for the denominator. Its syntax is:</p>
<pre>
RATIO_TO_REPORT ( expr ) OVER ( [query_partition_clause] )
</pre>
<p>In this, the following applies:</p>
<ul>
<li>
<p><code>expr</code> can be any valid expression involving column references or aggregates.</p>
</li>
<li>
<p>The <code>PARTITION</code> <code>BY</code> clause defines the groups on which the <code>RATIO_TO_REPORT</code> function is to be computed. If the <code>PARTITION</code> <code>BY</code> clause is absent, then the function is computed over the whole query result set.</p>
</li>
</ul>
<div class="example">
<p class="titleinexample"><a id="DWHSG8695"></a><a id="sthref1160"></a>Example 22-13 RATIO_TO_REPORT</p>
<p>To calculate <code>RATIO_TO_REPORT</code> of sales for each channel, you might use the following syntax:</p>
<pre>
SELECT ch.channel_desc, TO_CHAR(SUM(amount_sold),'9,999,999') AS SALES,
     TO_CHAR(SUM(SUM(amount_sold)) OVER (), '9,999,999') AS TOTAL_SALES,
     TO_CHAR(RATIO_TO_REPORT(SUM(amount_sold)) OVER (), '9.999')
     AS RATIO_TO_REPORT
FROM sales s, channels ch 
WHERE s.channel_id=ch.channel_id  AND s.time_id=to_DATE('11-OCT-2000')
GROUP BY ch.channel_desc;

CHANNEL_DESC         SALES      TOTAL_SALE RATIO_
-------------------- ---------- ---------- ------
Direct Sales             14,447     23,183   .623
Internet                    345     23,183   .015
Partners                  8,391     23,183   .362
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="DWHSG0204"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1161"></a>
<h3 class="sect2">LAG/LEAD</h3>
<p><a id="sthref1162"></a><a id="sthref1163"></a>The <code>LAG</code> and <code>LEAD</code> functions are useful for comparing values when the relative positions of rows can be known reliably. They work by specifying the count of rows which separate the target row from the current row. Because the functions provide access to more than one row of a table at the same time without a self-join, they can enhance processing speed. The <code>LAG</code> function provides access to a row at a given offset prior to the current position, and the <code>LEAD</code> function provides access to a row at a given offset after the current position.</p>
<p>The <code>LAG</code> and <code>LEAD</code> functions can be thought of as being related to, and a simplification of, the <code>NTH_VALUE</code> function. With <code>LAG</code> and <code>LEAD</code>, you can only retrieve values from a row at the specified physical offset. If this is insufficient, you can use <code>NTH_VALUE</code>, which enables you to retrieve values from a row based on what is called a logical offset or relative position. You can use the <code>IGNORE</code> <code>NULLS</code> option with the <code>NTH_VALUE</code> function to make it more useful, in the sense that you can specify conditions and filter out rows based on certain conditions. See <a href="#BCFIBFFI">Example 22-17, "NTH_VALUE"</a>, where rows with quantities less than eight are filtered out. This cannot be done with <code>LAG</code> or <code>LEAD</code>, as you would not know the offset to the row.</p>
<p>See <a href="#BCFBHAAI">"NTH_VALUE Function"</a> and <a class="olink SQLRF" href="../e41084/toc.htm"><span class="italic">Oracle Database SQL Language Reference</span></a> for more information.</p>
<a id="DWHSG8696"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1164"></a>
<h4 class="sect3">LAG/LEAD Syntax</h4>
<p>These functions have the following syntax:</p>
<pre>
{LAG | LEAD} ( value_expr [, offset] [, default] ) [RESPECT NULLS|IGNORE NULLS]
   OVER ( [query_partition_clause] order_by_clause )
</pre>
<p><code><span class="codeinlineitalic">offset</span></code> is an optional parameter and defaults to 1. <code><span class="codeinlineitalic">default</span></code> is an optional parameter and is the value returned if <code><span class="codeinlineitalic">offset</span></code> falls outside the bounds of the table or partition. When <code>IGNORE</code> <code>NULLS</code> is specified, the value returned will be from a row at a specified lag or lead offset after ignoring rows with NULLs.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8697"></a><a id="sthref1165"></a>Example 22-14 LAG/LEAD</p>
<p>This example illustrates a typical case of using <code>LAG</code> and <code>LEAD</code>:</p>
<pre>
SELECT time_id, TO_CHAR(SUM(amount_sold),'9,999,999') AS SALES, 
  TO_CHAR(LAG(SUM(amount_sold),1) OVER (ORDER BY time_id),'9,999,999') AS LAG1,
  TO_CHAR(LEAD(SUM(amount_sold),1) OVER (ORDER BY time_id),'9,999,999') AS LEAD1
FROM sales
WHERE time_id&gt;=TO_DATE('10-OCT-2000') AND time_id&lt;=TO_DATE('14-OCT-2000')
GROUP BY time_id;

TIME_ID   SALES      LAG1       LEAD1
--------- ---------- ---------- ----------
10-OCT-00    238,479                23,183
11-OCT-00     23,183    238,479     24,616
12-OCT-00     24,616     23,183     76,516
13-OCT-00     76,516     24,616     29,795
14-OCT-00     29,795     76,516
</pre></div>
<!-- class="example" -->
<p>See <a href="#i1014934">"Data Densification for Reporting"</a> for information showing how to use the <code>LAG</code>/<code>LEAD</code> functions for doing period-to-period comparison queries on sparse data.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8698"></a><a id="sthref1166"></a>Example 22-15 LAG/LEAD Using IGNORE NULLS</p>
<p>This example illustrates a typical case of using <code>LAG</code> and <code>LEAD</code> with the <code>IGNORE</code> <code>NULLS</code> option:</p>
<pre>
SELECT prod_id, channel_id, SUM(quantity_sold) quantity,
 CASE WHEN SUM(quantity_sold) &lt; 5000 THEN SUM(amount_sold) ELSE NULL END amount,
 LAG(CASE WHEN SUM(quantity_sold) &lt; 5000 THEN SUM(amount_sold) ELSE NULL END)
 IGNORE NULLS OVER (PARTITION BY prod_id ORDER BY channel_id) lag
FROM sales
WHERE prod_id IN (18,127,138)
GROUP BY prod_id, channel_id;

 PROD_ID  CHANNEL_ID  QUANTITY   AMOUNT          LAG
--------  ----------  --------   -----------     ----------
      18           2      2888    4420923.94
      18           3      5615                   4420923.94
      18           4      1088    1545729.81     4420923.94
     127           2      4508     274088.08  
     127           3      9626                    274088.08
     127           4      1850     173682.67      274088.08
     138           2      1120      127390.3       
     138           3      3878     393111.15       127390.3
     138           4       543      71203.21      393111.15

9 rows selected.
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="DWHSG8699"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1167"></a>
<h3 class="sect2">FIRST_VALUE, LAST_VALUE, and NTH_VALUE Functions</h3>
<p>This section illustrates the <code>FIRST_VALUE</code>, <code>LAST_VALUE</code>, and <code>NTH_VALUE</code> functions.</p>
<a id="DWHSG02016"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1168"></a>
<h4 class="sect3">FIRST_VALUE and LAST_VALUE Functions</h4>
<p>The <code>FIRST_VALUE</code> and <code>LAST_VALUE</code> <a id="sthref1169"></a><a id="sthref1170"></a><a id="sthref1171"></a><a id="sthref1172"></a>functions allow you to select the first and last rows from a window. These rows are especially valuable because they are often used as the baselines in calculations. For instance, with a partition holding sales data ordered by day, you might ask "How much was each day's sales compared to the first sales day (<code>FIRST_VALUE</code>) of the period?"</p>
<p>If the <code>IGNORE</code> <code>NULLS</code> option is used with <code>FIRST_VALUE</code>, it returns the first non-null value in the set, or <code>NULL</code> if all values are <code>NULL</code>. If <code>IGNORE</code> <code>NULLS</code> is used with <code>LAST_VALUE</code>, it returns the last non-null value in the set, or <code>NULL</code> if all values are <code>NULL</code>. The <code>IGNORE</code> <code>NULLS</code> option is particularly useful in populating an inventory table properly.</p>
<p>These functions have syntax as follows:</p>
<pre>
FIRST_VALUE|LAST_VALUE ( &lt;expr&gt; ) [RESPECT NULLS|IGNORE NULLS] OVER (analytic clause );
</pre>
<div class="example">
<p class="titleinexample"><a id="DWHSG8700"></a><a id="sthref1173"></a>Example 22-16 FIRST_VALUE</p>
<p>This example illustrates using the <code>IGNORE</code> <code>NULLS</code> option with <code>FIRST_VALUE</code>:</p>
<pre>
SELECT prod_id, channel_id, time_id,
  CASE WHEN MIN(amount_sold) &gt; 9.5 
    THEN MIN(amount_sold) ELSE NULL END amount_sold, 
  FIRST_VALUE(CASE WHEN MIN(amount_sold) &gt; 9.5 
    THEN min(amount_sold) ELSE NULL END) 
    IGNORE NULLS OVER (PARTITION BY prod_id 
                      ORDER BY channel_id DESC, time_id 
                      ROWS BETWEEN UNBOUNDED PRECEDING 
                      AND UNBOUNDED FOLLOWING) nv FROM sales 
                      WHERE prod_id = 115 AND time_id BETWEEN '18-DEC-01' 
                      AND '22-DEC-01' GROUP BY prod_id, channel_id, time_id 
                      ORDER BY prod_id;

  PROD_ID CHANNEL_ID TIME_ID    AMOUNT_SOLD         NV
---------- ---------- --------- ----------- ----------
       115          4 18-DEC-01                   9.66
       115          4 19-DEC-01                   9.66
       115          4 20-DEC-01                   9.66
       115          4 22-DEC-01                   9.66
       115          3 18-DEC-01        9.66       9.66
       115          3 19-DEC-01        9.66       9.66
       115          3 20-DEC-01        9.66       9.66
       115          3 21-DEC-01        9.66       9.66
       115          3 22-DEC-01        9.66       9.66
       115          2 18-DEC-01        9.67       9.66
       115          2 19-DEC-01        9.67       9.66
       115          2 21-DEC-01        9.67       9.66
       115          2 22-DEC-01        9.67       9.66
 
13 rows selected.
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" -->
<a id="BCFBHAAI"></a><a id="DWHSG02017"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">NTH_VALUE Function</h4>
<p>The <code>NTH_VALUE</code> function enabl<a id="sthref1174"></a><a id="sthref1175"></a>es you to find column values from an arbitrary row in the window. This could be used when, for example, you want to retrieve the 5th highest closing price for a company's shares during a year.</p>
<p>The <code>LAG</code> and <code>LEAD</code> functions can be thought of as being related to, and a simplification of, the <code>NTH_VALUE</code> function. With <code>LAG</code> and <code>LEAD</code>, you can only retrieve values from a row at the specified physical offset. If this is insufficient, you can use <code>NTH_VALUE</code>, which enables you to retrieve values from a row based on what is called a logical offset or relative position. You can use the <code>IGNORE</code> <code>NULLS</code> option with the <code>NTH_VALUE</code>, <code>FIRST_VALUE</code>, and <code>LAST_VALUE</code> functions to make it more powerful, in the sense that you can specify conditions and filter out rows based on certain conditions. See <a href="#BCFIBFFI">Example 22-17, "NTH_VALUE"</a>, where rows with quantities less than eight are filtered out. This cannot be done with <code>LAG</code> or <code>LEAD</code>, as you would not know the offset to the row.</p>
<p>See <a class="olink SQLRF" href="../e41084/toc.htm"><span class="italic">Oracle Database SQL Language Reference</span></a> for more information.</p>
<p>This function has syntax as follows:</p>
<pre>
NTH_VALUE (&lt;expr&gt;, &lt;n expr&gt;) [FROM FIRST | FROM LAST]
[RESPECT NULLS | IGNORE NULLS] OVER (&lt;window specification&gt;)
</pre>
<ul>
<li>
<p><code>expr</code> can be a column, constant, bind variable, or an expression involving them.</p>
</li>
<li>
<p><code>n</code> can be a column, constant, bind variable, or an expression involving them.</p>
</li>
<li>
<p><code>RESPECT</code> <code>NULLS</code> is the default NULL handling mechanism. It determines whether null values of <code>expr</code> are included in or eliminated from the calculation. The default is <code>RESPECT</code> <code>NULLS</code>.</p>
</li>
<li>
<p>The <code>FROM</code> <code>FIRST</code> and <code>FROM</code> <code>LAST</code> options determine whether the offset <code>n</code> is from the first or last row. The default is <code>FROM</code> <code>FIRST</code>.</p>
</li>
<li>
<p><code>IGNORE</code> <code>NULLS</code> enables you to skip NULLs in measure values.</p>
</li>
</ul>
<div class="example">
<p class="titleinexample"><a id="BCFIBFFI"></a><a id="DWHSG8701"></a>Example 22-17 NTH_VALUE</p>
<p>The following example returns the <code>amount_sold</code> value of the second <code>channel_id</code> in ascending order for each <code>prod_id</code> in the range between 10 and 20:</p>
<pre>
SELECT prod_id, channel_id, MIN(amount_sold), 
  NTH_VALUE(MIN(amount_sold), 2) OVER (PARTITION BY prod_id ORDER BY channel_id
  ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) NV 
FROM sales
WHERE prod_id BETWEEN 10 AND 20 GROUP BY prod_id, channel_id;

   PROD_ID CHANNEL_ID MIN(AMOUNT_SOLD)         NV
---------- ---------- ---------------- ----------
        13          2           907.34      906.2
        13          3            906.2      906.2
        13          4           842.21      906.2
        14          2          1015.94    1036.72
        14          3          1036.72    1036.72
        14          4           935.79    1036.72
        15          2           871.19     871.19
        15          3           871.19     871.19
        15          4           871.19     871.19
        16          2           266.84     266.84
        16          3           266.84     266.84
        16          4           266.84     266.84
        16          9            11.99     266.84
...
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="BCFIABEE"></a><a id="DWHSG02015"></a>
<div class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1">Advanced Aggregates for Analysis</h2>
<p>This section illustrates the following advanced analytic aggregate functions:</p>
<ul>
<li>
<p><a href="#BCFJDCJI">LISTAGG Function</a></p>
</li>
<li>
<p><a href="#BCFBCDEE">FIRST/LAST Functions</a></p>
</li>
<li>
<p><a href="#BCFGEIAB">Inverse Percentile</a></p>
</li>
<li>
<p><a href="#BCFBAFFB">Hypothetical Rank</a></p>
</li>
<li>
<p><a href="#BCFIIAGJ">Linear Regression</a></p>
</li>
<li>
<p><a href="#BCFBGEED">Statistical Aggregates</a></p>
</li>
<li>
<p><a href="#BCFDBHHI">User-Defined Aggregates</a></p>
</li>
</ul>
<a id="BCFJDCJI"></a><a id="DWHSG8702"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">LISTAGG Function</h3>
<p>The <code>LISTAGG</code> function<a id="sthref1176"></a><a id="sthref1177"></a> orders data within each group based on the <code>ORDER</code> <code>BY</code> clause and then concatenates the values of the measure column. Its syntax is as follows:</p>
<pre>
LISTAGG (&lt;expr&gt; [, &lt;delimiter&gt;) WITHIN GROUP (ORDER BY &lt;oby_expression_list&gt;)
</pre>
<p><code>expr</code> can be a column, constant, bind variable, or an expression involving them.</p>
<p><code>delimiter</code> can be a column, constant, bind variable, or an expression involving them.</p>
<p><code>oby_expression_list</code> can be a list of expressions with optional ordering options to sort in ascending or descending order (<code>ASC</code> or <code>DESC</code>), and to control the sort order of NULLs (<code>NULLS</code> <code>FIRST</code> or <code>NULLS</code> <code>LAST</code>). <code>ASCENDING</code> and <code>NULLS</code> <code>LAST</code> are the defaults.</p>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1178"></a>
<h4 class="sect3">LISTAGG as Aggregate</h4>
<p>You can use the <code>LISTAGG</code> function as an aggregate.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8703"></a><a id="sthref1179"></a>Example 22-18 LISTAGG as Aggregate</p>
<p>The following example illustrates using <code>LISTAGG</code> as an aggregate.</p>
<pre>
SELECT prod_id, LISTAGG(cust_first_name||' '||cust_last_name, '; ') 
  WITHIN GROUP (ORDER BY amount_sold DESC) cust_list
FROM sales, customers
WHERE sales.cust_id = customers.cust_id AND cust_gender = 'M' 
  AND cust_credit_limit = 15000 AND prod_id BETWEEN 15 AND 18 
  AND channel_id = 2 AND time_id &gt; '01-JAN-01'
GROUP BY prod_id;

PROD_ID  CUST_LIST
-------  -----------------------------------------------
     15  Hope Haarper; Roxanne Crocker; ... Mason Murray
     16  Manvil Austin; Bud Pinkston; ... Helga Nickols
     17  Opal Aaron; Thacher Rudder; ... Roxanne Crocker
     18  Boyd Lin; Bud Pinkston; ... Erik Ready
</pre></div>
<!-- class="example" -->
<p>The output has been modified for readability.</p>
</div>
<!-- class="sect3" -->
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1180"></a>
<h4 class="sect3">LISTAGG as Reporting Aggregate</h4>
<p>You can use the <code>LISTAGG</code> function as a reporting aggregate.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8704"></a><a id="sthref1181"></a>Example 22-19 LISTAGG as Reporting Aggregate</p>
<p>This example illustrates using <code>LISTAGG</code> as a reporting aggregate:</p>
<pre>
connect sh/sh
set lines 120 pages 20000
column list format A40

SELECT time_id, prod_id, MIN(amount_sold), LISTAGG(min(amount_sold),';') 
WITHIN GROUP (ORDER BY prod_id) OVER (PARTITION BY time_id) cust_list 
FROM sales WHERE time_id &gt; '20-DEC-01' AND prod_id BETWEEN 120 AND 125 
GROUP BY prod_id, time_id;

TIME_ID   PROD_ID   MIN(AMOUNT_SOLD)  CUST_LIST
-------   -------   ----------------  -----------
21-DEC-01     120            51.36    51.36;10.81
21-DEC-01     121            10.81    51.36;10.81
22-DEC-01     120            51.36    51.36;10.81;20.23;56.12;17.79;15.67
22-DEC-01     121            10.81    51.36;10.81;20.23;56.12;17.79;15.67
22-DEC-01     122            20.23    51.36;10.81;20.23;56.12;17.79;15.67
22-DEC-01     123            56.12    51.36;10.81;20.23;56.12;17.79;15.67
22-DEC-01     124            17.79    51.36;10.81;20.23;56.12;17.79;15.67
22-DEC-01     125            15.67    51.36;10.81;20.23;56.12;17.79;15.67
...
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFBCDEE"></a><a id="DWHSG0205"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">FIRST/LAST Functions</h3>
<p><a id="sthref1182"></a><a id="sthref1183"></a>The <code>FIRST/LAST</code> aggregate functions allow you to rank a data set and work with its top-ranked or bottom-ranked rows. After finding the top or bottom ranked rows, an aggregate function is applied to any desired column. That is, <code>FIRST</code>/<code>LAST</code> lets you rank on column A but return the result of an aggregate applied on the first-ranked or last-ranked rows of column B. This is valuable because it avoids the need for a self-join or subquery, thus improving performance. These functions' syntax begins with a regular aggregate function (<code>MIN</code>, <code>MAX</code>, <code>SUM</code>, <code>AVG</code>, <code>COUNT</code>, <code>VARIANCE</code>, <code>STDDEV</code>) that produces a single return value per group. To specify the ranking used, the <code>FIRST</code>/<code>LAST</code> functions add a new clause starting with the word <code>KEEP</code>.</p>
<p>These functions have the following syntax:</p>
<pre>
aggregate_function KEEP ( DENSE_RANK LAST ORDER BY
  expr [ DESC | ASC ] [NULLS { FIRST | LAST }]
  [, expr [ DESC | ASC ] [NULLS { FIRST | LAST }]]...)
[OVER query_partitioning_clause]
</pre>
<p>Note that the <code>ORDER</code> <code>BY</code> clause can take multiple expressions.</p>
<a id="DWHSG8705"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1184"></a>
<h4 class="sect3">FIRST/LAST As Regular Aggregates</h4>
<p>You can use the <code>FIRST</code>/<code>LAST</code> family of aggregates as regular aggregate functions.</p>
<div class="example">
<p class="titleinexample"><a id="i1007076"></a><a id="DWHSG8706"></a>Example 22-20 FIRST/LAST Example 1</p>
<p>The following query lets us compare minimum price and list price of our products. For each product subcategory within the Men's clothing category, it returns the following:</p>
</div>
<!-- class="example" -->
<ul>
<li>
<p>List price of the product with the lowest minimum price</p>
</li>
<li>
<p>Lowest minimum price</p>
</li>
<li>
<p>List price of the product with the highest minimum price</p>
</li>
<li>
<p>Highest minimum price</p>
</li>
</ul>
<pre>
SELECT prod_subcategory, MIN(prod_list_price) 
  KEEP (DENSE_RANK FIRST ORDER BY (prod_min_price)) AS LP_OF_LO_MINP,
MIN(prod_min_price) AS LO_MINP,
MAX(prod_list_price) KEEP (DENSE_RANK LAST ORDER BY (prod_min_price))
 AS LP_OF_HI_MINP,
MAX(prod_min_price) AS HI_MINP
FROM products WHERE prod_category='Electronics'
GROUP BY prod_subcategory;

PROD_SUBCATEGORY LP_OF_LO_MINP LO_MINP  LP_OF_HI_MINP  HI_MINP
---------------- ------------- -------  ------------- ----------
Game Consoles          299.99  299.99          299.99     299.99
Home Audio             499.99  499.99          599.99     599.99
Y Box Accessories        7.99    7.99           20.99      20.99
Y Box Games              7.99    7.99           29.99      29.99
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8707"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1185"></a>
<h4 class="sect3">FIRST/LAST As Reporting Aggregates</h4>
<p>You can also use the <code>FIRST</code>/<code>LAST</code> family of aggregates as reporting aggregate functions. An example is calculating which months had the greatest and least increase in head count throughout the year. The syntax for these functions is similar to the syntax for any other reporting aggregate.</p>
<p>Consider the example in <a href="#i1007076">Example 22-20</a> for <code>FIRST/LAST</code>. What if we wanted to find the list prices of individual products and compare them to the list prices of the products in their subcategory that had the highest and lowest minimum prices?</p>
<p>The following query lets us find that information for the Documentation subcategory by using <code>FIRST/LAST</code> as reporting aggregates.</p>
<div class="example">
<p class="titleinexample"><a id="i1007114"></a><a id="DWHSG8708"></a>Example 22-21 FIRST/LAST Example 2</p>
<pre>
SELECT prod_id, prod_list_price,
    MIN(prod_list_price) KEEP (DENSE_RANK FIRST ORDER BY (prod_min_price))
       OVER(PARTITION BY (prod_subcategory)) AS LP_OF_LO_MINP,
    MAX(prod_list_price) KEEP (DENSE_RANK LAST ORDER BY (prod_min_price))
       OVER(PARTITION BY (prod_subcategory)) AS LP_OF_HI_MINP
FROM products WHERE prod_subcategory = 'Documentation';

   PROD_ID PROD_LIST_PRICE LP_OF_LO_MINP LP_OF_HI_MINP
---------- --------------- ------------- -------------
        40           44.99         44.99         44.99
        41           44.99         44.99         44.99
        42           44.99         44.99         44.99
        43           44.99         44.99         44.99
        44           44.99         44.99         44.99
        45           44.99         44.99         44.99
</pre>
<p>Using the <code>FIRST</code> and <code>LAST</code> functions as reporting aggregates makes it easy to include the results in calculations such as "Salary as a percent of the highest salary."</p>
</div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFGEIAB"></a><a id="DWHSG0206"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Inverse Percentile</h3>
<p>Using the <code>CUME_DIST</code> function, you can find the cumulative distribution (percentile) of a set of values. However, the inverse operation (finding what value computes to a certain percentile) is neither easy to do nor efficiently computed. To overcome this difficulty, the <code>PERCENTILE_CONT</code> and <code>PERCENTILE_DISC</code> functions were introduced. These can be used both as window reporting functions as well as normal aggregate functions.</p>
<p>These functions need a sort specification and a parameter that takes a percentile value between 0 and 1. The sort specification is handled by using an <code>ORDER</code> <code>BY</code> clause with one expression. When used as a normal aggregate function, it returns a single value for each ordered set.</p>
<p><code>PERCENTILE_CONT</code>, which is a continuous function computed by interpolation, and <code>PERCENTILE_DISC</code>, which is a step function that assumes discrete values. Like other aggregates, <code>PERCENTILE_CONT</code> and <code>PERCENTILE_DISC</code> operate on a group of rows in a grouped query, but with the following differences:</p>
<ul>
<li>
<p>They require a parameter between 0 and 1 (inclusive). A parameter specified out of this range results in error. This parameter should be specified as an expression that evaluates to a constant.</p>
</li>
<li>
<p>They require a sort specification. This sort specification is an <code>ORDER</code> <code>BY</code> clause with a single expression. Multiple expressions are not allowed.</p>
</li>
</ul>
<a id="DWHSG8709"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1186"></a>
<h4 class="sect3">Normal Aggregate Syntax</h4>
<pre>
[PERCENTILE_CONT | PERCENTILE_DISC]( <span class="italic">constant expression</span> ) 
    WITHIN GROUP ( ORDER BY <span class="italic">single order by expression</span>
[ASC|DESC] [NULLS FIRST| NULLS LAST])
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8710"></a>
<div class="sect3"><a id="sthref1187"></a>
<h4 class="sect3">Inverse Percentile Example Basis</h4>
<p>We use the following query to return the 17 rows of data used in the examples of this section:</p>
<pre>
SELECT cust_id, cust_credit_limit, CUME_DIST() 
   OVER (ORDER BY cust_credit_limit) AS CUME_DIST
FROM customers WHERE cust_city='Marshal';

   CUST_ID CUST_CREDIT_LIMIT  CUME_DIST
---------- ----------------- ----------
     28344              1500 .173913043
      8962              1500 .173913043
     36651              1500 .173913043
     32497              1500 .173913043
     15192              3000 .347826087
    102077              3000 .347826087
    102343              3000 .347826087
      8270              3000 .347826087
     21380              5000  .52173913
     13808              5000  .52173913
    101784              5000  .52173913
     30420              5000  .52173913
     10346              7000 .652173913
     31112              7000 .652173913
     35266              7000 .652173913
      3424              9000 .739130435
    100977              9000 .739130435
    103066             10000 .782608696
     35225             11000 .956521739
     14459             11000 .956521739
     17268             11000 .956521739
    100421             11000 .956521739
     41496             15000          1
</pre>
<p><code>PERCENTILE_DISC</code>(<code>x</code>) is computed by scanning up the <code>CUME_DIST</code> values in each group till you find the first one greater than or equal to <code>x</code>, where <code>x</code> is the specified percentile value. For the example query where <code>PERCENTILE_DISC(0.5)</code>, the result is 5,000, as the following illustrates:</p>
<pre>
SELECT PERCENTILE_DISC(0.5) WITHIN GROUP
  (ORDER BY cust_credit_limit) AS perc_disc, PERCENTILE_CONT(0.5) WITHIN GROUP 
  (ORDER BY cust_credit_limit) AS perc_cont
 FROM customers WHERE cust_city='Marshal';

PERC_DISC   PERC_CONT
---------   ---------
     5000        5000
</pre>
<p>The result of <code>PERCENTILE_CONT</code> is computed by linear interpolation between rows after ordering them. To compute <code>PERCENTILE_CONT(x)</code>, we first compute the row number = <code>RN</code>= (1+x*(n-1)), where <span class="italic">n</span> is the number of rows in the group and <span class="italic">x</span> is the specified percentile value. The final result of the aggregate function is computed by linear interpolation between the values from rows at row numbers <code>CRN = CEIL(RN)</code> and <code>FRN = FLOOR(RN)</code>.</p>
<p>The final result is: <code>PERCENTILE_CONT(X)</code> = if (<code>CRN = FRN = RN</code>), then (value of expression from row at <code>RN</code>) else (<code>CRN - RN</code>) * (value of expression for row at <code>FRN</code>) + (<code>RN -FRN</code>) * (value of expression for row at <code>CRN</code>).</p>
<p>Consider the previous example query, where we compute <code>PERCENTILE_CONT(0.5)</code>. Here <span class="italic">n</span> is 17. The row number <code>RN</code> = (1 + 0.5*(n-1))= 9 for both groups. Putting this into the formula, (<code>FRN=CRN=9</code>), we return the value from row 9 as the result.</p>
<p>Another example is, if you want to compute <code>PERCENTILE_CONT</code><code>(0.66)</code>. The computed row number <code>RN</code>=(1 + 0.66*(<code>n</code>-1))= (1 + 0.66*16)= 11.67. <code>PERCENTILE_CONT</code>(0.66) = (12-11.67)*(value of row 11)+(11.67-11)*(value of row 12). These results are:</p>
<pre>
SELECT PERCENTILE_DISC(0.66) WITHIN GROUP 
  (ORDER BY cust_credit_limit) AS perc_disc, PERCENTILE_CONT(0.66) WITHIN GROUP 
  (ORDER BY cust_credit_limit) AS perc_cont
FROM customers WHERE cust_city='Marshal';

 PERC_DISC  PERC_CONT
---------- ----------
      9000       8040
</pre>
<p>Inverse percentile aggregate functions can appear in the <code>HAVING</code> clause of a query like other existing aggregate functions.</p>
</div>
<!-- class="sect3" -->
<a id="DWHSG8711"></a>
<div class="sect3"><a id="sthref1188"></a>
<h4 class="sect3">As Reporting Aggregates</h4>
<p>You can also use the aggregate functions <code>PERCENTILE_CONT</code>, <code>PERCENTILE_DISC</code> as reporting aggregate functions. When used as reporting aggregate functions, the syntax is similar to those of other reporting aggregates.</p>
<pre>
[PERCENTILE_CONT | PERCENTILE_DISC](<span class="italic">constant expression</span>) 
WITHIN GROUP ( ORDER BY <span class="italic">single order by expression</span>
[ASC|DESC] [NULLS FIRST| NULLS LAST])
OVER ( [PARTITION BY <span class="italic">value expression</span> [,...]] )
</pre>
<p>This query performs the same computation (median credit limit for customers in this result set), but reports the result for every row in the result set, as shown in the following output:</p>
<pre>
SELECT cust_id, cust_credit_limit, PERCENTILE_DISC(0.5) WITHIN GROUP
     (ORDER BY cust_credit_limit) OVER () AS perc_disc,
  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY cust_credit_limit) 
  OVER () AS perc_cont
FROM customers WHERE cust_city='Marshal';

   CUST_ID CUST_CREDIT_LIMIT  PERC_DISC  PERC_CONT
---------- ----------------- ---------- ----------
     28344              1500       5000       5000
      8962              1500       5000       5000
     36651              1500       5000       5000
     32497              1500       5000       5000
     15192              3000       5000       5000
    102077              3000       5000       5000
    102343              3000       5000       5000
      8270              3000       5000       5000
     21380              5000       5000       5000
     13808              5000       5000       5000
    101784              5000       5000       5000
     30420              5000       5000       5000
     10346              7000       5000       5000
     31112              7000       5000       5000
     35266              7000       5000       5000
      3424              9000       5000       5000
    100977              9000       5000       5000
    103066             10000       5000       5000
     35225             11000       5000       5000
     14459             11000       5000       5000
     17268             11000       5000       5000
    100421             11000       5000       5000
     41496             15000       5000       5000
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8712"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1189"></a>
<h4 class="sect3">Restrictions</h4>
<p>For <code>PERCENTILE_DISC</code>, the expression in the <code>ORDER</code> <code>BY</code> clause can be of any data type that you can sort (numeric, string, date, and so on). However, the expression in the <code>ORDER</code> <code>BY</code> clause must be a numeric or datetime type (including intervals) because linear interpolation is used to evaluate <code>PERCENTILE_CONT</code>. If the expression is of type <code>DATE</code>, the interpolated result is rounded to the smallest unit for the type. For a <code>DATE</code> type, the interpolated value is rounded to the nearest second, for interval types to the nearest second (<code>INTERVAL</code> <code>DAY</code> <code>TO</code> <code>SECOND</code>) or to the month (<code>INTERVAL</code> <code>YEAR</code> <code>TO</code> <code>MONTH</code>).</p>
<p>Like other aggregates, the inverse percentile functions ignore <code>NULLs</code> in evaluating the result. For example, when you want to find the median value in a set, Oracle Database ignores the <code>NULLs</code> and finds the median among the non-null values. You can use the <code>NULLS</code> <code>FIRST</code>/<code>NULLS</code> <code>LAST</code> option in the <code>ORDER</code> <code>BY</code> clause, but they will be ignored as <code>NULLs</code> are ignored.</p>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFBAFFB"></a><a id="DWHSG0207"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Hypothetical Rank</h3>
<p>These functions <a id="sthref1190"></a>provide functionality useful for what-if analysis. As an example, what would be the rank of a row, if the row was hypothetically inserted into a set of other rows?</p>
<p>This family of aggregates takes one or more arguments of a hypothetical row and an ordered group of rows, returning the <code>RANK</code>, <code>DENSE_RANK</code>, <code>PERCENT_RANK</code> or <code>CUME_DIST</code> of the row as if it was hypothetically inserted into the group.</p>
<pre>
[RANK | DENSE_RANK | PERCENT_RANK | CUME_DIST]( <span class="italic">constant expression</span> [, ...] )
WITHIN GROUP ( ORDER BY <span class="italic">order by expression</span> [ASC|DESC] [NULLS FIRST|NULLS LAST][, ...] )
</pre>
<p>Here, <code><span class="codeinlineitalic">constant expression</span></code> refers to an expression that evaluates to a constant, and there may be more than one such expressions that are passed as arguments to the function. The <code>ORDER</code> <code>BY</code> clause can contain one or more expressions that define the sorting order on which the ranking will be based. <code>ASC</code>, <code>DESC</code>, <code>NULLS</code> <code>FIRST</code>, <code>NULLS</code> <code>LAST</code> options will be available for each expression in the <code>ORDER</code> <code>BY</code>.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8713"></a><a id="sthref1191"></a>Example 22-22 Hypothetical Rank and Distribution Example 1</p>
<p>Using the list price data from the <code>products</code> table used throughout this section, you can calculate the <code>RANK</code>, <code>PERCENT_RANK</code> and <code>CUME_DIST</code> for a hypothetical sweater with a price of $50 for how it fits within each of the sweater subcategories. The query and results are:</p>
<pre>
SELECT cust_city,
  RANK(6000) WITHIN GROUP (ORDER BY CUST_CREDIT_LIMIT DESC) AS HRANK,
  TO_CHAR(PERCENT_RANK(6000) WITHIN GROUP
  (ORDER BY cust_credit_limit),'9.999') AS HPERC_RANK,
  TO_CHAR(CUME_DIST (6000) WITHIN GROUP
  (ORDER BY cust_credit_limit),'9.999') AS HCUME_DIST
FROM customers
WHERE cust_city LIKE 'Fo%'
GROUP BY cust_city;

CUST_CITY                           HRANK HPERC_ HCUME_
------------------------------ ---------- ------ ------
Fondettes                              13   .455   .478
Fords Prairie                          18   .320   .346
Forest City                            47   .370   .378
Forest Heights                         38   .456   .464
Forestville                            58   .412   .418
Forrestcity                            51   .438   .444
Fort Klamath                           59   .356   .363
Fort William                           30   .500   .508
Foxborough                             52   .414   .420
</pre>
<p>Unlike the inverse percentile aggregates, the <code>ORDER</code> <code>BY</code> clause in the sort specification for hypothetical rank and distribution functions may take multiple expressions. The number of arguments and the expressions in the <code>ORDER</code> <code>BY</code> clause should be the same and the arguments must be constant expressions of the same or compatible type to the corresponding <code>ORDER</code> <code>BY</code> expression. The following is an example using two arguments in several hypothetical ranking functions.</p>
</div>
<!-- class="example" -->
<div class="example">
<p class="titleinexample"><a id="DWHSG8714"></a><a id="sthref1192"></a>Example 22-23 Hypothetical Rank and Distribution Example 2</p>
<pre>
SELECT prod_subcategory,
  RANK(10,8) WITHIN GROUP (ORDER BY prod_list_price DESC,prod_min_price)
  AS HRANK, TO_CHAR(PERCENT_RANK(10,8) WITHIN GROUP
  (ORDER BY prod_list_price, prod_min_price),'9.999') AS HPERC_RANK,
  TO_CHAR(CUME_DIST (10,8) WITHIN GROUP
  (ORDER BY prod_list_price, prod_min_price),'9.999') AS HCUME_DIST
FROM products WHERE prod_subcategory LIKE 'Recordable%'
GROUP BY prod_subcategory;

PROD_SUBCATEGORY          HRANK   HPERC_   HCUME_
--------------------      -----   ------   ------
Recordable CDs                4     .571     .625
Recordable DVD Discs          5     .200     .333
</pre>
<p>These functions can appear in the <code>HAVING</code> clause of a query just like other aggregate functions. They cannot be used as either reporting aggregate functions or windowing aggregate functions.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect2" -->
<a id="BCFIIAGJ"></a><a id="DWHSG0208"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Linear Regression</h3>
<p><a id="sthref1193"></a><a id="sthref1194"></a>The regression functions support the fitting of an ordinary-least-squares regression line to a set of number pairs. You can use them as both aggregate functions or windowing or reporting functions.</p>
<p>The regression functions are as follows:</p>
<ul>
<li>
<p><a href="#BCFBFFHF">REGR_COUNT Function</a></p>
</li>
<li>
<p><a href="#BCFEFBHG">REGR_AVGY and REGR_AVGX Functions</a></p>
</li>
<li>
<p><a href="#BCFGGDCI">REGR_SLOPE and REGR_INTERCEPT Functions</a></p>
</li>
<li>
<p><a href="#BCFCIFFG">REGR_R2 Function</a></p>
</li>
<li>
<p><a href="#BCFCGDII">REGR_SXX, REGR_SYY, and REGR_SXY Functions</a></p>
</li>
</ul>
<p>Oracle applies the function to the set of (<code>e1</code>, <code>e2</code>) pairs after eliminating all pairs for which either of <code>e1</code> or <code>e2</code> is null. e1 is interpreted as a value of the dependent variable (a "y value"), and <code>e2</code> is interpreted as a value of the independent variable (an "x value"). Both expressions must be numbers.</p>
<p>The regression functions are all computed simultaneously during a single pass through the data. They are frequently combined with the <code>COVAR_POP</code>, <code>COVAR_SAMP</code>, and <code>CORR</code> functions.</p>
<a id="BCFBFFHF"></a><a id="DWHSG8715"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">REGR_COUNT Function</h4>
<p><code><a id="sthref1195"></a><a id="sthref1196"></a>REGR_COUNT</code> returns the number of non-null number pairs used to fit the regression line. If applied to an empty set (or if there are no (<code>e1</code>, <code>e2</code>) pairs where neither of <code>e1</code> or <code>e2</code> is null), the function returns 0.</p>
</div>
<!-- class="sect3" -->
<a id="BCFEFBHG"></a><a id="DWHSG8716"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">REGR_AVGY and REGR_AVGX Functions</h4>
<p><code><a id="sthref1197"></a><a id="sthref1198"></a><a id="sthref1199"></a><a id="sthref1200"></a>REGR_AVGY</code> and <code>REGR_AVGX</code> compute the averages of the dependent variable and the independent variable of the regression line, respectively. <code>REGR_AVGY</code> computes the average of its first argument (<code>e1</code>) after eliminating (<code>e1</code>, <code>e2</code>) pairs where either of <code>e1</code> or <code>e2</code> is null. Similarly, <code>REGR_AVGX</code> computes the average of its second argument (<code>e2</code>) after null elimination. Both functions return <code>NULL</code> if applied to an empty set.</p>
</div>
<!-- class="sect3" -->
<a id="BCFGGDCI"></a><a id="DWHSG8717"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">REGR_SLOPE and REGR_INTERCEPT Functions</h4>
<p><a id="sthref1201"></a><a id="sthref1202"></a><a id="sthref1203"></a><a id="sthref1204"></a>The <code>REGR_SLOPE</code> function computes the slope of the regression line fitted to non-null (<code>e1</code>, <code>e2</code>) pairs.</p>
<p>The <code>REGR_INTERCEPT</code> function computes the y-intercept of the regression line. <code>REGR_INTERCEPT</code> returns <code>NULL</code> whenever slope or the regression averages are <code>NULL</code>.</p>
</div>
<!-- class="sect3" -->
<a id="BCFCIFFG"></a><a id="DWHSG8718"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">REGR_R2 Function</h4>
<p><a id="sthref1205"></a>The <code>REGR_R2</code> function computes the coefficient of determination (usually called "R-squared" or "goodness of fit") for the regression line.</p>
<p><code>REGR_R2</code> returns values between 0 and 1 when the regression line is defined (slope of the line is not null), and it returns <code>NULL</code> otherwise. The closer the value is to 1, the better the regression line fits the data.</p>
</div>
<!-- class="sect3" -->
<a id="BCFCGDII"></a><a id="DWHSG8719"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3">REGR_SXX, REGR_SYY, and REGR_SXY Functions</h4>
<p><code><a id="sthref1206"></a><a id="sthref1207"></a><a id="sthref1208"></a><a id="sthref1209"></a><a id="sthref1210"></a><a id="sthref1211"></a>REGR_SXX</code>, <code>REGR_SYY</code> and <code>REGR_SXY</code> functions are used in computing various diagnostic statistics for regression analysis. After eliminating (<code>e1</code>, <code>e2</code>) pairs where either of <code>e1</code> or <code>e2</code> is null, these functions make the following computations:</p>
<pre>
REGR_SXX:    REGR_COUNT(e1,e2) * VAR_POP(e2)
REGR_SYY:    REGR_COUNT(e1,e2) * VAR_POP(e1) 
REGR_SXY:    REGR_COUNT(e1,e2) * COVAR_POP(e1, e2)
</pre></div>
<!-- class="sect3" -->
<a id="DWHSG8720"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1212"></a>
<h4 class="sect3">Linear Regression Statistics Examples</h4>
<p>Some common diagnostic statistics that accompany linear regression analysis are given in <a href="#g1016125">Table 22-2, "Common Diagnostic Statistics and Their Expressions&nbsp;"</a>. Note that this release's new functions allow you to calculate all of these.</p>
<div class="tblformalwidemax">
<p class="titleintable"><a id="DWHSG8721"></a><a id="sthref1213"></a><a id="g1016125"></a>Table 22-2 Common Diagnostic Statistics and Their Expressions&nbsp;</p>
<table class="cellalignment1383" title="Common Diagnostic Statistics and Their Expressions&nbsp;" summary="Common Diagnostic Statistics and Their Expressions" dir="ltr">
<thead>
<tr class="cellalignment1372">
<th class="cellalignment1382" id="r1c1-t4">Type of Statistic</th>
<th class="cellalignment1382" id="r1c2-t4">Expression</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r2c1-t4" headers="r1c1-t4">
<p>Adjusted R2</p>
</td>
<td class="cellalignment1378" headers="r2c1-t4 r1c2-t4">
<p><code>1-((1 - REGR_R2)*((REGR_COUNT-1)/(REGR_COUNT-2)))</code></p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r3c1-t4" headers="r1c1-t4">
<p>Standard error</p>
</td>
<td class="cellalignment1378" headers="r3c1-t4 r1c2-t4">
<p><code>SQRT((REGR_SYY-(POWER(REGR_SXY,2)/REGR_SXX))/(REGR_COUNT-2))</code></p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r4c1-t4" headers="r1c1-t4">
<p>Total sum of squares</p>
</td>
<td class="cellalignment1378" headers="r4c1-t4 r1c2-t4">
<p><code>REGR_SYY</code></p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r5c1-t4" headers="r1c1-t4">
<p>Regression sum of squares</p>
</td>
<td class="cellalignment1378" headers="r5c1-t4 r1c2-t4">
<p><code>POWER(REGR_SXY,2) / REGR_SXX</code></p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r6c1-t4" headers="r1c1-t4">
<p>Residual sum of squares</p>
</td>
<td class="cellalignment1378" headers="r6c1-t4 r1c2-t4">
<p><code>REGR_SYY</code> - (<code>POWER(REGR_SXY,2)/REGR_SXX)</code></p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r7c1-t4" headers="r1c1-t4">
<p>t statistic for slope</p>
</td>
<td class="cellalignment1378" headers="r7c1-t4 r1c2-t4">
<p><code>REGR_SLOPE * SQRT(REGR_SXX)</code> / (Standard error)</p>
</td>
</tr>
<tr class="cellalignment1372">
<td class="cellalignment1378" id="r8c1-t4" headers="r1c1-t4">
<p>t statistic for y-intercept</p>
</td>
<td class="cellalignment1378" headers="r8c1-t4 r1c2-t4">
<p><code>REGR_INTERCEPT</code> / ((Standard error) * <code>SQRT((1/REGR_COUNT)+(POWER(REGR_AVGX,2)/REGR_SXX))</code></p>
</td>
</tr>
</tbody>
</table>
<br /></div>
<!-- class="tblformalwidemax" --></div>
<!-- class="sect3" -->
<a id="DWHSG8722"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1214"></a>
<h4 class="sect3">Sample Linear Regression Calculation</h4>
<p>In this example, we compute an ordinary-least-squares regression line that expresses the quantity sold of a product as a linear function of the product's list price. The calculations are grouped by sales channel. The values <code>SLOPE</code>, <code>INTCPT</code>, <code>RSQR</code> are slope, intercept, and coefficient of determination of the regression line, respectively. The (integer) value <code>COUNT</code> is the number of products in each channel for whom both quantity sold and list price data are available.</p>
<pre>
SELECT s.channel_id, REGR_SLOPE(s.quantity_sold, p.prod_list_price) SLOPE,
  REGR_INTERCEPT(s.quantity_sold, p.prod_list_price) INTCPT,
  REGR_R2(s.quantity_sold, p.prod_list_price) RSQR,
  REGR_COUNT(s.quantity_sold, p.prod_list_price) COUNT,
  REGR_AVGX(s.quantity_sold, p.prod_list_price) AVGLISTP,
  REGR_AVGY(s.quantity_sold, p.prod_list_price) AVGQSOLD
FROM sales s, products p WHERE s.prod_id=p.prod_id
  AND p.prod_category='Electronics' AND s.time_id=to_DATE('10-OCT-2000')
GROUP BY s.channel_id;

CHANNEL_ID      SLOPE     INTCPT       RSQR      COUNT   AVGLISTP   AVGQSOLD
---------- ---------- ---------- ---------- ---------- ---------- ----------
         2          0          1          1         39 466.656667          1
         3          0          1          1         60     459.99          1
         4          0          1          1         19 526.305789          1
</pre></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFBGEED"></a><a id="DWHSG8723"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Statistical Aggregates</h3>
<p>Oracle provides a set of SQL statistical functions and a statistics package, <code>DBMS_STAT_FUNCS</code>. This section lists some of the new functions along with basic syntax.</p>
<p>See <a class="olink ARPLS006" href="../../appdev.112/e40758/d_stat_f.htm#ARPLS006"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for detailed information about the <code>DBMS_STAT_FUNCS</code> package and <a class="olink SQLRF00647" href="../../server.112/e41084/functions071.htm#SQLRF00647"><span class="italic">Oracle Database SQL Language Reference</span></a> for syntax and semantics.</p>
<a id="DWHSG8724"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1215"></a>
<h4 class="sect3">Descriptive Statistics</h4>
<p>You can calculate the following descriptive statistics:</p>
<ul>
<li>
<p>Median of a Data Set</p>
<pre>
Median (expr) [OVER (query_partition_clause)]
</pre></li>
</ul>
<pre>
</pre>
<ul>
<li>
<p>Mode of a Data Set</p>
<pre>
STATS_MODE (expr)
</pre></li>
</ul>
</div>
<!-- class="sect3" -->
<a id="DWHSG8725"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1216"></a>
<h4 class="sect3">Hypothesis Testing - Parametric Tests</h4>
<p>You can calculate the following descriptive statistics:</p>
<ul>
<li>
<p>One-Sample T-Test</p>
<pre>
STATS_T_TEST_ONE (expr1, expr2 (a constant) [, return_value])
</pre></li>
<li>
<p>Paired-Samples T-Test</p>
<pre>
STATS_T_TEST_PAIRED (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>Independent-Samples T-Test. Pooled Variances</p>
<pre>
STATS_T_TEST_INDEP (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>Independent-Samples T-Test, Unpooled Variances</p>
<pre>
STATS_T_TEST_INDEPU (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>The F-Test</p>
<pre>
STATS_F_TEST (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>One-Way ANOVA</p>
<pre>
STATS_ONE_WAY_ANOVA (expr1, expr2 [, return_value])
</pre></li>
</ul>
</div>
<!-- class="sect3" -->
<a id="DWHSG8726"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1217"></a>
<h4 class="sect3">Crosstab Statistics</h4>
<p>You can calculate crosstab statistics using the following syntax:</p>
<pre>
STATS_CROSSTAB (expr1, expr2 [, return_value])
</pre>
<p>Can return any one of the following:</p>
<ul>
<li>
<p>Observed value of chi-squared</p>
</li>
<li>
<p>Significance of observed chi-squared</p>
</li>
<li>
<p>Degree of freedom for chi-squared</p>
</li>
<li>
<p>Phi coefficient, Cramer's V statistic</p>
</li>
<li>
<p>Contingency coefficient</p>
</li>
<li>
<p>Cohen's Kappa</p>
</li>
</ul>
</div>
<!-- class="sect3" -->
<a id="DWHSG8727"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1218"></a>
<h4 class="sect3">Hypothesis Testing - Non-Parametric Tests</h4>
<p>You can calculate hypothesis statistics using the following syntax:</p>
<pre>
STATS_BINOMIAL_TEST (expr1, expr2, p [, return_value])
</pre>
<ul>
<li>
<p>Binomial Test/Wilcoxon Signed Ranks Test</p>
<pre>
STATS_WSR_TEST (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>Mann-Whitney Test</p>
<pre>
STATS_MW_TEST (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>Kolmogorov-Smirnov Test</p>
<pre>
STATS_KS_TEST (expr1, expr2 [, return_value])
</pre></li>
</ul>
</div>
<!-- class="sect3" -->
<a id="DWHSG8728"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1219"></a>
<h4 class="sect3">Non-Parametric Correlation</h4>
<p>You can calculate the following parametric statistics:</p>
<ul>
<li>
<p>Spearman's rho Coefficient</p>
<pre>
CORR_S (expr1, expr2 [, return_value])
</pre></li>
<li>
<p>Kendall's tau-b Coefficient</p>
<pre>
CORR_K (expr1, expr2 [, return_value])
</pre></li>
</ul>
<p>In addition to the functions, this release has a PL/SQL package, <code>DBMS_STAT_FUNCS</code>. It contains the descriptive statistical function <code>SUMMARY</code> along with functions to support distribution fitting. The <code>SUMMARY</code> function summarizes a numerical column of a table with a variety of descriptive statistics. The five distribution fitting functions support normal, uniform, Weibull, Poisson, and exponential distributions.</p>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFDBHHI"></a><a id="DWHSG02011"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">User-Defined Aggregates</h3>
<p>Oracle offers a facility for creating your own functions, called user-defined aggregate functions. These functions are written in programming languages such as PL/SQL, Java, and C, and can be used as analytic functions or aggregates in materialized views. See <a class="olink ADDCI" href="../../appdev.112/e10765/toc.htm"><span class="italic">Oracle Database Data Cartridge Developer's Guide</span></a> for further information regarding syntax and restrictions.</p>
<p>The advantages of these functions are:</p>
<ul>
<li>
<p>Highly complex functions can be programmed using a fully procedural language.</p>
</li>
<li>
<p>Higher scalability than other techniques when user-defined functions are programmed for parallel processing.</p>
</li>
<li>
<p>Object data types can be processed.</p>
</li>
</ul>
<p>As a simple example of a user-defined aggregate function, consider the skew statistic. This calculation measures if a data set has a lopsided distribution about its mean. It will tell you if one tail of the distribution is significantly larger than the other. If you created a user-defined aggregate called <code>udskew</code> and applied it to the credit limit data in the prior example, the SQL statement and results might look like this:</p>
<pre>
SELECT USERDEF_SKEW(cust_credit_limit) FROM customers
WHERE cust_city='Marshal';

USERDEF_SKEW
============
0.583891
</pre>
<p>Before building user-defined aggregate functions, you should consider if your needs can be met in regular SQL. Many complex calculations are possible directly in SQL, particularly by using the <code>CASE</code> expression.</p>
<p>Staying with regular SQL will enable simpler development, and many query operations are already well-parallelized in SQL. Even the earlier example, the skew statistic, can be created using standard, albeit lengthy, SQL.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="BCFHHHHF"></a><a id="DWHSG0209"></a>
<div class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1">Pivoting Operations</h2>
<p>The d<a id="sthref1220"></a><a id="sthref1221"></a>ata returned by business intelligence queries is often most usable if presented in a crosstabular format. The <code>pivot_clause</code> of the <code>SELECT</code> statement lets you write crosstabulation queries that rotate rows into columns, aggregating data in the process of the rotation. Pivoting is a key technique in data warehouses. In it, you transform multiple rows of input into fewer and generally wider rows in the data warehouse. When pivoting, an aggregation operator is applied for each item in the pivot column value list. The pivot column cannot contain an arbitrary expression. If you need to pivot on an expression, then you should alias the expression in a view before the <code>PIVOT</code> operation. The basic syntax is as follows:</p>
<pre>
SELECT ....
FROM &lt;table-expr&gt;
   PIVOT
     (
      aggregate-function(&lt;column&gt;)
      FOR &lt;pivot-column&gt; IN (&lt;value1&gt;, &lt;value2&gt;,..., &lt;valuen&gt;)
        ) AS &lt;alias&gt;
WHERE .....
</pre>
<p>See <a class="olink SQLRF01702" href="../../server.112/e41084/statements_10002.htm#SQLRF01702"><span class="italic">Oracle Database SQL Language Reference</span></a> for <code>pivot_clause</code> syntax.</p>
<p>To illustrate the use of pivoting, create the following view as a basis for later examples:</p>
<pre>
CREATE VIEW sales_view AS
SELECT
 prod_name product, country_name country, channel_id channel,
 SUBSTR(calendar_quarter_desc, 6,2) quarter,
 SUM(amount_sold) amount_sold, SUM(quantity_sold) quantity_sold 
FROM sales, times, customers, countries, products
WHERE sales.time_id = times.time_id AND
  sales.prod_id = products.prod_id AND
  sales.cust_id = customers.cust_id AND
  customers.country_id = countries.country_id
GROUP BY prod_name, country_name, channel_id,
 SUBSTR(calendar_quarter_desc, 6, 2);
</pre>
<a id="DWHSG8729"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1222"></a>
<h3 class="sect2">Example: Pivoting</h3>
<p>The following statement illustrates a typical pivot on the <code>channel</code> column:</p>
<pre>
SELECT * FROM
  (SELECT product, channel, amount_sold
   FROM sales_view
   ) S PIVOT (SUM(amount_sold)
   FOR CHANNEL IN (3 AS DIRECT_SALES, 4 AS INTERNET_SALES, 
                   5 AS CATALOG_SALES, 9 AS TELESALES))
ORDER BY product;

PRODUCT                  DIRECT&shy;_SALES  INTERNET_SALES  CATALOG_SALES  TELESALES
----------------------   ------------  --------------  -------------  ---------
...
Internal 6X CD-ROM          229512.97        26249.55
Internal 8X CD-ROM          286291.49        42809.44
Keyboard Wrist Rest         200959.84        38695.36                   1522.73
... 
</pre>
<p>Note that the output has created four new aliased columns, <code>DIRECT_SALES</code>, <code>INTERNET_SALES</code>, <code>CATALOG_SALES</code>, and <code>TELESALES</code>, one for each of the pivot values. The output is a sum. If no alias is provided, the column heading will be the values of the <code>IN</code>-list.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8730"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1223"></a>
<h3 class="sect2">Pivoting on Multiple Columns</h3>
<p>You can pivot on more than one column. The following statement illustrates a typical multiple column pivot:</p>
<pre>
SELECT *
FROM
     (SELECT product, channel, quarter, quantity_sold
      FROM sales_view
     ) PIVOT (SUM(quantity_sold)
                FOR (channel, quarter) IN
                  ((5, '02') AS CATALOG_Q2,
                   (4, '01') AS INTERNET_Q1,
                   (4, '04') AS INTERNET_Q4,
                   (2, '02') AS PARTNERS_Q2,
                   (9, '03') AS TELE_Q3
                  )
                );

PRODUCT              CATALOG_Q2  INTERNET_Q1  INTERNET_Q4  PARTNERS_Q2   TELE_Q3
-------              ----------  -----------  -----------  -----------   -------
...
Bounce                                  347           632          954
...      
Smash Up Boxing                         129           280          560
...  
Comic Book Heroes                        47           155          275
...
</pre>
<p>Note that this example specifies a multi-column <code>IN</code>-list with column headings designed to match the <code>IN</code>-list members.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8731"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1224"></a>
<h3 class="sect2">Pivoting: Multiple Aggregates</h3>
<p>You can pivot with multiple aggregates, as shown in the following example:</p>
<pre>
SELECT *
FROM
     (SELECT product, channel, amount_sold, quantity_sold
      FROM sales_view
     ) PIVOT (SUM(amount_sold) AS sums,
              SUM(quantity_sold) AS sumq
              FOR channel IN (5, 4, 2, 9)
               )
ORDER BY product;
</pre>
<pre>
PRODUCT                5_SUMS  5_SUMQ    4_SUMS   4_SUMQ      2_SUMS   2_SUMQ    9_SUMS   9_SUMQ
-------------          ------  ------    ------   ------      ------   ------    ------   ------
O/S Doc Set English                   142780.36     3081   381397.99     8044   6028.66      134
O/S Doc Set French                     55503.58     1192   132000.77     2782   
... 
 
</pre>
<p>Note that the query creates column headings by concatenating the pivot values (or alias) with the alias of the aggregate function, plus an underscore.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8732"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1225"></a>
<h3 class="sect2">Distinguishing PIVOT-Generated Nulls from Nulls in Source Data</h3>
<p>You can distinguish between null values that are generated from the use of <code>PIVOT</code> and those that exist in the source data. The following example illustrates nulls that <code>PIVOT</code> generates.</p>
<p>The following query returns rows with 5 columns, column <code>prod_id</code>, and pivot resulting columns <code>Q1</code>, <code>Q1_COUNT_TOTAL</code>, <code>Q2</code>, <code>Q2_COUNT_TOTAL</code>. For each unique value of <code>prod_id</code>, <code>Q1_COUNT_TOTAL</code> returns the total number of rows whose <code>qtr</code> value is <code>Q1</code>, that is, and <code>Q2_COUNT_TOTAL</code> returns the total number of rows whose <code>qtr</code> value is <code>Q2</code>.</p>
<p>Assume we have a table <code>sales2</code> of the following structure:</p>
<pre>
PROD_ID   QTR  AMOUNT_SOLD
-------   ---  -----------
100        Q1           10
100        Q1           20
100        Q2         NULL
200        Q1           50

SELECT *
FROM sales2
        PIVOT
         ( SUM(amount_sold), COUNT(*) AS count_total
           FOR qtr IN ('Q1', 'Q2')
         );

PROD_ID    "Q1"   "Q1_COUNT_TOTAL"  "Q2"       "Q2_COUNT_TOTAL"
-------    ----   ----------------  ---------  ----------------
    100      20                  2   NULL &lt;1&gt;                 1
    200      50                  1   NULL &lt;2&gt;                 0
</pre>
<p>From the result, we know that for <code>prod_id</code> 100, there are 2 sales rows for quarter <code>Q1</code>, and 1 sales row for quarter <code>Q2</code>; for <code>prod_id</code> 200, there is 1 sales row for quarter <code>Q1</code>, and no sales row for quarter <code>Q2</code>.So, in <code>Q2_COUNT_TOTAL</code>, you can identify that <code>NULL&lt;1&gt;</code> comes from a row in the original table whose measure is of null value, while <code>NULL&lt;2&gt;</code> is due to no row being present in the original table for <code>prod_id</code> 200 in quarter <code>Q2</code>.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8733"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1226"></a>
<h3 class="sect2">Unpivoting Operations</h3>
<p>An unpivot does not reverse a <code>PIVOT</code> operation. Instead, it rotates data from columns into rows. If you are working with pivoted data, an <code>UNPIVOT</code> operation cannot reverse any aggregations that have been made by <code>PIVOT</code> or any other means.</p>
<p>To illustrate unpivoting, first create a pivoted table that includes four columns, for quarters of the year:</p>
<pre>
CREATE TABLE pivotedTable AS
SELECT *
FROM (SELECT product, quarter, quantity_sold, amount_sold
      FROM sales_view)
    PIVOT
    (
       SUM(quantity_sold) AS sumq, SUM(amount_sold) AS suma
       FOR quarter IN ('01' AS Q1, '02' AS Q2, '03' AS Q3, '04' AS Q4));
</pre>
<p>The table's contents resemble the following:</p>
<pre>
SELECT *
FROM pivotedTable
ORDER BY product;

PRODUCT         Q1_SUMQ  Q1_SUMA  Q2_SUMQ  Q2_SUMA   Q3_SUMQ  Q3_SUMA   Q4_SUMQ    Q4_SUMA
--------------- -------  -------  -------  --------  -------  --------  -------    ---------
1.44MB External   6098   58301.33    5112   49001.56    6050   56974.3     5848     55341.28
128MB Memory      1963  110763.63    2361  132123.12    3069  170710.4     2832    157736.6
17" LCD           1492 1812786.94    1387 1672389.06    1591 1859987.66    1540   1844008.11
...
</pre>
<p>The following <code>UNPIVOT</code> operation rotates the quarter columns into rows. For each product, there will be four rows, one for each quarter.</p>
<pre>
SELECT product, DECODE(quarter, 'Q1_SUMQ', 'Q1', 'Q2_SUMQ', 'Q2', 'Q3_SUMQ', 'Q3',
   'Q4_SUMQ', 'Q4') AS quarter, quantity_sold
FROM  pivotedTable
   UNPIVOT INCLUDE NULLS
       (quantity_sold
        FOR quarter IN (Q1_SUMQ, Q2_SUMQ, Q3_SUMQ, Q4_SUMQ))
ORDER BY product, quarter;

PRODUCT                          QU    QUANTITY_SOLD
-------                          --    -------------
1.44MB External 3.5" Diskette    Q1             6098
1.44MB External 3.5" Diskette    Q2             5112
1.44MB External 3.5" Diskette    Q3             6050
1.44MB External 3.5" Diskette    Q4             5848
128MB Memory Card                Q1             1963
128MB Memory Card                Q2             2361
128MB Memory Card                Q3             3069
128MB Memory Card                Q4             2832
...
</pre>
<p>Note the use of <code>INCLUDE</code> <code>NULLS</code> in this example. You can also use <code>EXCLUDE</code> <code>NULLS</code>, which is the default setting.</p>
<p>In addition, you can also unpivot using two columns, as in the following:</p>
<pre>
SELECT product, quarter, quantity_sold, amount_sold
FROM  pivotedTable
   UNPIVOT INCLUDE NULLS
       (
        (quantity_sold, amount_sold)
        FOR quarter IN ((Q1_SUMQ, Q1_SUMA) AS 'Q1', (Q2_SUMQ, Q2_SUMA) AS 'Q2', (Q3_SUMQ, Q3_SUMA) AS 'Q3', (Q4_SUMQ, Q4_SUMA) AS 'Q4'))
ORDER BY product, quarter;
 
PRODUCT                          QU   QUANTITY_SOLD   AMOUNT_SOLD
-----------------------------    --   -------------   ------------
1.44MB External 3.5" Diskette    Q1            6098       58301.33
1.44MB External 3.5" Diskette    Q2            5112       49001.56
1.44MB External 3.5" Diskette    Q3            6050       56974.3
1.44MB External 3.5" Diskette    Q4            5848       55341.28
128MB Memory Card                Q1            1963      110763.63
128MB Memory Card                Q2            2361      132123.12
128MB Memory Card                Q3            3069      170710.4
128MB Memory Card                Q4            2832      157736.6
</pre></div>
<!-- class="sect2" -->
<a id="DWHSG8734"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1227"></a>
<h3 class="sect2">Wildcard and Subquery Pivoting with XML Operations</h3>
<p>If you want to use a wildcard argument or subquery in your pivoting columns, you can do so with <code>PIVOT</code> XML syntax. With <code>PIVOT</code> XML, the output of the operation is properly formatted XML.</p>
<p>The following example illustrates using the wildcard keyword, <code>ANY</code>. It outputs XML that includes all channel values in <code>sales_view</code>:</p>
<pre>
SELECT *
FROM
   (SELECT product, channel, quantity_sold
    FROM sales_view
    ) PIVOT XML(SUM(quantity_sold)
                FOR channel IN (ANY)
               );
</pre>
<p>Note that the keyword <code>ANY</code> is available in <code>PIVOT</code> operations only as part of an XML operation. This output includes data for cases where the channel exists in the data set. Also note that aggregation functions must specify a <code>GROUP</code> <code>BY</code> clause to return multiple values, yet the <code>pivot_clause</code> does not contain an explicit <code>GROUP</code> <code>BY</code> clause. Instead, the <code>pivot_clause</code> performs an implicit <code>GROUP</code> <code>BY</code>.</p>
<p>The following example illustrates using a subquery. It outputs XML that includes all channel values and the sales data corresponding to each channel:</p>
<pre>
SELECT *
FROM
     (SELECT product, channel, quantity_sold
      FROM sales_view
     ) PIVOT XML(SUM(quantity_sold)
                FOR channel IN (SELECT DISTINCT channel_id FROM CHANNELS)
             );
</pre>
<p>The output densifies the data to include all possible channels for each product.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="i1014934"></a><a id="DWHSG02013"></a>
<div class="sect1">
<h2 class="sect1">Data Densification for Reporting</h2>
<p>Data is <a id="sthref1228"></a><a id="sthref1229"></a><a id="sthref1230"></a>normally stored in sparse form. That is, if no value exists for a given combination of dimension values, no row exists in the fact table. However, you may want to view the data in dense form, with rows for all combination of dimension values displayed even when no fact data exist for them. For example, if a product did not sell during a particular time period, you may still want to see the product for that time period with zero sales value next to it. Moreover, time series calculations can be performed most easily when data is dense along the time dimension. This is because dense data will fill a consistent number of rows for each period, which in turn makes it simple to use the analytic windowing functions with physical offsets. Data densification is the process of converting sparse data into dense form.To overcome the problem of sparsity, you can use a partitioned outer join to fill the gaps in a time series or any other dimension. Such a join extends the conventional outer join syntax by applying the outer join to each logical partition defined in a query. Oracle logically partitions the rows in your query based on the expression you specify in the <code>PARTITION</code> <code>BY</code> clause. The result of a partitioned outer join is a <code>UNION</code> of the outer joins of each of the partitions in the logically partitioned table with the table on the other side of the join.</p>
<p>Note that you can use this type of join to fill the gaps in any dimension, not just the time dimension. Most of the examples here focus on the time dimension because it is the dimension most frequently used as a basis for comparisons.</p>
<a id="DWHSG8735"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1231"></a>
<h3 class="sect2">Partition Join Syntax</h3>
<p>The syntax for partitioned outer join extends the ANSI SQL <code>JOIN</code> clause with the phrase <code>PARTITION</code> <code>BY</code> followed by an expression list. The expressions in the list specify the group to which the outer join is applied. The following are the two forms of syntax normally used for partitioned outer join:</p>
<pre>
SELECT .....
FROM table_reference
PARTITION BY (expr [, expr ]... )
RIGHT OUTER JOIN table_reference

SELECT .....
FROM table_reference
LEFT OUTER JOIN table_reference
PARTITION BY {expr [,expr ]...)
</pre>
<p>Note that <code>FULL</code> <code>OUTER</code> <code>JOIN</code> is not supported with a partitioned outer join.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8736"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1232"></a>
<h3 class="sect2">Sample of Sparse Data</h3>
<p>A typi<a id="sthref1233"></a><a id="sthref1234"></a>cal situation with a sparse dimension is shown in the following example, which computes the weekly sales and year-to-date sales for the product Bounce for weeks 20-30 in 2000 and 2001:</p>
<pre>
SELECT SUBSTR(p.Prod_Name,1,15) Product_Name, t.Calendar_Year Year,
 t.Calendar_Week_Number Week, SUM(Amount_Sold) Sales
FROM Sales s, Times t, Products p
WHERE s.Time_id = t.Time_id AND s.Prod_id = p.Prod_id AND
 p.Prod_name IN ('Bounce') AND t.Calendar_Year IN (2000,2001) AND
 t.Calendar_Week_Number BETWEEN 20 AND 30
GROUP BY p.Prod_Name, t.Calendar_Year, t.Calendar_Week_Number;

PRODUCT_NAME          YEAR       WEEK      SALES
--------------- ---------- ---------- ----------
Bounce                2000         20        801
Bounce                2000         21    4062.24
Bounce                2000         22    2043.16
Bounce                2000         23    2731.14
Bounce                2000         24    4419.36
Bounce                2000         27    2297.29
Bounce                2000         28    1443.13
Bounce                2000         29    1927.38
Bounce                2000         30    1927.38
Bounce                2001         20    1483.3
Bounce                2001         21    4184.49
Bounce                2001         22    2609.19
Bounce                2001         23    1416.95
Bounce                2001         24    3149.62
Bounce                2001         25    2645.98
Bounce                2001         27    2125.12
Bounce                2001         29    2467.92
Bounce                2001         30    2620.17
</pre>
<p>In this example, we would expect 22 rows of data (11 weeks each from 2 years) if the data were dense. However, we get only 18 rows because weeks 25 and 26 are missing in 2000, and weeks 26 and 28 in 2001.</p>
</div>
<!-- class="sect2" -->
<a id="CIHIGEDA"></a><a id="DWHSG8737"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Filling Gaps in Data</h3>
<p>We can take the sparse data of the preceding query and do a partitioned outer join with a dense set of time data. In the following query, we alias our original query as <code>v</code> and we select data from the <code>times</code> table, which we alias as <code>t</code>. Here we retrieve 22 rows because there are no gaps in the series. The four added rows each have 0 as their Sales value set to 0 by using the <code>NVL</code> function.</p>
<pre>
SELECT Product_Name, t.Year, t.Week, NVL(Sales,0) dense_sales
FROM
 (SELECT SUBSTR(p.Prod_Name,1,15) Product_Name,
  t.Calendar_Year Year, t.Calendar_Week_Number Week, SUM(Amount_Sold) Sales
  FROM Sales s, Times t, Products p
  WHERE s.Time_id = t.Time_id AND s.Prod_id = p.Prod_id AND
   p.Prod_name IN ('Bounce') AND t.Calendar_Year IN (2000,2001) AND
   t.Calendar_Week_Number BETWEEN 20 AND 30
  GROUP BY p.Prod_Name, t.Calendar_Year, t.Calendar_Week_Number) v
PARTITION BY (v.Product_Name)
RIGHT OUTER JOIN
 (SELECT DISTINCT Calendar_Week_Number Week, Calendar_Year Year
  FROM Times
  WHERE Calendar_Year IN (2000, 2001)
  AND Calendar_Week_Number BETWEEN 20 AND 30) t
ON (v.week = t.week AND v.Year = t.Year)
ORDER BY t.year, t.week;
</pre>
<pre>
PRODUCT_NAME          YEAR       WEEK DENSE_SALES
--------------- ---------- ---------- -----------
Bounce                2000         20         801
Bounce                2000         21     4062.24
Bounce                2000         22     2043.16
Bounce                2000         23     2731.14
Bounce                2000         24     4419.36
Bounce                2000         25           0
Bounce                2000         26           0
Bounce                2000         27     2297.29
Bounce                2000         28     1443.13
Bounce                2000         29     1927.38
Bounce                2000         30     1927.38
Bounce                2001         20      1483.3
Bounce                2001         21     4184.49
Bounce                2001         22     2609.19
Bounce                2001         23     1416.95
Bounce                2001         24     3149.62
Bounce                2001         25     2645.98
Bounce                2001         26           0
Bounce                2001         27     2125.12
Bounce                2001         28           0
Bounce                2001         29     2467.92
Bounce                2001         30     2620.17
</pre>
<p>Note that in this query, a <code>WHERE</code> condition was placed for weeks between 20 and 30 in the inline view for the time dimension. This was introduced to keep the result set small.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8738"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1235"></a>
<h3 class="sect2">Filling Gaps in Two Dimensions</h3>
<p>N-dimensional data is typically displayed as a dense 2-dimensional cross tab of (n - 2) page dimensions. This requires that all dimension values for the two dimensions appearing in the cross tab be filled in. The following is another example where the partitioned outer join capability can be used for filling the gaps on two dimensions:</p>
<pre>
WITH v1 AS
 (SELECT p.prod_id, country_id, calendar_year,
    SUM(quantity_sold) units, SUM(amount_sold) sales
  FROM sales s, products p, customers c, times t
  WHERE s.prod_id in (147, 148) AND t.time_id = s.time_id AND
    c.cust_id = s.cust_id AND p.prod_id = s.prod_id
  GROUP BY p.prod_id, country_id, calendar_year),
v2 AS                                  --countries to use for densifications
  (SELECT DISTINCT country_id 
  FROM customers
  WHERE country_id IN (52782, 52785, 52786, 52787, 52788)),
v3 AS                                   --years to use for densifications
  (SELECT DISTINCT calendar_year FROM times)
SELECT v4.prod_id, v4.country_id, v3.calendar_year, units, sales
FROM 
  (SELECT prod_id, v2.country_id, calendar_year, units, sales
   FROM v1 PARTITION BY (prod_id) 
   RIGHT OUTER JOIN v2                  --densifies on country
   ON (v1.country_id = v2.country_id)) v4 
PARTITION BY (prod_id,country_id)
RIGHT OUTER JOIN v3                     --densifies on year
ON (v4.calendar_year = v3.calendar_year)
ORDER BY 1, 2, 3;
</pre>
<p>In this query, the <code>WITH</code> subquery factoring clause <code>v1</code> summarizes sales data at the product, country, and year level. This result is sparse but users may want to see all the country, year combinations for each product. To achieve this, we take each partition of <code>v1</code> based on product values and outer join it on the country dimension first. This will give us all values of country for each product. We then take that result and partition it on product and country values and then outer join it on time dimension. This will give us all time values for each product and country combination.</p>
<pre>
   PROD_ID COUNTRY_ID CALENDAR_YEAR      UNITS      SALES
---------- ---------- ------------- ---------- ----------
       147      52782          1998
       147      52782          1999         29     209.82
       147      52782          2000         71     594.36
       147      52782          2001        345    2754.42
       147      52782          2002
       147      52785          1998          1       7.99
       147      52785          1999
       147      52785          2000
       147      52785          2001
       147      52785          2002
       147      52786          1998          1       7.99
       147      52786          1999
       147      52786          2000          2      15.98
       147      52786          2001
       147      52786          2002
       147      52787          1998
       147      52787          1999
       147      52787          2000
       147      52787          2001
       147      52787          2002
       147      52788          1998
       147      52788          1999
       147      52788          2000          1       7.99
       147      52788          2001
       147      52788          2002
       148      52782          1998        139    4046.67
       148      52782          1999        228    5362.57
       148      52782          2000        251    5629.47
       148      52782          2001        308    7138.98
       148      52782          2002
       148      52785          1998
       148      52785          1999
       148      52785          2000
       148      52785          2001
       148      52785          2002
       148      52786          1998
       148      52786          1999
       148      52786          2000
       148      52786          2001
       148      52786          2002
       148      52787          1998
       148      52787          1999
       148      52787          2000
       148      52787          2001
       148      52787          2002
       148      52788          1998          4     117.23
       148      52788          1999
       148      52788          2000
       148      52788          2001
       148      52788          2002
</pre></div>
<!-- class="sect2" -->
<a id="DWHSG8739"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1236"></a>
<h3 class="sect2">Filling Gaps in an Inventory Table</h3>
<p>An inventory table typically tracks quantity of units available for various products. This table is sparse: it only stores a row for a product when there is an event. For a sales table, the event is a sale, and for the inventory table, the event is a change in quantity available for a product. For example, consider the following inventory table:</p>
<pre>
CREATE TABLE invent_table (
product VARCHAR2(10),
time_id DATE,
quant NUMBER);

INSERT INTO invent_table VALUES
 ('bottle', TO_DATE('01/04/01', 'DD/MM/YY'), 10);
INSERT INTO invent_table VALUES
 ('bottle', TO_DATE('06/04/01', 'DD/MM/YY'), 8);
INSERT INTO invent_table VALUES
 ('can', TO_DATE('01/04/01', 'DD/MM/YY'), 15);
INSERT INTO invent_table VALUES
 ('can', TO_DATE('04/04/01', 'DD/MM/YY'), 11);
</pre>
<p>The inventory table now has the following rows:</p>
<pre>
PRODUCT    TIME_ID   QUANT
---------- --------- -----
bottle     01-APR-01    10
bottle     06-APR-01     8
can        01-APR-01    15
can        04-APR-01    11
</pre>
<p>For reporting purposes, users may want to see this inventory data differently. For example, they may want to see all values of time for each product. This can be accomplished using partitioned outer join. In addition, for the newly inserted rows of missing time periods, users may want to see the values for quantity of units column to be carried over from the most recent existing time period. The latter can be accomplished using analytic window function <code>LAST_VALUE</code> value. Here is the query and the desired output:</p>
<pre>
WITH v1 AS
 (SELECT time_id
  FROM times
  WHERE times.time_id BETWEEN
   TO_DATE('01/04/01', 'DD/MM/YY')
   AND TO_DATE('07/04/01', 'DD/MM/YY'))
SELECT product, time_id, quant quantity,
  LAST_VALUE(quant IGNORE NULLS)
    OVER (PARTITION BY product ORDER BY time_id)
    repeated_quantity
FROM 
 (SELECT product, v1.time_id, quant
  FROM invent_table PARTITION BY (product)
  RIGHT OUTER JOIN v1
  ON (v1.time_id = invent_table.time_id))
ORDER BY 1, 2;
</pre>
<p>The inner query computes a partitioned outer join on time within each product. The inner query densifies the data on the time dimension (meaning the time dimension will now have a row for each day of the week). However, the measure column <code>quantity</code> will have nulls for the newly added rows (see the output in the column <code>quantity</code> in the following results.</p>
<p>The outer query uses the analytic function <code>LAST_VALUE</code>. Applying this function partitions the data by product and orders the data on the time dimension column (<code>time_id</code>). For each row, the function finds the last non-null value in the window due to the option <code>IGNORE</code> <code>NULLS</code>, which you can use with both <code>LAST_VALUE</code> and <code>FIRST_VALUE</code>. We see the desired output in the column <code>repeated_quantity</code> in the following output:</p>
<pre>
PRODUCT    TIME_ID   QUANTITY REPEATED_QUANTITY
---------- --------- -------- -----------------
bottle     01-APR-01       10                10
bottle     02-APR-01                         10
bottle     03-APR-01                         10
bottle     04-APR-01                         10
bottle     05-APR-01                         10
bottle     06-APR-01        8                 8
bottle     07-APR-01                          8
can        01-APR-01       15                15
can        02-APR-01                         15
can        03-APR-01                         15
can        04-APR-01       11                11
can        05-APR-01                         11
can        06-APR-01                         11
can        07-APR-01                         11
</pre></div>
<!-- class="sect2" -->
<a id="DWHSG8740"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1237"></a>
<h3 class="sect2">Computing Data Values to Fill Gaps</h3>
<p>Examples<a id="sthref1238"></a> in previous section illustrate how to use partitioned outer join to fill gaps in one or more dimensions. However, the result sets produced by partitioned outer join have null values for columns that are not included in the <code>PARTITION</code> <code>BY</code> list. Typically, these are measure columns. Users can make use of analytic SQL functions to replace those null values with a non-null value.</p>
<p>For example, the following query computes monthly totals for products 64MB Memory card and DVD-R Discs (product IDs 122 and 136) for the year 2000. It uses partitioned outer join to densify data for all months. For the missing months, it then uses the analytic SQL function <code>AVG</code> to compute the sales and units to be the average of the months when the product was sold.</p>
<p>If working in SQL*Plus, the following two commands wraps the column headings for greater readability of results:</p>
<pre>
col computed_units  heading  'Computed|_units'
col computed_sales  heading  'Computed|_sales'

WITH V AS
 (SELECT substr(p.prod_name,1,12) prod_name, calendar_month_desc,
     SUM(quantity_sold) units, SUM(amount_sold) sales
   FROM sales s, products p, times t
   WHERE s.prod_id IN (122,136) AND calendar_year = 2000
     AND t.time_id = s.time_id
     AND p.prod_id = s.prod_id
   GROUP BY p.prod_name, calendar_month_desc)
SELECT v.prod_name, calendar_month_desc, units, sales,
  NVL(units, AVG(units) OVER (partition by v.prod_name)) computed_units,
  NVL(sales, AVG(sales) OVER (partition by v.prod_name)) computed_sales
FROM 
  (SELECT DISTINCT calendar_month_desc
   FROM times
   WHERE calendar_year = 2000) t
   LEFT OUTER JOIN V
   PARTITION BY (prod_name)
   USING (calendar_month_desc);
                                              computed   computed
PROD_NAME    CALENDAR      UNITS      SALES     _units     _sales
------------ -------- ---------- ---------- ---------- ----------
64MB Memory  2000-01         112    4129.72        112    4129.72
64MB Memory  2000-02         190       7049        190       7049
64MB Memory  2000-03          47    1724.98         47    1724.98
64MB Memory  2000-04          20      739.4         20      739.4
64MB Memory  2000-05          47    1738.24         47    1738.24
64MB Memory  2000-06          20      739.4         20      739.4
64MB Memory  2000-07                        72.6666667    2686.79
64MB Memory  2000-08                        72.6666667    2686.79
64MB Memory  2000-09                        72.6666667    2686.79
64MB Memory  2000-10                        72.6666667    2686.79
64MB Memory  2000-11                        72.6666667    2686.79
64MB Memory  2000-12                        72.6666667    2686.79
DVD-R Discs, 2000-01         167     3683.5        167     3683.5
DVD-R Discs, 2000-02         152    3362.24        152    3362.24
DVD-R Discs, 2000-03         188    4148.02        188    4148.02
DVD-R Discs, 2000-04         144    3170.09        144    3170.09
DVD-R Discs, 2000-05         189    4164.87        189    4164.87
DVD-R Discs, 2000-06         145    3192.21        145    3192.21
DVD-R Discs, 2000-07                            124.25    2737.71
DVD-R Discs, 2000-08                            124.25    2737.71
DVD-R Discs, 2000-09           1      18.91          1      18.91
DVD-R Discs, 2000-10                            124.25    2737.71
DVD-R Discs, 2000-11                            124.25    2737.71
DVD-R Discs, 2000-12           8     161.84          8     161.84
</pre></div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="CIHIGJII"></a><a id="DWHSG8741"></a>
<div class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1">Time Series Calculations on Densified Data</h2>
<p>Densificatio<a id="sthref1239"></a><a id="sthref1240"></a>n is not just for reporting purpose. It also enables certain types of calculations, especially, time series calculations. Time series calculations are easier when data is dense along the time dimension. Dense data has a consistent number of rows for each time periods which in turn make it simple to use analytic window functions with physical offsets.</p>
<p>To illustrate, let us first take the example on <a href="#CIHIGEDA">"Filling Gaps in Data"</a>, and let's add an analytic function to that query. In the following enhanced version, we calculate weekly year-to-date sales alongside the weekly sales. The <code>NULL</code> values that the partitioned outer join inserts in making the time series dense are handled in the usual way: the <code>SUM</code> function treats them as 0's.</p>
<pre>
SELECT Product_Name, t.Year, t.Week, NVL(Sales,0) Current_sales,
 SUM(Sales) 
   OVER (PARTITION BY Product_Name, t.year ORDER BY t.week) Cumulative_sales
FROM 
 (SELECT SUBSTR(p.Prod_Name,1,15) Product_Name, t.Calendar_Year Year,
   t.Calendar_Week_Number Week, SUM(Amount_Sold) Sales
  FROM Sales s, Times t, Products p
  WHERE s.Time_id = t.Time_id AND
   s.Prod_id = p.Prod_id AND p.Prod_name IN ('Bounce') AND
   t.Calendar_Year IN (2000,2001) AND
   t.Calendar_Week_Number BETWEEN 20 AND 30
  GROUP BY p.Prod_Name, t.Calendar_Year, t.Calendar_Week_Number) v
PARTITION BY (v.Product_Name)
RIGHT OUTER JOIN
(SELECT DISTINCT
 Calendar_Week_Number Week, Calendar_Year Year
 FROM Times
 WHERE Calendar_Year in (2000, 2001)
 AND Calendar_Week_Number BETWEEN 20 AND 30) t
ON (v.week = t.week AND v.Year = t.Year)
ORDER BY t.year, t.week;

PRODUCT_NAME          YEAR       WEEK CURRENT_SALES CUMULATIVE_SALES
--------------- ---------- ---------- ------------- ----------------
Bounce                2000         20           801              801
Bounce                2000         21       4062.24          4863.24
Bounce                2000         22       2043.16           6906.4
Bounce                2000         23       2731.14          9637.54
Bounce                2000         24       4419.36          14056.9
Bounce                2000         25             0          14056.9
Bounce                2000         26             0          14056.9
Bounce                2000         27       2297.29         16354.19
Bounce                2000         28       1443.13         17797.32
Bounce                2000         29       1927.38          19724.7
Bounce                2000         30       1927.38         21652.08
Bounce                2001         20        1483.3           1483.3
Bounce                2001         21       4184.49          5667.79
Bounce                2001         22       2609.19          8276.98
Bounce                2001         23       1416.95          9693.93
Bounce                2001         24       3149.62         12843.55
Bounce                2001         25       2645.98         15489.53
Bounce                2001         26             0         15489.53
Bounce                2001         27       2125.12         17614.65
Bounce                2001         28             0         17614.65
Bounce                2001         29       2467.92         20082.57
Bounce                2001         30       2620.17         22702.74
</pre>
<a id="DWHSG8742"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1241"></a>
<h3 class="sect2">Period-to-Period Comparison for One Time Level: Example</h3>
<p>How do we use this feature to compare values across time periods? Specifically, how do we calculate a year-over-year sales comparison at the week level? The following query returns on the same row, for each product, the year-to-date sales for each week of 2001 with that of 2000.</p>
<p>Note that in this example we start with a <code>WITH</code> clause. This improves readability of the query and lets us focus on the partitioned outer join. If working in SQL*Plus, the following command wraps the column headings for greater readability of results:</p>
<pre>
col Weekly_ytd_sales_prior_year  heading  'Weekly_ytd|_sales_|prior_year'

WITH v AS
  (SELECT SUBSTR(p.Prod_Name,1,6) Prod, t.Calendar_Year Year,
     t.Calendar_Week_Number Week, SUM(Amount_Sold) Sales
   FROM Sales s, Times t, Products p
   WHERE s.Time_id = t.Time_id AND
     s.Prod_id = p.Prod_id AND p.Prod_name in ('Y Box') AND
     t.Calendar_Year in (2000,2001) AND 
     t.Calendar_Week_Number BETWEEN 30 AND 40
   GROUP BY p.Prod_Name, t.Calendar_Year, t.Calendar_Week_Number)
SELECT Prod , Year, Week, Sales,
  Weekly_ytd_sales, Weekly_ytd_sales_prior_year
FROM 
  (SELECT Prod, Year, Week, Sales, Weekly_ytd_sales,
     LAG(Weekly_ytd_sales, 1) OVER 
       (PARTITION BY Prod , Week ORDER BY Year) Weekly_ytd_sales_prior_year
   FROM 
     (SELECT v.Prod  Prod , t.Year Year, t.Week Week,
        NVL(v.Sales,0) Sales, SUM(NVL(v.Sales,0)) OVER
          (PARTITION BY v.Prod , t.Year ORDER BY t.week) weekly_ytd_sales
      FROM v
      PARTITION BY (v.Prod )
      RIGHT OUTER JOIN
        (SELECT DISTINCT Calendar_Week_Number Week, Calendar_Year Year
         FROM Times
         WHERE Calendar_Year IN (2000, 2001)) t
      ON (v.week = t.week AND v.Year = t.Year)
     ) dense_sales
  ) year_over_year_sales
WHERE Year = 2001 AND Week BETWEEN 30 AND 40
ORDER BY 1, 2, 3;
                                                         Weekly_ytd
                                                            _sales_
PROD         YEAR       WEEK      SALES WEEKLY_YTD_SALES prior_year
------ ---------- ---------- ---------- ---------------- ----------
Y Box        2001         30    7877.45          7877.45          0
Y Box        2001         31   13082.46         20959.91    1537.35
Y Box        2001         32   11569.02         32528.93    9531.57
Y Box        2001         33   38081.97          70610.9   39048.69
Y Box        2001         34   33109.65        103720.55   69100.79
Y Box        2001         35          0        103720.55   71265.35
Y Box        2001         36     4169.3        107889.85   81156.29
Y Box        2001         37   24616.85         132506.7   95433.09
Y Box        2001         38   37739.65        170246.35  107726.96
Y Box        2001         39     284.95         170531.3   118817.4
Y Box        2001         40   10868.44        181399.74  120969.69
</pre>
<p>In the <code>FROM</code> clause of the inline view <code>dense_sales</code>, we use a partitioned outer join of aggregate view <code>v</code> and time view <code>t</code> to fill gaps in the sales data along the time dimension. The output of the partitioned outer join is then processed by the analytic function <code>SUM ... OVER</code> to compute the weekly year-to-date sales (the <code>weekly_ytd_sales</code> column). Thus, the view <code>dense_sales</code> computes the year-to-date sales data for each week, including those missing in the aggregate view s. The inline view <code>year_over_year_sales</code> then computes the year ago weekly year-to-date sales using the <code>LAG</code> function. The <code>LAG</code> function labeled <code>weekly_ytd_sales_prior_year</code> specifies a <code>PARTITION</code> <code>BY</code> clause that pairs rows for the same week of years 2000 and 2001 into a single partition. We then pass an offset of 1 to the <code>LAG</code> function to get the weekly year to date sales for the prior year.The outermost query block selects data from <code>year_over_year_sales</code> with the condition <code>yr = 2001</code>, and thus the query returns, for each product, its weekly year-to-date sales in the specified weeks of years 2001 and 2000.</p>
</div>
<!-- class="sect2" -->
<a id="DWHSG8743"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1242"></a>
<h3 class="sect2">Period-to-Period Comparison for Multiple Time Levels: Example</h3>
<p>While the prior example shows us a way to create comparisons for a single time level, it would be even more useful to handle multiple time levels in a single query. For example, we could compare sales versus the prior period at the year, quarter, month and day levels. How can we create a query which performs a year-over-year comparison of year-to-date sales for all levels of our time hierarchy?</p>
<p>We will take several steps to perform this task. The goal is a single query with comparisons at the day, week, month, quarter, and year level. The steps are as follows:</p>
<ol>
<li>
<p>We will create a view called <code>cube_prod_time</code>, which holds a hierarchical cube of sales aggregated across <code>times</code> and <code>products</code>.</p>
</li>
<li>
<p>Then we will create a view of the time dimension to use as an edge of the cube. The time edge, which holds a complete set of dates, will be partitioned outer joined to the sparse data in the view <code>cube_prod_time</code>.</p>
</li>
<li>
<p>Finally, for maximum performance, we will create a materialized view, <code>mv_prod_time</code>, built using the same definition as <code>cube_prod_time</code>.</p>
</li>
</ol>
<p>For more information regarding hierarchical cubes, see <a href="aggreg.htm#g1023942">Chapter 21, "SQL for Aggregation in Data Warehouses"</a>. The materialized view is defined in Step 1 in the following section.</p>
<dl>
<dd><a id="sthref1243"></a></dd>
<dt class="seghead">Step 1&nbsp;&nbsp;&nbsp;Create the hierarchical cube view</dt>
<dd>
<p>The materialized view shown in the following may already exist in your system; if not, create it now. If you must generate it, note that we limit the query to just two products to keep processing time short:</p>
<pre>
CREATE OR REPLACE VIEW cube_prod_time AS
SELECT 
  (CASE
     WHEN ((GROUPING(calendar_year)=0 ) 
       AND (GROUPING(calendar_quarter_desc)=1 ))
       THEN (TO_CHAR(calendar_year) || '_0')
     WHEN ((GROUPING(calendar_quarter_desc)=0 )
       AND (GROUPING(calendar_month_desc)=1 ))
       THEN (TO_CHAR(calendar_quarter_desc) || '_1')
     WHEN ((GROUPING(calendar_month_desc)=0 )
       AND (GROUPING(t.time_id)=1 ))
       THEN (TO_CHAR(calendar_month_desc) || '_2')
     ELSE (TO_CHAR(t.time_id) || '_3')
  END) Hierarchical_Time,
  calendar_year year, calendar_quarter_desc quarter,
  calendar_month_desc month, t.time_id day,
  prod_category cat, prod_subcategory subcat, p.prod_id prod,
  GROUPING_ID(prod_category, prod_subcategory, p.prod_id,
    calendar_year, calendar_quarter_desc, calendar_month_desc,t.time_id) gid,
  GROUPING_ID(prod_category, prod_subcategory, p.prod_id) gid_p,
  GROUPING_ID(calendar_year, calendar_quarter_desc,
    calendar_month_desc, t.time_id) gid_t,
  SUM(amount_sold) s_sold, COUNT(amount_sold) c_sold, COUNT(*) cnt
FROM SALES s, TIMES t, PRODUCTS p
WHERE s.time_id = t.time_id AND
  p.prod_name IN ('Bounce', 'Y Box') AND s.prod_id = p.prod_id
GROUP BY
  ROLLUP(calendar_year, calendar_quarter_desc, calendar_month_desc, t.time_id),
  ROLLUP(prod_category, prod_subcategory, p.prod_id);
</pre>
<p>Because this view is limited to two products, it returns just over 2200 rows. Note that the column <code>Hierarchical_Time</code> contains string representations of time from all levels of the time hierarchy. The <code>CASE</code> expression used for the <code>Hierarchical_Time</code> column appends a marker (_0, _1, ...) to each date string to denote the time level of the value. A _0 represents the year level, _1 is quarters, _2 is months, and _3 is day. Note that the <code>GROUP</code> <code>BY</code> clause is a concatenated <code>ROLLUP</code> which specifies the rollup hierarchy for the time and product dimensions. The <code>GROUP</code> <code>BY</code> clause is what determines the hierarchical cube contents.</p>
</dd>
<dd><a id="sthref1244"></a></dd>
<dt class="seghead">Step 2&nbsp;&nbsp;&nbsp;Create the view edge_time, which is a complete set of date values</dt>
<dd>
<p><code>edge_time</code> is the source for filling time gaps in the hierarchical cube using a partitioned outer join. The column <code>Hierarchical_Time</code> in <code>edge_time</code> will be used in a partitioned join with the <code>Hierarchical_Time</code> column in the view <code>cube_prod_time</code>. The following statement defines <code>edge_time</code>:</p>
<pre>
CREATE OR REPLACE VIEW edge_time AS
SELECT 
  (CASE
     WHEN ((GROUPING(calendar_year)=0 )
       AND (GROUPING(calendar_quarter_desc)=1 ))
       THEN (TO_CHAR(calendar_year) || '_0')
     WHEN ((GROUPING(calendar_quarter_desc)=0 )
       AND (GROUPING(calendar_month_desc)=1 ))
       THEN (TO_CHAR(calendar_quarter_desc) || '_1')
     WHEN ((GROUPING(calendar_month_desc)=0 )
       AND (GROUPING(time_id)=1 ))
       THEN (TO_CHAR(calendar_month_desc) || '_2')
     ELSE (TO_CHAR(time_id) || '_3')
   END) Hierarchical_Time,
   calendar_year yr, calendar_quarter_number qtr_num,
   calendar_quarter_desc qtr, calendar_month_number mon_num,
   calendar_month_desc mon, time_id - TRUNC(time_id, 'YEAR') + 1 day_num,
   time_id day,
GROUPING_ID(calendar_year, calendar_quarter_desc,
  calendar_month_desc, time_id) gid_t
FROM TIMES
GROUP BY ROLLUP
 (calendar_year, (calendar_quarter_desc, calendar_quarter_number),
 (calendar_month_desc, calendar_month_number), time_id);
</pre></dd>
<dd><a id="sthref1245"></a></dd>
<dt class="seghead">Step 3&nbsp;&nbsp;&nbsp;Create the materialized view mv_prod_time to support faster performance</dt>
<dd>
<p>The materialized view definition is a duplicate of the view <code>cube_prod_time</code> defined earlier. Because it is a duplicate query, references to <code>cube_prod_time</code> will be rewritten to use the <code>mv_prod_time</code> materialized view. The following materialized may already exist in your system; if not, create it now. If you must generate it, note that we limit the query to just two products to keep processing time short.</p>
<pre>
CREATE MATERIALIZED VIEW mv_prod_time
REFRESH COMPLETE ON DEMAND AS
SELECT 
  (CASE
     WHEN ((GROUPING(calendar_year)=0 )
       AND (GROUPING(calendar_quarter_desc)=1 ))
       THEN (TO_CHAR(calendar_year) || '_0')
     WHEN ((GROUPING(calendar_quarter_desc)=0 )
       AND (GROUPING(calendar_month_desc)=1 ))
       THEN (TO_CHAR(calendar_quarter_desc) || '_1')
     WHEN ((GROUPING(calendar_month_desc)=0 )
       AND (GROUPING(t.time_id)=1 ))
       THEN (TO_CHAR(calendar_month_desc) || '_2')
     ELSE (TO_CHAR(t.time_id) || '_3')
  END) Hierarchical_Time,
  calendar_year year, calendar_quarter_desc quarter,
  calendar_month_desc month, t.time_id day,
  prod_category cat, prod_subcategory subcat, p.prod_id prod,
  GROUPING_ID(prod_category, prod_subcategory, p.prod_id,
    calendar_year, calendar_quarter_desc, calendar_month_desc,t.time_id) gid,
  GROUPING_ID(prod_category, prod_subcategory, p.prod_id) gid_p,
  GROUPING_ID(calendar_year, calendar_quarter_desc,
    calendar_month_desc, t.time_id) gid_t,
  SUM(amount_sold) s_sold, COUNT(amount_sold) c_sold, COUNT(*) cnt
FROM SALES s, TIMES t, PRODUCTS p
WHERE s.time_id = t.time_id AND
  p.prod_name IN ('Bounce', 'Y Box') AND s.prod_id = p.prod_id
GROUP BY
  ROLLUP(calendar_year, calendar_quarter_desc, calendar_month_desc, t.time_id),
  ROLLUP(prod_category, prod_subcategory, p.prod_id);
</pre></dd>
<dd><a id="sthref1246"></a></dd>
<dt class="seghead">Step 4&nbsp;&nbsp;&nbsp;Create the comparison query</dt>
<dd>
<p>We have now set the stage for our comparison query. We can obtain period-to-period comparison calculations at all time levels. It requires applying analytic functions to a hierarchical cube with dense data along the time dimension.</p>
<p>Some of the calculations we can achieve for each time level are:</p>
<ul>
<li>
<p>Sum of sales for prior period at all levels of time.</p>
</li>
<li>
<p>Variance in sales over prior period.</p>
</li>
<li>
<p>Sum of sales in the same period a year ago at all levels of time.</p>
</li>
<li>
<p>Variance in sales over the same period last year.</p>
</li>
</ul>
<p>The following example performs all four of these calculations. It uses a partitioned outer join of the views <code>cube_prod_time</code> and <code>edge_time</code> to create an inline view of dense data called <code>dense_cube_prod_time</code>. The query then uses the <code>LAG</code> function in the same way as the prior single-level example. The outer <code>WHERE</code> clause specifies time at three levels: the days of August 2001, the entire month, and the entire third quarter of 2001. Note that the last two rows of the results contain the month level and quarter level aggregations.Note that to make the results easier to read if you are using SQL*Plus, the column headings should be adjusted with the following commands. The commands will fold the column headings to reduce line length:</p>
<pre>
col sales_prior_period heading 'sales_prior|_period'
col variance_prior_period heading 'variance|_prior|_period'
col sales_same_period_prior_year heading 'sales_same|_period_prior|_year'
col variance_same_period_p_year heading 'variance|_same_period|_prior_year'
</pre>
<p>Here is the query comparing current sales to prior and year ago sales:</p>
<pre>
SELECT SUBSTR(prod,1,4) prod, SUBSTR(Hierarchical_Time,1,12) ht, 
  sales, sales_prior_period, 
  sales - sales_prior_period variance_prior_period,
  sales_same_period_prior_year,
  sales - sales_same_period_prior_year variance_same_period_p_year
FROM
 (SELECT cat, subcat, prod, gid_p, gid_t, 
    Hierarchical_Time, yr, qtr, mon, day, sales,
    LAG(sales, 1) OVER (PARTITION BY gid_p, cat, subcat, prod,
      gid_t ORDER BY yr, qtr, mon, day)
      sales_prior_period,
    LAG(sales, 1) OVER (PARTITION BY gid_p, cat, subcat, prod,
      gid_t, qtr_num, mon_num, day_num ORDER BY yr)
      sales_same_period_prior_year
  FROM 
   (SELECT c.gid, c.cat, c.subcat, c.prod, c.gid_p,
      t.gid_t, t.yr, t.qtr, t.qtr_num, t.mon, t.mon_num,
      t.day, t.day_num, t.Hierarchical_Time, NVL(s_sold,0) sales
    FROM cube_prod_time c
    PARTITION BY (gid_p, cat, subcat, prod)
    RIGHT OUTER JOIN edge_time t
    ON ( c.gid_t = t.gid_t AND 
      c.Hierarchical_Time = t.Hierarchical_Time)
   ) dense_cube_prod_time
 )                        --side by side current and prior year sales
WHERE prod IN (139) AND gid_p=0 AND    --1 product and product level data
  ( (mon IN ('2001-08' ) AND gid_t IN (0, 1)) OR --day and month data
  (qtr IN ('2001-03' ) AND gid_t IN (3)))        --quarter level data
ORDER BY day;

                                           variance    sales_same     variance
                             sales_prior     _prior _period_prior _same_period
PROD HT                SALES     _period    _period         _year  _prior_year
---- ------------ ---------- ----------- ---------- ------------- ------------
139  01-AUG-01_3           0           0          0             0            0
139  02-AUG-01_3     1347.53           0    1347.53             0      1347.53
139  03-AUG-01_3           0     1347.53   -1347.53         42.36       -42.36
139  04-AUG-01_3       57.83           0      57.83        995.75      -937.92
139  05-AUG-01_3           0       57.83     -57.83             0            0
139  06-AUG-01_3           0           0          0             0            0
139  07-AUG-01_3      134.81           0     134.81        880.27      -745.46
139  08-AUG-01_3     1289.89      134.81    1155.08             0      1289.89
139  09-AUG-01_3           0     1289.89   -1289.89             0            0
139  10-AUG-01_3           0           0          0             0            0
139  11-AUG-01_3           0           0          0             0            0
139  12-AUG-01_3           0           0          0             0            0
139  13-AUG-01_3           0           0          0             0            0
139  14-AUG-01_3           0           0          0             0            0
139  15-AUG-01_3       38.49           0      38.49       1104.55     -1066.06
139  16-AUG-01_3           0       38.49     -38.49             0            0
139  17-AUG-01_3       77.17           0      77.17       1052.03      -974.86
139  18-AUG-01_3     2467.54       77.17    2390.37             0      2467.54
139  19-AUG-01_3           0     2467.54   -2467.54        127.08      -127.08
139  20-AUG-01_3           0           0          0             0            0
139  21-AUG-01_3           0           0          0             0            0
139  22-AUG-01_3           0           0          0             0            0
139  23-AUG-01_3     1371.43           0    1371.43             0      1371.43
139  24-AUG-01_3      153.96     1371.43   -1217.47        2091.3     -1937.34
139  25-AUG-01_3           0      153.96    -153.96             0            0
139  26-AUG-01_3           0           0          0             0            0
139  27-AUG-01_3     1235.48           0    1235.48             0      1235.48
139  28-AUG-01_3       173.3     1235.48   -1062.18       2075.64     -1902.34
139  29-AUG-01_3           0       173.3     -173.3             0            0
139  30-AUG-01_3           0           0          0             0            0
139  31-AUG-01_3           0           0          0             0            0
139  2001-08_2       8347.43     7213.21    1134.22       8368.98       -21.55
139  2001-03_1       24356.8    28862.14   -4505.34      24168.99       187.81
</pre>
<p>The first <code>LAG</code> function (<code>sales_prior_period</code>) partitions the data on <code>gid_p</code>, <code>cat</code>, <code>subcat</code>, <code>prod</code>, <code>gid_t</code> and orders the rows on all the time dimension columns. It gets the sales value of the prior period by passing an offset of 1. The second <code>LAG</code> function (<code>sales_same_period_prior_year</code>) partitions the data on additional columns <code>qtr_num</code>, <code>mon_num</code>, and <code>day_num</code> and orders it on <code>yr</code> so that, with an offset of 1, it can compute the year ago sales for the same period. The outermost <code>SELECT</code> clause computes the variances.</p>
</dd>
</dl>
</div>
<!-- class="sect2" -->
<a id="DWHSG8744"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref1247"></a>
<h3 class="sect2">Creating a Custom Member in a Dimension: Example</h3>
<p>In many analytical SQL tasks, it is helpful to define custom members in a dimension. For instance, you might define a specialized time period for analyses. You can use a partitioned outer join to temporarily add a member to a dimension. Note that the new SQL <code>MODEL</code> clause is suitable for creating more complex scenarios involving new members in dimensions. See <a href="sqlmodel.htm#g1042989">Chapter 23, "SQL for Modeling"</a> for more information on this topic.</p>
<p>As an example of a task, what if we want to define a new member for our <code>time</code> dimension? We want to create a 13th member of the Month level in our <code>time</code> dimension. This 13th month is defined as the summation of the sales for each product in the first month of each quarter of year 2001.</p>
<p>The solution has two steps. Note that we will build this solution using the views and tables created in the prior example. Two steps are required. First, create a view with the new member added to the appropriate dimension. The view uses a <code>UNION</code> <code>ALL</code> operation to add the new member. To query using the custom member, use a <code>CASE</code> expression and a partitioned outer join.</p>
<p>Our new member for the <code>time</code> dimension is created with the following view:</p>
<pre>
CREATE OR REPLACE VIEW time_c AS
(SELECT * FROM edge_time
UNION ALL
SELECT '2001-13_2', 2001, 5, '2001-05', 13, '2001-13', null, null,
8 -- &lt;gid_of_mon&gt;
FROM DUAL);
</pre>
<p>In this statement, the view <code>time_c</code> is defined by performing a <code>UNION</code> <code>ALL</code> of the <code>edge_time</code> view (defined in the prior example) and the user-defined 13th month. The <code>gid_t</code> value of 8 was chosen to differentiate the custom member from the standard members. The <code>UNION</code> <code>ALL</code> specifies the attributes for a 13th month member by doing a <code>SELECT</code> from the <code>DUAL</code> table. Note that the grouping id, column <code>gid_t</code>, is set to 8, and the quarter number is set to 5.</p>
<p>Then, the second step is to use an inline view of the query to perform a partitioned outer join of <code>cube_prod_time</code> with <code>time_c</code>. This step creates sales data for the 13th month at each level of product aggregation. In the main query, the analytic function <code>SUM</code> is used with a <code>CASE</code> expression to compute the 13th month, which is defined as the summation of the first month's sales of each quarter.</p>
<pre>
SELECT * FROM  (SELECT SUBSTR(cat,1,12) cat, SUBSTR(subcat,1,12) subcat,
     prod, mon, mon_num,
     SUM(CASE WHEN mon_num IN (1, 4, 7, 10)
           THEN s_sold 
           ELSE NULL 
         END)
       OVER (PARTITION BY gid_p, prod, subcat, cat, yr) sales_month_13
   FROM
     (SELECT c.gid, c.prod, c.subcat, c.cat, gid_p,
        t.gid_t, t.day, t.mon, t.mon_num,
        t.qtr, t.yr, NVL(s_sold,0) s_sold
      FROM cube_prod_time c
      PARTITION BY (gid_p, prod, subcat, cat)
      RIGHT OUTER JOIN time_c t
      ON (c.gid_t = t.gid_t AND 
        c.Hierarchical_Time = t.Hierarchical_Time)
     )
  )
WHERE mon_num=13;

CAT          SUBCAT             PROD MON         MON_NUM SALES_MONTH_13
------------ ------------ ---------- -------- ---------- --------------
Electronics  Game Console         16 2001-13          13      762334.34
Electronics  Y Box Games         139 2001-13          13       75650.22
Electronics  Game Console            2001-13          13      762334.34
Electronics  Y Box Games             2001-13          13       75650.22
Electronics                          2001-13          13      837984.56
                                     2001-13          13      837984.56
</pre>
<p>The <code>SUM</code> function uses a <code>CASE</code> to limit the data to months 1, 4, 7, and 10 within each year. Due to the tiny data set, with just 2 products, the rollup values of the results are necessarily repetitions of lower level aggregations. For more realistic set of rollup values, you can include more products from the Game Console and Y Box Games subcategories in the underlying materialized view.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="BCFBEEAG"></a><a id="DWHSG8745"></a>
<div class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1">Miscellaneous Analysis and Reporting Capabilities</h2>
<p>This section illustrates the following additional analytic capabilities:</p>
<ul>
<li>
<p><a href="#BCFHJBCE">WIDTH_BUCKET Function</a></p>
</li>
<li>
<p><a href="#BCFEFEBD">Linear Algebra</a></p>
</li>
<li>
<p><a href="#BCFEBBID">CASE Expressions</a></p>
</li>
<li>
<p><a href="#BCFCICGA">Frequent Itemsets</a></p>
</li>
</ul>
<a id="BCFHJBCE"></a><a id="DWHSG02010"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">WIDTH_BUCKET Function</h3>
<p>For<a id="sthref1248"></a><a id="sthref1249"></a> a given expression, the <code>WIDTH_BUCKET</code> function returns the bucket number that the result of this expression will be assigned after it is evaluated. You can generate equiwidth histograms with this function. Equiwidth histograms divide data sets into buckets whose interval size (highest value to lowest value) is equal. The number of rows held by each bucket will vary. A related function, <code>NTILE</code>, creates equiheight buckets.</p>
<p>Equiwidth histograms can be generated only for numeric, date or datetime types. So the first three parameters should be all numeric expressions or all date expressions. Other types of expressions are not allowed. If the first parameter is <code>NULL</code>, the result is <code>NULL</code>. If the second or the third parameter is <code>NULL</code>, an error message is returned, as a <code>NULL</code> value cannot denote any end point (or any point) for a range in a date or numeric value dimension. The last parameter (number of buckets) should be a numeric expression that evaluates to a positive integer value; 0, <code>NULL</code>, or a negative value will result in an error.</p>
<p>Buckets are numbered from 0 to (<code>n</code>+1). Bucket 0 holds the count of values less than the minimum. Bucket(<code>n</code>+1) holds the count of values greater than or equal to the maximum specified value.</p>
<a id="DWHSG8746"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1250"></a>
<h4 class="sect3">WIDTH_BUCKET Syntax</h4>
<p>The <code>WIDTH_BUCKET</code> takes four expressions as parameters. The first parameter is the expression that the equiwidth histogram is for. The second and third parameters are expressions that denote the end points of the acceptable range for the first parameter. The fourth parameter denotes the number of buckets.</p>
<pre>
WIDTH_BUCKET(expression, minval expression, maxval expression, num buckets)
</pre>
<p>Consider the following data from table <code>customers</code>, that shows the credit limits of 17 customers. This data is gathered in the query shown in <a href="#i1007460">Example 22-24</a>.</p>
<pre>
CUST_ID    CUST_CREDIT_LIMIT
---------  -----------------
    10346               7000
    35266               7000
    41496              15000
    35225              11000
     3424               9000
    28344               1500
    31112               7000
     8962               1500
    15192               3000
    21380               5000
    36651               1500
    30420               5000
     8270               3000
    17268              11000
    14459              11000
    13808               5000
    32497               1500
   100977               9000
   102077               3000
   103066              10000
   101784               5000
   100421              11000
   102343               3000
</pre>
<p>In the table <code>customers</code>, the column <code>cust_credit_limit</code> contains values between 1500 and 15000, and we can assign the values to four equiwidth buckets, numbered from 1 to 4, by using <code>WIDTH_BUCKET (cust_credit_limit, 0, 20000, 4)</code>. Ideally each bucket is a closed-open interval of the real number line, for example, bucket number 2 is assigned to scores between 5000.0000 and 9999.9999..., sometimes denoted [5000, 10000) to indicate that 5,000 is included in the interval and 10,000 is excluded. To accommodate values outside the range [0, 20,000), values less than 0 are assigned to a designated underflow bucket which is numbered 0, and values greater than or equal to 20,000 are assigned to a designated overflow bucket which is numbered 5 (num buckets + 1 in general). See <a href="#i1007452">Figure 22-3</a> for a graphical illustration of how the buckets are assigned.</p>
<div class="figure">
<p class="titleinfigure"><a id="i1007452"></a><a id="DWHSG8747"></a>Figure 22-3 Bucket Assignments</p>
<img width="470" height="91" src="./dwhsg078.gif" alt="Description of Figure 22-3 follows" /><br />
<a id="sthref1251" href="./img_text/dwhsg078.htm">Description of "Figure 22-3 Bucket Assignments"</a><br />
<br /></div>
<!-- class="figure" -->
<p>You can specify the bounds in the reverse order, for example, <code>WIDTH_BUCKET</code> (<code>cust_credit_limit</code>, <code>20000</code>, <code>0</code>, <code>4</code>). When the bounds are reversed, the buckets will be open-closed intervals. In this example, bucket number 1 is (<code>15000,20000</code>], bucket number 2 is (<code>10000,15000</code>], and bucket number 4, is (<code>0</code><code>,5000</code>]. The overflow bucket will be numbered 0 (<code>20000</code>, +<code>infinity</code>), and the underflow bucket will be numbered 5 (-<code>infinity</code>, <code>0</code>].</p>
<p>It is an error if the bucket count parameter is 0 or negative.</p>
<div class="example">
<p class="titleinexample"><a id="i1007460"></a><a id="DWHSG8748"></a>Example 22-24 WIDTH_BUCKET</p>
<p>The followin<a id="sthref1252"></a><a id="sthref1253"></a>g query shows the bucket numbers for the credit limits in the customers table for both cases where the boundaries are specified in regular or reverse order. We use a range of 0 to 20,000.</p>
<pre>
SELECT cust_id, cust_credit_limit,
 WIDTH_BUCKET(cust_credit_limit,0,20000,4) AS WIDTH_BUCKET_UP,
 WIDTH_BUCKET(cust_credit_limit,20000, 0, 4) AS WIDTH_BUCKET_DOWN
FROM customers WHERE cust_city = 'Marshal';

   CUST_ID CUST_CREDIT_LIMIT WIDTH_BUCKET_UP WIDTH_BUCKET_DOWN
---------- ----------------- --------------- -----------------
     10346              7000               2                 3
     35266              7000               2                 3
     41496             15000               4                 2
     35225             11000               3                 2
      3424              9000               2                 3
     28344              1500               1                 4
     31112              7000               2                 3
      8962              1500               1                 4
     15192              3000               1                 4
     21380              5000               2                 4
     36651              1500               1                 4
     30420              5000               2                 4
      8270              3000               1                 4
     17268             11000               3                 2
     14459             11000               3                 2
     13808              5000               2                 4
     32497              1500               1                 4
    100977              9000               2                 3
    102077              3000               1                 4
    103066             10000               3                 3
    101784              5000               2                 4
    100421             11000               3                 2
    102343              3000               1                 4
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFEFEBD"></a><a id="DWHSG02014"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Linear Algebra</h3>
<p>Linear algebra is a branch of mathematics with a wide range of practical applications. Many areas have tasks that can be expressed using linear algebra, and here are some examples from several fields: statistics (multiple linear regression and principle components analysis), data mining (clustering and classification), bioinformatics (analysis of microarray data), operations research (supply chain and other optimization problems), econometrics (analysis of consumer demand data), and finance (asset allocation problems). Various libraries for linear algebra are freely available for anyone to use. Oracle's <code>UTL_NLA</code> package exposes matrix PL/SQL data types and wrapper PL/SQL subprograms for two of the most popular and robust of these libraries, BLAS and LAPACK.</p>
<p>Linear algebra depends on matrix manipulation. Performing matrix manipulation in PL/SQL in the past required inventing a matrix representation based on PL/SQL's native data types and then writing matrix manipulation routines from scratch. This required substantial programming effort and the performance of the resulting implementation was limited. If developers chose to send data to external packages for processing rather than create their own routines, data transfer back and forth could be time consuming. Using the <code>UTL_NLA</code> package lets data stay within Oracle, removes the programming effort, and delivers a fast implementation.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8749"></a><a id="sthref1254"></a>Example 22-25 Linear Algebra</p>
<p>Here is an example of how Oracle's linear algebra support could be used for business analysis. It invokes a multiple linear regression application built using the <code>UTL_NLA</code> package. The multiple regression application is implemented in an object called <code>OLS_Regression</code>. Note that sample files for the OLS Regression object can be found in <code>$ORACLE_HOME/plsql/demo</code>.</p>
<p>Consider the scenario of a retailer analyzing the effectiveness of its marketing program. Each of its stores allocates its marketing budget over the following possible programs: media advertisements (<code>media</code>), promotions (<code>promo</code>), discount coupons (<code>disct</code>), and direct mailers (<code>dmail</code>). The regression analysis builds a linear relationship between the amount of sales that an average store has in a given year (<code>sales</code>) and the spending on the four components of the marketing program. Suppose that the marketing data is stored in the following table:</p>
<pre>
sales_marketing_data (
  /* Store information*/
  store_no   NUMBER,
  year       NUMBER,
  /* Sales revenue (in dollars)*/
  sales      NUMBER,   /* sales amount*/
  /* Marketing expenses (in dollars)*/
  media      NUMBER,   /*media advertisements*/
  promo      NUMBER,   /*promotions*/
  disct      NUMBER,   /*dicount coupons*/
  dmail      NUMBER,   /*direct mailers*/
</pre>
<p>Then you can build the following sales-marketing linear model using coefficients:</p>
<pre>
Sales Revenue = a  + b Media Advisements
                   + c Promotions
                   + d Discount Coupons
                   + e Direct Mailer
</pre>
<p>This model can be implemented as the following view, which refers to the OLS regression object:</p>
<pre>
CREATE OR REPLACE VIEW sales_marketing_model (year, ols)
   AS SELECT year,
        OLS_Regression( 
        /* mean_y =&gt; */
        AVG(sales),
        /* variance_y =&gt; */
        var_pop(sales),
        /* MV mean vector =&gt; */
        UTL_NLA_ARRAY_DBL (AVG(media),AVG(promo),
                           AVG(disct),AVG(dmail)),
        /* VCM variance covariance matrix =&gt; */
        UTL_NLA_ARRAY_DBL (var_pop(media),covar_pop(media,promo),
                           covar_pop(media,disct),covar_pop(media,dmail),
                           var_pop(promo),covar_pop(promo,disct),
                           covar_pop(promo,dmail),var_pop(disct),
                           covar_pop(disct,dmail),var_pop(dmail)),
        /* CV covariance vector =&gt; */
  UTL_NLA_ARRAY_DBL (covar_pop(sales,media),covar_pop(sales,promo),
                           covar_pop(sales,disct),covar_pop(sales,dmail)))
 FROM sales_marketing_data
 GROUP BY year;
</pre>
<p>Using this view, a marketing program manager can perform an analysis such as "Is this sales-marketing model reasonable for year 2004 data? That is, is the multiple-correlation greater than some acceptable value, say, 0.9?" The SQL for such a query might be as follows:</p>
<pre>
SELECT model.ols.getCorrelation(1)
       AS "Applicability of Linear Model"
FROM sales_marketing_model model
WHERE year = 2004;
</pre>
<p>You could also solve questions such as "What is the expected base-line sales revenue of a store without any marketing programs in 2003?" or "Which component of the marketing program was the most effective in 2004? That is, a dollar increase in which program produced the greatest expected increase in sales?"</p>
<p>See <a class="olink ARPLS224" href="../../appdev.112/e40758/u_nla.htm#ARPLS224"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for further information regarding the use of the <code>UTL_NLA</code> package and linear algebra.</p>
</div>
<!-- class="example" --></div>
<!-- class="sect2" -->
<a id="BCFEBBID"></a><a id="DWHSG02012"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">CASE Expressions</h3>
<p><a id="sthref1255"></a>Oracle now supports simple and searched <code>CASE</code> statements. <code>CASE</code> statements are similar in purpose to the <code>DECODE</code> statement, but they offer more flexibility and logical power. They are also easier to read than traditional <code>DECODE</code> statements, and offer better performance as well. They are commonly used when breaking categories into buckets like age (for example, 20-29, 30-39, and so on).</p>
<p>The syntax for simple <code>CASE</code> statements is:</p>
<pre>
CASE expr WHEN comparison_expr THEN return_expr
 [, WHEN comparison_expr THEN return_expr]... [ELSE else_expr] END
</pre>
<p>Simple <code>CASE</code> expressions test if the <code>expr</code> value equals the <code>comparison_expr</code>.</p>
<p>The syntax for searched <code>CASE</code> statements is:</p>
<pre>
CASE WHEN condition THEN return_expr [, WHEN condition THEN return_expr]
   ... ELSE else_expr] END
</pre>
<p>You can use any kind of condition in a searched <code>CASE</code> expression, not just an equality test.</p>
<p>You can specify only 65,535 arguments and each <code>WHEN</code> ... <code>THEN</code> pair counts as two arguments. To avoid exceeding this limit, you can nest <code>CASE</code> expressions so that the <code>return_expr</code> itself is a <code>CASE</code> expression.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8750"></a><a id="sthref1256"></a>Example 22-26 CASE</p>
<p>Suppose you wanted to find the average salary of all employees in the company. If an employee's salary is less than $2000, you want the query to use $2000 instead. Without a <code>CASE</code> statement, you might choose to write this query as follows:</p>
<pre>
SELECT AVG(foo(e.salary)) FROM employees e;
</pre>
<p>Note that this runs against the <code>hr</code> sample schema. In this, <code>foo</code> is a function that returns its input if the input is greater than 2000, and returns 2000 otherwise. The query has performance implications because it needs to invoke a function for each row. Writing custom functions can also add to the development load.</p>
<p>Using <code>CASE</code> expressions in the database without PL/SQL, this query can be rewritten as:</p>
<pre>
SELECT AVG(CASE when e.salary &gt; 2000 THEN e.salary ELSE 2000 end) 
  AS avg_sal_2k_floor
FROM employees e;
</pre>
<p>Using a <code>CASE</code> expression lets you avoid developing custom functions and can also perform faster.</p>
</div>
<!-- class="example" -->
<div class="example">
<p class="titleinexample"><a id="DWHSG8751"></a><a id="sthref1257"></a>Example 22-27 CASE for Aggregating Independent Subsets</p>
<p>Using <code>CASE</code> inside aggregate functions is a convenient way to perform aggregates on multiple subsets of data when a plain <code>GROUP</code> <code>BY</code> will not suffice. For instance, the preceding example could have included multiple <code>AVG</code> columns in its <code>SELECT</code> list, each with its own <code>CASE</code> expression. We might have had a query find the average salary for all employees in the salary ranges 0-2000 and 2000-5000. It would look like:</p>
<pre>
SELECT AVG(CASE WHEN e.sal BETWEEN 0 AND 2000 THEN e.sal ELSE null END) avg2000,
    AVG(CASE WHEN e.sal BETWEEN 2001 AND 5000 THEN e.sal ELSE null END) avg5000
 FROM emps e;
</pre>
<p>Although this query places the aggregates of independent subsets data into separate columns, by adding a <code>CASE</code> expression to the <code>GROUP</code> <code>BY</code> clause we can display the aggregates as the rows of a single column. The next section shows the flexibility of this approach with two approaches to creating histograms with <code>CASE</code>.</p>
</div>
<!-- class="example" -->
<a id="DWHSG8752"></a>
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref1258"></a>
<h4 class="sect3">Creating Histograms</h4>
<p><a id="sthref1259"></a>You can use the <code>CASE</code> statement when you want to obtain histograms with user-defined buckets (both in number of buckets and width of each bucket). The following are two examples of histograms created with <code>CASE</code> statements. In the first example, the histogram totals are shown in multiple columns and a single row is returned. In the second example, the histogram is shown with a label column and a single column for totals, and multiple rows are returned.</p>
<div class="example">
<p class="titleinexample"><a id="DWHSG8753"></a><a id="sthref1260"></a>Example 22-28 Histogram Example 1</p>
<pre>
SELECT SUM(CASE WHEN cust_credit_limit BETWEEN  0 AND 3999 THEN 1 ELSE 0 END)
  AS "0-3999",
SUM(CASE WHEN cust_credit_limit BETWEEN  4000 AND 7999 THEN 1 ELSE 0 END)
  AS "4000-7999",
SUM(CASE WHEN cust_credit_limit BETWEEN  8000 AND 11999 THEN 1 ELSE 0 END)
  AS "8000-11999",
SUM(CASE WHEN cust_credit_limit BETWEEN  12000 AND 16000 THEN 1 ELSE 0 END)
  AS "12000-16000"
FROM customers WHERE cust_city = 'Marshal';

    0-3999  4000-7999 8000-11999 12000-16000
---------- ---------- ---------- -----------
         8          7          7           1
</pre></div>
<!-- class="example" -->
<div class="example">
<p class="titleinexample"><a id="DWHSG8754"></a><a id="sthref1261"></a>Example 22-29 Histogram Example 2</p>
<pre>
SELECT (CASE WHEN cust_credit_limit BETWEEN  0 AND 3999 THEN  ' 0 - 3999'
   WHEN cust_credit_limit BETWEEN  4000 AND 7999 THEN ' 4000 - 7999'
   WHEN cust_credit_limit BETWEEN  8000 AND 11999 THEN  ' 8000 - 11999'
   WHEN cust_credit_limit BETWEEN  12000 AND 16000 THEN '12000 - 16000' END)
  AS BUCKET, COUNT(*) AS Count_in_Group
FROM customers WHERE cust_city = 'Marshal' GROUP BY
 (CASE WHEN cust_credit_limit BETWEEN  0 AND 3999 THEN ' 0 - 3999'
 WHEN cust_credit_limit BETWEEN  4000 AND 7999 THEN ' 4000 - 7999'
 WHEN cust_credit_limit BETWEEN  8000 AND 11999 THEN  ' 8000 - 11999'
 WHEN cust_credit_limit BETWEEN  12000 AND 16000 THEN '12000 - 16000' END);

BUCKET        COUNT_IN_GROUP
------------- --------------
 0 - 3999                  8
 4000 - 7999               7
 8000 - 11999              7
12000 - 16000              1
</pre></div>
<!-- class="example" --></div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BCFCICGA"></a><a id="DWHSG8755"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2">Frequent Itemsets</h3>
<p>Instead <a id="sthref1262"></a><a id="sthref1263"></a>of counting how often a given event occurs (for example, how often someone has purchased milk at the grocery), you may find it useful to count how often multiple events occur together (for example, how often someone has purchased both milk and cereal together at the grocery store). You can count these multiple events using what is called a frequent itemset, which is, as the name implies, a set of items. Some examples of itemsets could be all of the products that a given customer purchased in a single trip to the grocery store (commonly called a market basket), the web pages that a user accessed in a single session, or the financial services that a given customer utilizes.</p>
<p>The practical motivation for using a frequent itemset is to find those itemsets that occur most often. If you analyze a grocery store's point-of-sale data, you might, for example, discover that milk and bananas are the most commonly bought pair of items. Frequent itemsets have thus been used in business intelligence environments for many years, with the most common one being for market basket analysis in the retail industry. Frequent itemset calculations are integrated with the database, operating on top of relational tables and accessed through SQL. This integration provides the following key benefits:</p>
<ul>
<li>
<p>Applications that previously relied on frequent itemset operations now benefit from significantly improved performance as well as simpler implementation.</p>
</li>
<li>
<p>SQL-based applications that did not previously use frequent itemsets can now be easily extended to take advantage of this functionality.</p>
</li>
</ul>
<p>Frequent itemsets analysis is performed with the PL/SQL package <code>DBMS_FREQUENT_ITEMSETS</code>. See <a class="olink ARPLS007" href="../../appdev.112/e40758/d_frqist.htm#ARPLS007"><span class="italic">Oracle Database PL/SQL Packages and Types Reference</span></a> for more information. In addition, there is an example of frequent itemset usage in <a href="advbi.htm#CEFGAEGI">"Frequent itemsets"</a>.</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" --></div>
<!-- class="ind" -->
<!-- Start Footer -->
</div>
<!-- add extra wrapper close div-->
<footer><!--
<hr />
<table class="cellalignment1371">
<tr>
<td class="cellalignment1378">
<table class="cellalignment1376">
<tr>
<td class="cellalignment1375"><a href="aggreg.htm"><img width="24" height="24" src="../../dcommon/gifs/leftnav.gif" alt="Go to previous page" /><br />
<span class="icon">Previous</span></a></td>
<td class="cellalignment1375"><a href="sqlmodel.htm"><img width="24" height="24" src="../../dcommon/gifs/rightnav.gif" alt="Go to next page" /><br />
<span class="icon">Next</span></a></td>
</tr>
</table>
</td>
<td class="cellalignment-copyrightlogo"><img width="144" height="18" src="../../dcommon/gifs/oracle.gif" alt="Oracle" /><br />
Copyright&nbsp;&copy;&nbsp;2001, 2013,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved.<br />
<a href="../../dcommon/html/cpyr.htm">Legal Notices</a></td>
<td class="cellalignment1380">
<table class="cellalignment1374">
<tr>
<td class="cellalignment1375"><a href="../../index.htm"><img width="24" height="24" src="../../dcommon/gifs/doclib.gif" alt="Go to Documentation Home" /><br />
<span class="icon">Home</span></a></td>
<td class="cellalignment1375"><a href="../../nav/portal_booklist.htm"><img width="24" height="24" src="../../dcommon/gifs/booklist.gif" alt="Go to Book List" /><br />
<span class="icon">Book List</span></a></td>
<td class="cellalignment1375"><a href="toc.htm"><img width="24" height="24" src="../../dcommon/gifs/toc.gif" alt="Go to Table of Contents" /><br />
<span class="icon">Contents</span></a></td>
<td class="cellalignment1375"><a href="index.htm"><img width="24" height="24" src="../../dcommon/gifs/index.gif" alt="Go to Index" /><br />
<span class="icon">Index</span></a></td>
<td class="cellalignment1375"><a href="../../nav/mindx.htm"><img width="24" height="24" src="../../dcommon/gifs/masterix.gif" alt="Go to Master Index" /><br />
<span class="icon">Master Index</span></a></td>
<td class="cellalignment1375"><a href="../../dcommon/html/feedback.htm"><img width="24" height="24" src="../../dcommon/gifs/feedbck2.gif" alt="Go to Feedback page" /><br />
<span class="icon">Contact Us</span></a></td>
</tr>
</table>
</td>
</tr>
</table>
--></footer>
<noscript>
<p>Scripting on this page enhances content navigation, but does not change the content in any way.</p>
</noscript>
</body>
</html>
